<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Confessions of a Wall Street Programmer]]></title>
  <link href="http://btorpey.github.io/atom.xml" rel="self"/>
  <link href="http://btorpey.github.io/"/>
  <updated>2019-07-14T21:02:09-04:00</updated>
  <id>http://btorpey.github.io/</id>
  <author>
    <name><![CDATA[Bill Torpey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Memory Checking]]></title>
    <link href="http://btorpey.github.io/blog/2019/07/14/memory-checking/"/>
    <updated>2019-07-14T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2019/07/14/memory-checking</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/memory-testing.jpg" /></p>

<p>At my day job, I spend a fair amount of time working on software reliability.  One way to make software more reliable is to use memory-checking tools like valgrind’s <a href="http://www.valgrind.org/info/tools.html#memcheck">memcheck</a> and clang’s <a href="https://github.com/google/sanitizers/wiki/AddressSanitizer">AddressSanitizer</a> to detect memory errors at runtime.  </p>

<p>But these tools are typically not appropriate to use all the time – valgrind causes programs to run much more slowly than normal, and AddressSanitizer needs a special instrumented build of the code to work properly.  So neither tool is typically well-suited for production code.</p>

<p>But there’s another memory-checking tool that is “always on”.  That tool is plain old <code>malloc</code>, and it is the subject of this article.</p>

<!-- more -->

<p>The <a href="https://www.gnu.org/software/libc/">GNU C library</a> (glibc for short) provides implementations for the C standard library functions (e.g., <code>strlen</code> etc.) including functions that interface to the underlying OS (e.g., <code>open</code> et al.).  glibc also provides functions to manage memory, including <code>malloc</code>, <code>free</code> and their cousins, and in most code these memory management functions are among the most heavily used.</p>

<p>It’s not possible to be a C programmer and not be reasonably familiar with the memory management functions in glibc.  But what is not so well-known is the memory-checking functionality built into the library.</p>

<p>It turns out that glibc contains two separate sets of memory management functions – the core functions do minimal checking, and are significantly faster than the “debug” functions, which provide additional runtime checks.</p>

<p>The memory checking in <code>malloc</code> is controlled by an environment variable, named appropriately enough <code>MALLOC_CHECK_</code> (note the trailing underscore).  You can configure <code>malloc</code> to perform additional checking, and whether to print an error message and/or abort with a core file if it detects an error.  You can find full details at <a href="http://man7.org/linux/man-pages/man3/mallopt.3.html">http://man7.org/linux/man-pages/man3/mallopt.3.html</a>, but here’s the short version:</p>

<table>
  <thead>
    <tr>
      <th>Value</th>
      <th>Impl</th>
      <th>Checking</th>
      <th>Message</th>
      <th>Backtrace + mappings (since glibc 2.4+)</th>
      <th>Abort w/core</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>default (unset)</strong></td>
      <td><strong>Fast</strong></td>
      <td><strong>Minimal</strong></td>
      <td><strong>Detailed</strong></td>
      <td><strong>Yes</strong></td>
      <td><strong>Yes</strong></td>
    </tr>
    <tr>
      <td>0</td>
      <td>Fast</td>
      <td>Minimal</td>
      <td>None</td>
      <td>No</td>
      <td>No</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Slow</td>
      <td>Full</td>
      <td>Detailed</td>
      <td>No</td>
      <td>No</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Slow</td>
      <td>Full</td>
      <td>None</td>
      <td>No</td>
      <td>Yes</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Slow</td>
      <td>Full</td>
      <td>Detailed</td>
      <td>Yes</td>
      <td>Yes</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Slow</td>
      <td>Full</td>
      <td>Brief</td>
      <td>No</td>
      <td>Yes</td>
    </tr>
    <tr>
      <td>7</td>
      <td>Slow</td>
      <td>Full</td>
      <td>Brief</td>
      <td>Yes</td>
      <td>Yes</td>
    </tr>
  </tbody>
</table>

<p>What may be surprising is that the default behavior is for <code>malloc</code> to do at least minimal checking at runtime, and to <strong>abort the executable with a core file</strong> if those checks fail.  </p>

<p>This may or may not be what you want.  Given that the minimal checks in the fast implementation only detect certain specific errors, and that those errors (e.g., double free) tend not to cause additional problems, you may decide that a “no harm, no foul” approach is more appropriate (for example with production code where aborting with a core file is frowned upon ;-).</p>

<p>The other relevant point here is that setting <code>MALLOC_CHECK_</code> to any non-zero value causes <code>malloc</code> to use the slower heap functions that perform additional checks.  I’ve included a <a href="https://github.com/WallStProg/malloc-check/blob/master/malloc-bench.cpp">sample benchmark program</a> that shows the additional checking adds about 30% to the overhead of the <code>malloc</code>/<code>free</code> calls.  (And while the benchmark program is dumb as dirt, its results are similar to results on “real-world” tests).</p>

<p>It would be nice if one could get a fast implementation with the option to output an error message and continue execution, but with the current<sup id="fnref:rh7"><a href="#fn:rh7" rel="footnote">1</a></sup> implementation of glibc that doesn’t appear to be possible.  If you want the fast implementation but you don’t want to abort on errors, the only option is to turn off checking entirely (by explicitly setting <code>MALLOC_CHECK_</code> to 0).  </p>

<p>Note also that the <a href="http://man7.org/linux/man-pages/man3/mallopt.3.html">documentation</a> is a bit misleading:</p>

<blockquote>
  <p>Since glibc 2.3.4, the default value for the M_CHECK_ACTION              parameter is 3.</p>
</blockquote>

<p>While it’s true that with no value specified for <code>MALLOC_CHECK_</code> an error will cause a detailed error message with backtrace and mappings to be output, along with an abort with core file, that is <strong>NOT</strong> the same as explicitly setting <code>MALLOC_CHECK_=3</code> – that setting also causes <code>malloc</code> to use the slower functions that perform additional checks.</p>

<h3 id="minimal-vs-full-checking">“Minimal” vs. “Full” Checking</h3>

<ul>
  <li>In the table above, the “checking” setting for <code>MALLOC_CHECK_=0</code> is “minimal” – the checks are still performed, but errors are simply not reported.
    <ul>
      <li>Note that it is not possible to completely disable checking – minimal checking is <em>always</em> performed, even if the results are ignored. </li>
    </ul>
  </li>
  <li>The errors that can be detected with “minimal” checking are limited to a small subset of those detected with “full” checking – sometimes even for the same error.  For instance, with minimal checking a double-free can be detected <em>only</em> if the second free occurs immediately after the first.  With full checking the double-free is detected even if there are intervening calls to <code>malloc</code> and <code>free</code>.</li>
</ul>

<p>And, of course, the built-in checking in glibc can’t detect a <em>lot</em> of errors that can be found with more robust tools, like <a href="http://www.valgrind.org/">valgrind</a> and <a href="https://github.com/google/sanitizers/wiki/AddressSanitizer">AddressSanitizer</a>.  Nevertheless, <code>MALLOC_CHECK_</code> can be a useful adjunct to those tools for everyday use in development.</p>

<h2 id="conclusions">Conclusions</h2>
<ul>
  <li>For typical development, it’s probably best to explicitly set <code>MALLOC_CHECK_=3</code>.  This provides additional checking over and above the default setting, at the cost of somewhat poorer performance.</li>
  <li>For production use, you may want to decide whether the benefit of minimal checking is worth the possibility of having programs abort with errors that may be benign.  If the default is not appropriate, you basically have two choices:
    <ul>
      <li>Setting <code>MALLOC_CHECK_=1</code> will allow execution to continue after an error, but will at least provide a message that can be logged<sup id="fnref:log"><a href="#fn:log" rel="footnote">2</a></sup> to provide a warning that things are not quite right, and trigger additional troubleshooting, but at the cost of somewhat poorer performance.</li>
      <li>If you can’t afford to give up any performance at all, you can set <code>MALLOC_CHECK=0</code>, but any errors detected will be silently ignored.</li>
    </ul>
  </li>
</ul>

<h2 id="code">Code</h2>
<p>The code for this article is available <a href="https://github.com/WallStProg/malloc-check.git">here</a>.  There’s a benchmark program, which requires <a href="https://github.com/google/benchmark">Google Benchmark</a>.  There are also sample programs which demonstrate a double-free error that can be caught even with minimal checking (<code>double-free.c</code>), and which cannot (<code>double-free2.c</code>), and a simple script that ties everything together.  </p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes">
  <ol>
    <li id="fn:rh7">
      <p>Current for RedHat/CentOS 7 in any case, which is glibc 2.17.<a href="#fnref:rh7" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:log">
      <p>The error message from glibc is written directly to the console (tty device), not to <code>stderr</code>, which means that it will not be redirected.  If you need the message to appear on stderr, you will need to <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1519182">set another environment variable</a>:
<code>export LIBC_FATAL_STDERR_=1</code><a href="#fnref:log" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lots o' static]]></title>
    <link href="http://btorpey.github.io/blog/2017/09/17/lotso-static/"/>
    <updated>2017-09-17T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2017/09/17/lotso-static</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/static-cat.jpg" width="250" height="188" /></p>

<ul id="markdown-toc">
  <li><a href="#openmama">OpenMAMA</a></li>
  <li><a href="#following-along">Following along</a></li>
  <li><a href="#false-positives">False Positives</a></li>
  <li><a href="#style-vs-substance">Style vs. Substance</a></li>
  <li><a href="#dead-code">Dead Code</a></li>
  <li><a href="#buffer-overflow">Buffer Overflow</a></li>
  <li><a href="#null-pointer-dereference">NULL pointer dereference</a></li>
  <li><a href="#leaks">Leaks</a></li>
  <li><a href="#pointer-errors">Pointer Errors</a></li>
  <li><a href="#but-wait-theres-more">But Wait, There’s More!</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>I’ve written before about <a href="http://btorpey.github.io/blog/categories/static-analysis/">static analysis</a>, but in those earlier posts I wasn’t able to give specific examples of real-world code where static analysis is able to discover latent errors.</p>

<p>In the earlier articles I used a synthetic code-base <a href="https://github.com/regehr/itc-benchmarks">from ITC Research</a> to test clang, cppcheck and PVS-Studio.  I also ran all three tools on the code-bases that I’m responsible for maintaining at my “day job”, but I wasn’t able to share detailed results from that analysis, given that the code is not public.</p>

<p>In this article, I want to expand the discussion of static analysis by diving into a real-world, open-source code base that I’ve been working with lately, with specific examples of the kinds of problems static analysis can expose.</p>

<!-- more -->

<h2 id="openmama">OpenMAMA</h2>
<p>For this example, I’ll be using the <a href="http://openmama.org">OpenMAMA</a> source code.  OpenMAMA is an open-source messaging middleware framework that provides a high-level API for a bunch of messaging transports, including open-source (Qpid/AMQP, ZeroMQ) and commercial (DataFabric, Rendezvous, Solace, etc).</p>

<p>OpenMAMA is an interesting project – it started back in 2004 with Wombat Financial Software, which was attempting to sell its market-data software, but found it to be tough sledding.  While Wombat’s software performed better and was less expensive than Tibco’s Rendezvous (the de-facto standard at the time), no one wanted to rewrite their applications to target an API from a small company that might not be around in a couple of years.</p>

<p>So Wombat developed an open API which could sit on top of any messaging middleware, and they called it MAMA, for Middleware Agnostic Messaging API.  They also developed bindings for Rendezvous, in addition to their own software, so that prospective customers would have a warm and fuzzy feeling that they could write their applications once, and seamlessly switch out the underlying middleware software with little or no changes to their code.</p>

<p>That strategy worked well enough that in 2008 Wombat was acquired by the New York Stock Exchange, which renamed the software “Data Fabric” and used it as the backbone of their market-data network (SuperFeed).</p>

<p>When the company I was working for was also acquired by NYSE in 2009 I was tasked with replacing our existing middleware layer with the Mama/Wombat middleware, and in the process I came to appreciate the “pluggable” architecture of MAMA – it doesn’t make the issues related to different messaging systems go away, but it does provide a framework for dealing with them.</p>

<p>In 2011 NYSE Technologies donated OpenMAMA to the Linux Foundation.  Then, in 2014, the Wombat business was sold by NYSE to <a href="https://www.velatradingtech.com/">Vela Trading Technologies</a> (née SR Labs), which provides the proprietary Data Fabric middleware, and is also the primary maintainer for OpenMAMA.  There are a number of different <a href="http://www.openmama.org/what-is-openmama/supported-software">open-source and commercial implementations of OpenMAMA</a>.</p>

<p>Which brings us to the present day – I’ve recently started working with OpenMAMA again, so it seemed like a good idea to use that code as an example of how to use static analysis tools to identify latent bugs.</p>

<p>And, just to be clear, this is not a criticism of OpenMAMA – it’s an impressive piece of work, and has proven itself in demanding real-world situations.  </p>

<h2 id="following-along">Following along</h2>

<p>The analysis presented here is based on OpenMAMA release 6.2.1, which can be found <a href="https://github.com/OpenMAMA/OpenMAMA/releases/tag/OpenMAMA-6.2.1-release">here</a>.</p>

<p>I used <a href="http://cppcheck.sourceforge.net/">cppcheck version 1.80</a> and <a href="http://releases.llvm.org/download.html#5.0.0">clang version 5.0.0</a>.</p>

<p>Check out the <a href="http://btorpey.github.io/blog/categories/static-analysis/">earlier articles in this series</a> for more on building and running the various tools, including a bunch of helper scripts in the <a href="https://github.com/btorpey/static">GitHub repo</a>.</p>

<p>For the OpenMAMA analysis, I first built OpenMAMA using <a href="https://github.com/rizsotto/Bear">Bear</a> to create a compilation database from the scons build:  </p>

<pre><code class="language-bash">bear scons blddir=build product=mama with_unittest=n \
middleware=qpid with_testtools=n with_docs=n
</code></pre>

<p>With the compilation database in place, I ran the following scripts<sup id="fnref:tools"><a href="#fn:tools" rel="footnote">1</a></sup>, redirecting their output to create the result files:</p>

<pre><code class="language-bash">cc_cppcheck.sh -i common/c_cpp/src/c/ -i mama/c_cpp/src/c/ -c 
cc_clangcheck.sh -i common/c_cpp/src/c/ -i mama/c_cpp/src/c/ -c 
cc_clangtidy.sh -i common/c_cpp/src/c/ -i mama/c_cpp/src/c/ -c 
</code></pre>

<p>The results from running the tools on OpenMAMA can also be found in <a href="https://github.com/btorpey/static/tree/master/openmama">the repo</a>, along with a <code>compile_commands.json</code> file that can be used without the need to build OpenMAMA from source<sup id="fnref:mamabuild"><a href="#fn:mamabuild" rel="footnote">2</a></sup>.  To do that, use the following commands:</p>

<pre><code>cd [directory]
git clone https://github.com/OpenMAMA/OpenMAMA.git
git clone https://github.com/btorpey/static.git
export PATH=$(/bin/pwd)/static/scripts:$PATH
cp static/openmama/* OpenMAMA
cd OpenMAMA
cc_cppcheck.sh -i common/c_cpp/src/c/ -i mama/c_cpp/src/c/ -c 
</code></pre>

<p>I use the wonderful <a href="http://btorpey.github.io/blog/2013/01/29/beyond-compare/">Beyond Compare</a> to, well, compare the results from different tools.</p>

<h2 id="false-positives">False Positives</h2>
<p>Before we do anything else, let’s deal with the elephant in the room – false positives.  As in, warning messages for code that is actually perfectly fine.  Apparently, a lot of people have been burned by “lint”-type programs with terrible signal-to-noise ratios.  I know – I’ve been there too.</p>

<p>Well, let me be clear – these are not your father’s lints.  I’ve been running these tools on a lot of real-world code for a while now, and there are essentially  NO false positives.  If one of these tools complains about some code, there’s something wrong with it, and you really want to fix it.</p>

<h2 id="style-vs-substance">Style vs. Substance</h2>
<p>cppcheck includes a lot of “style” checks, although the term can be misleading  – there are a number of “style” issues that can have a significant impact on quality.</p>

<p>One of them crops up all over the place in OpenMAMA code, and that is the “The scope of the variable ‘&lt;name&gt;’ can be reduced” messages.  The reason for these is because of OpenMAMA’s insistence on K&amp;R-style variable declarations (i.e., all block-local variables must be declared before any executable statements).  Which, in turn, is caused by OpenMAMA’s decision to support several old and broken Microsoft compilers<sup id="fnref:vs"><a href="#fn:vs" rel="footnote">3</a></sup>.</p>

<p>The consensus has come to favor declaring variables as close to first use as possible, and that is part of the <a href="https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#es21-dont-introduce-a-variable-or-constant-before-you-need-to-use-it">C++ Core Guidelines</a>.  The only possible down-side to this approach is that it makes it easier to inadvertently declare “shadow” variables (i.e., variables with the same name in both inner and outer scopes), but modern compilers can flag shadow variables, which mitigates this potential problem (see my earlier article <a href="http://btorpey.github.io/blog/2015/03/17/shadow/">“Who Knows What Evil Lurks…”</a> for more).</p>

<p>Some other “style” warnings produced by cppcheck include:</p>

<blockquote>
  <p>[mama/c_cpp/src/c/bridge/qpid/transport.c:1413]: (style) Consecutive return, break, continue, goto or throw statements are unnecessary.</p>
</blockquote>

<p>These are mostly benign, but reflect a lack of understanding of what statements like <code>continue</code> and <code>return</code> do, and can be confusing.
<br />
<br /></p>

<blockquote>
  <p>[common/c_cpp/src/c/list.c:295 -&gt; common/c_cpp/src/c/list.c:298]: (style) Variable ‘rval’ is reassigned a value before the old one has been used.</p>
</blockquote>

<p>There are a lot of these in OpenMAMA, and most of them are probably caused by the unfortunate decision to standardize on K&amp;R-style local variable declarations, but in other cases this can point to a potential logic problem.  (Another good reason to avoid K&amp;R-style declarations).</p>

<p><br />
Similar, but potentially more serious is this one:</p>

<blockquote>
  <p>[mama/c_cpp/src/c/bridge/qpid/transport.c:275]: (style) Variable ‘next’ is assigned a value that is never used.</p>
</blockquote>

<p>Maybe the variable was used in an earlier version of the code, but is no longer needed.  Or maybe we ended up using the wrong variable when we mean to use <code>next</code>.</p>

<h2 id="dead-code">Dead Code</h2>

<p>There are also cases where the analyzer can determine that the code as written is meaningless</p>

<blockquote>
  <p>[mama/c_cpp/src/c/bridge/qpid/subscription.c:179]: (style) A pointer can not be negative so it is either pointless or an error to check if it is.</p>
</blockquote>

<p>If something cannot happen, there is little point to testing for it – so testing for impossible conditions is almost always a sign that something is wrong with the code.</p>

<p>Here are a few more of the same ilk:</p>

<blockquote>
  <p>[mama/c_cpp/src/c/dictionary.c:323]: (style) Checking if unsigned variable ‘*size’ is less than zero.</p>
</blockquote>

<!-- -->
<blockquote>
  <p>[mama/c_cpp/src/c/statslogger.c:731]: (style) Condition ‘status!=MAMA_STATUS_OK’ is always false</p>
</blockquote>

<!-- -->
<blockquote>
  <p>[mama/c_cpp/src/c/dqstrategy.c:543]: (style) Redundant condition: If ‘EXPR == 3’, the comparison ‘EXPR != 2’ is always true.</p>
</blockquote>

<p>Whether these warnings represent real bugs is a question that needs to be answered on a case-by-case basis, but I hope we can agree that they at the very least represent a <a href="https://en.wikipedia.org/wiki/Code_smell">“code smell”</a>, and the fewer of these in our code, the better.</p>

<h2 id="buffer-overflow">Buffer Overflow</h2>
<p>There are bugs, and there are bugs, but bugs that have a “delayed reaction”, are arguably the worst, partly because they can be so hard to track down.  Buffer overflows are a major cause of these kinds of bugs – a buffer overflow can trash return addresses on the stack causing a crash, or worse they can alter the program’s flow in ways that seem completely random.</p>

<p>Here’s an example of a buffer overflow in OpenMAMA that was detected by cppcheck:</p>

<blockquote>
  <p>[common/c_cpp/src/c/strutils.c:632]: (error) Array ‘version.mExtra[16]’ accessed at index 16, which is out of bounds.</p>
</blockquote>

<p>Here’s the offending line of code:</p>

<pre><code>    version-&gt;mExtra[VERSION_INFO_EXTRA_MAX] = '\0';
</code></pre>

<p>And here’s the declaration:</p>

<pre><code class="language-c">    char mExtra[VERSION_INFO_EXTRA_MAX];
</code></pre>

<p>It turns out that this particular bug was fixed subsequent to the release – the bug report is <a href="https://github.com/OpenMAMA/OpenMAMA/pull/310">here</a>.  Interestingly, the bug report mentions that the bug was found using clang’s Address Sanitizer,  which means that code must have been executed to expose the bug.     Static analyzers like cppcheck can detect this bug without the need to run the code, which is a big advantage of static analysis.  In this example, cppcheck can tell at compile-time that the access is out-of-bounds, since it knows the size of mExtra.</p>

<p>Of course, a static analyzer like cppcheck can’t detect <em>all</em> buffer overflows – just the ones that can be evaluated at compile-time.  So, we still need Address Sanitizer, or valgrind, or some other run-time analyzer, to detect overflows that depend on the run-time behavior of the program.  But I’ll take all the help I can get, and detecting at least some of these nasty bugs at compile-time is a win.</p>

<h2 id="null-pointer-dereference">NULL pointer dereference</h2>
<p>In contrast to the buffer overflow type of problem, dereferencing a NULL pointer is not mysterious at all – you’re going down hard, right now.</p>

<p>So, reasonable programmers insert checks for NULL pointers, but reasonable is not the same as perfect, and sometimes we get it wrong.</p>

<blockquote>
  <p>[mama/c_cpp/src/c/msg.c:3619] -&gt; [mama/c_cpp/src/c/msg.c:3617]: (warning, inconclusive) Either the condition ‘!impl’ is redundant or there is possible null pointer dereference: impl.</p>
</blockquote>

<p>Here’s a snip of the code in question – see if you can spot the problem:</p>

<pre><code class="language-c">3613    mamaMsgField
3614    mamaMsgIterator_next (mamaMsgIterator iterator)
3615    {
3616        mamaMsgIteratorImpl* impl         = (mamaMsgIteratorImpl*)iterator;
3617        mamaMsgFieldImpl*    currentField = (mamaMsgFieldImpl*) impl-&gt;mCurrentField;
3618
3619        if (!impl)
3620            return (NULL);
</code></pre>

<p>cppcheck works similarly to other static analyzers when checking for possible NULL pointer dereference – it looks to see if a pointer is checked for NULL, and if it is, looks for code that dereferences the pointer outside the scope of that check.</p>

<p>In this case, the code checks for <code>impl</code> being NULL, but not until it has already dereferenced the pointer.  cppcheck even helpfully ties together the check for NULL and the (earlier) dereference. (Ahem – yet another reason to avoid K&amp;R-style declarations).</p>

<h2 id="leaks">Leaks</h2>
<p>Similarly to checking for NULL pointers, detecting leaks is more of a job for valgrind, Address Sanitizer or some other run-time analysis tool.  However, that doesn’t mean that static analysis can’t give us a head-start on getting rid of our leaks.</p>

<p>For instance, cppcheck has gotten quite clever about being able to infer run-time behavior at compile-time, as in this example:</p>

<blockquote>
  <p>[mama/c_cpp/src/c/transport.c:269]: (error) Memory leak: transport
<br />
[mama/c_cpp/src/c/transport.c:278]: (error) Memory leak: transport</p>
</blockquote>

<p>Here’s the code:</p>

<pre><code class="language-c">253 mama_status
254 mamaTransport_allocate (mamaTransport* result)
255 {
256     transportImpl*  transport    =   NULL;
257     mama_status     status       =   MAMA_STATUS_OK;
258
259
260     transport = (transportImpl*)calloc (1, sizeof (transportImpl ) );
261     if (transport == NULL)  return MAMA_STATUS_NOMEM;
262
263     /*We need to create the throttle here as properties may be set
264      before the transport is actually created.*/
265     if (MAMA_STATUS_OK!=(status=wombatThrottle_allocate (&amp;self-&gt;mThrottle)))
266     {
267         mama_log (MAMA_LOG_LEVEL_ERROR, "mamaTransport_allocate (): Could not"
268                   " create throttle.");
269         return status;
270     }
271
272     wombatThrottle_setRate (self-&gt;mThrottle,
273                            MAMA_DEFAULT_THROTTLE_RATE);
274
275     if (MAMA_STATUS_OK !=
276        (status = wombatThrottle_allocate (&amp;self-&gt;mRecapThrottle)))
277     {
278         return status;
279     }
280
281     wombatThrottle_setRate (self-&gt;mRecapThrottle,
282                             MAMA_DEFAULT_RECAP_THROTTLE_RATE);
283
284     self-&gt;mDescription          = NULL;
285     self-&gt;mLoadBalanceCb        = NULL;
286     self-&gt;mLoadBalanceInitialCb = NULL;
287     self-&gt;mLoadBalanceHandle    = NULL;
288     self-&gt;mCurTransportIndex    = 0;
289     self-&gt;mDeactivateSubscriptionOnError = 1;
290     self-&gt;mGroupSizeHint        = DEFAULT_GROUP_SIZE_HINT;
291     *result = (mamaTransport)transport;
292
293     self-&gt;mName[0] = '\0';
294
295     return MAMA_STATUS_OK;
296 }
</code></pre>

<p>cppcheck is able to determine that the local variable <code>transport</code> is never assigned in the two early returns, and thus can never be freed.
<br /></p>

<p>Not to be outdone, clang-tidy is doing some kind of flow analysis that allows it to catch this one:</p>

<blockquote>
  <p>[mama/c_cpp/src/c/queue.c:778]: warning: Use of memory after it is freed</p>
</blockquote>

<p>Here’s a snip of the code that clang-tidy is complaining about:</p>

<pre><code class="language-c">651 mama_status
652 mamaQueue_destroy (mamaQueue queue)
653 {
654     mamaQueueImpl* impl = (mamaQueueImpl*)queue;
655     mama_status    status = MAMA_STATUS_OK;
...
776         free (impl);
777
778         mama_log (MAMA_LOG_LEVEL_FINEST, "Leaving mamaQueue_destroy for queue 0x%X.", queue);
779         status = MAMA_STATUS_OK;
780     }
781
782    return status;
783 }
</code></pre>

<p>clang-tidy understands that <code>queue</code> and <code>impl</code> are aliases for the same variable, and thus knows that it is illegal to access <code>queue</code> after <code>impl</code> has been freed.  In this case, the access causes no problems, because we’re only printing the address, but clang-tidy can’t know that<sup id="fnref:interproc"><a href="#fn:interproc" rel="footnote">4</a></sup>.</p>

<h2 id="pointer-errors">Pointer Errors</h2>
<p>I’ve <del>ranted</del> written <a href="http://btorpey.github.io/blog/2014/09/23/into-the-void/">before</a> on how much I hate <code>void*</code>’s.  For better or worse, the core OpenMAMA code is written in C, so there are a whole bunch of casts between <code>void*</code>s and “real” pointers that have the purpose of encapsulating the internal workings of the internal objects managed by the code.</p>

<p>In C this is about the best that can be done, but it can be hard to keep things straight, which can be a source of errors (like this one):</p>

<blockquote>
  <p>[mama/c_cpp/src/c/fielddesc.c:76]: (warning) Assignment of function parameter has no effect outside the function. Did you forget dereferencing it?</p>
</blockquote>

<p>And here’s the code:</p>

<pre><code class="language-c">65  mama_status
66  mamaFieldDescriptor_destroy (mamaFieldDescriptor descriptor)
67  {
68      mamaFieldDescriptorImpl* impl = (mamaFieldDescriptorImpl*) descriptor;
69
70      if (impl == NULL)
71          return MAMA_STATUS_OK;
72
73      free (impl-&gt;mName);
74      free (impl);
75
76      descriptor = NULL;
77      return MAMA_STATUS_OK;
78  }
</code></pre>

<p>Of course <code>mamaFieldDescriptor</code> is defined as a <code>void*</code>, so it’s perfectly OK to set it to NULL, but since it’s passed by value, the assignment has no effect other than to zero out the copy of the parameter on the stack.</p>

<h2 id="but-wait-theres-more">But Wait, There’s More!</h2>
<p>The preceding sections go into detail about specific examples of serious errors detected by cppcheck and clang.  But, these are very much the tip of the iceberg.</p>

<p>Some of the other problems detected include:</p>

<ul>
  <li>use of non-reentrant system functions (e.g., <code>strtok</code>) in multi-threaded code;</li>
  <li>use of obsolete functions (e.g., <code>gethostbyname</code>);</li>
  <li>incorrect usage of <code>printf</code>-style functions;</li>
  <li>incorrect usage of <code>strcpy</code>-style functions (e.g., leaving strings without terminating NULL characters);</li>
  <li>incorrect usage of varargs functions;</li>
  <li>different parameter names in function declarations vs. definitions;</li>
</ul>

<p>Some of these are nastier than others, but they are all legitimate problems and should be fixed.</p>

<p>The full results for both tools are available in the <a href="https://github.com/btorpey/static/tree/master/openmama">GitHub repo</a>, so it’s easy to compare the warnings against the code.</p>

<h2 id="conclusion">Conclusion</h2>
<p>The state of the art in static analysis keeps advancing, thanks to people like Daniel Marjamäki and the rest of the <a href="https://github.com/danmar/cppcheck/graphs/contributors">cppcheck team</a>, and Gábor Horváth and the <a href="https://github.com/llvm-mirror/clang/graphs/contributors">team supporting clang</a>.</p>

<p>In particular, the latest releases of cppcheck and clang-tidy are detecting errors that previously could only be found by run-time analyzers like valgrind and Address Sanitizer.  This is great stuff, especially given how easy it is to run static analysis on your code.</p>

<p>The benefits of using one (or more) static analysis tools just keep getting more and more compelling – if you aren’t using one of these tools, I hope this will encourage you to do so.</p>

<p>If you found this article interesting or helpful, you might want to also check out the other posts in <a href="http://btorpey.github.io/blog/categories/static-analysis/">this series</a>.  And please leave a comment below or <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#119;&#097;&#108;&#108;&#115;&#116;&#112;&#114;&#111;&#103;&#064;&#103;&#109;&#097;&#105;&#108;&#046;&#099;&#111;&#109;">drop me a line</a> with any questions, suggestions, etc.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes">
  <ol>
    <li id="fn:tools">
      <p>Simply clone the <a href="https://github.com/btorpey/static">GitHub repo</a> to any directory, and then add the <code>scripts</code> directory to your <code>PATH</code>.<a href="#fnref:tools" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:mamabuild">
      <p>OpenMAMA has its share of prerequisites – you can get a full list <a href="https://openmama.github.io/openmama_build_instructions.html">here</a>.<a href="#fnref:mamabuild" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:vs">
      <p>The list of supported platforms for OpenMAMA is <a href="https://openmama.github.io/openmama_supported_platforms.html">here</a>.  You can also find a lot of griping on the intertubes about Microsoft’s refusal to support C99, including <a href="https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/suggestions/2089423-c99-support">this rather weak response</a> from Herb Sutter.  Happily, VS 2013 ended up supporting (most of) C99. <a href="#fnref:vs" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:interproc">
      <p>Unless it knows what <code>mama_log</code> does.  It turns out that clang-tidy can do inter-procedural analysis, but only within a single translation unit.  There is some work ongoing to add support for analysis across translation units by Gábor Horvath et al. – for more see <a href="http://llvm.org/devmtg/2017-03//2017/02/20/accepted-sessions.html#7">“Cross Translational Unit Analysis in Clang Static Analyzer: Prototype and Measurements”</a>.<a href="#fnref:interproc" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[We Don't Need No Stinkin' Databases]]></title>
    <link href="http://btorpey.github.io/blog/2017/05/10/join/"/>
    <updated>2017-05-10T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2017/05/10/join</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/Gold_Hat_portrayed_by_Alfonso_Bedoya.jpg" width="220" height="162" /></p>

<p>I’ve been working on performance analysis recently, and a large part of that is scraping log files to capture interesting events and chart them.</p>

<p>I’m continually surprised by the things that you can do using plain old bash and his friends, but this latest one took the cake for me.</p>

<!-- more -->

<p>Did you know that Linux includes a utility named <code>join</code>?  Me neither.  Can you guess what it does?  Yup, that’s right – it does the equivalent of a database join across plain text files.</p>

<p>Let me clarify that with a real-world example – one of the datasets I’ve been analyzing counts the number of messages sent and received in a format roughly like this:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Timestamp</th>
      <th style="text-align: right">Recv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">HH:MM:SS</td>
      <td style="text-align: right">x</td>
    </tr>
  </tbody>
</table>

<p><br />
Complicating matters is that sent and received messages are parsed out separately, so we also have a separate file that looks like this:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Timestamp</th>
      <th style="text-align: right">Send</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">HH:MM:SS</td>
      <td style="text-align: right">y</td>
    </tr>
  </tbody>
</table>

<p><br />
But what we really want is something like this:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Timestamp</th>
      <th style="text-align: right">Recv</th>
      <th style="text-align: right">Send</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">HH:MM:SS</td>
      <td style="text-align: right">x</td>
      <td style="text-align: right">y</td>
    </tr>
  </tbody>
</table>

<p><br />
Here are snips from the two files:</p>

<pre><code>$ cat recv.txt
Timestamp	Recv
2016/10/25-16:04:58	7
2016/10/25-16:04:59	1
2016/10/25-16:05:00	7
2016/10/25-16:05:01	9
2016/10/25-16:05:28	3
2016/10/25-16:05:31	9
2016/10/25-16:05:58	3
2016/10/25-16:06:01	9
2016/10/25-16:06:28	3
$ cat send.txt
Timestamp	Send
2016/10/25-16:04:58	6
2016/10/25-16:05:01	18
2016/10/25-16:05:28	3
2016/10/25-16:05:31	9
2016/10/25-16:05:58	3
2016/10/25-16:06:01	9
2016/10/25-16:06:28	3
2016/10/25-16:06:31	9
2016/10/25-16:06:58	3
</code></pre>

<p>I had stumbled across the <code>join</code> command and thought it would be a good way to combine the two files.</p>

<p>Doing a simple join with no parameters gives this:</p>

<pre><code>$ join recv.txt send.txt
Timestamp Recv Send
2016/10/25-16:04:58 7 6
2016/10/25-16:05:01 9 18
2016/10/25-16:05:28 3 3
2016/10/25-16:05:31 9 9
2016/10/25-16:05:58 3 3
2016/10/25-16:06:01 9 9
2016/10/25-16:06:28 3 3
</code></pre>

<p>As you can see, we’re missing some of the measurements.  This is because by default <code>join</code> does an <a href="https://en.wikipedia.org/wiki/Join_(SQL)#Inner_join">inner join</a> of the two files (the intersection, in set theory).</p>

<p>That’s OK, but not really what we want.  We really need to be able to reflect each value from both datasets, and for that we need an <a href="https://en.wikipedia.org/wiki/Join_(SQL)#Outer_join">outer join</a>, or union.</p>

<p>It turns out that <code>join</code> can do that too, although the syntax is a bit more complicated:</p>

<pre><code>$ join -t $'\t' -o 0,1.2,2.2 -a 1 -a 2 recv.txt send.txt
Timestamp	Recv	Send
2016/10/25-16:04:58	7	6
2016/10/25-16:04:59	1
2016/10/25-16:05:00	7
2016/10/25-16:05:01	9	18
2016/10/25-16:05:28	3	3
2016/10/25-16:05:31	9	9
2016/10/25-16:05:58	3	3
2016/10/25-16:06:01	9	9
2016/10/25-16:06:28	3	3
2016/10/25-16:06:31		9
2016/10/25-16:06:58		3
</code></pre>

<p>A brief run-down of the parameters is probably in order:</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>-t $'\t'</code></td>
      <td>The <code>-t</code> parameter tells <code>join</code> what to use as the separator between fields.  The tab character is the best choice, as most Unix utilities assume that by default, and both Excel and Numbers can work with tab-delimited files.<br />The leading dollar-sign is a <a href="https://unix.stackexchange.com/a/46931/198530">trick</a> used to to pass a literal tab character on the command line  .</td>
    </tr>
    <tr>
      <td><code>-o 0,1.2,2.2</code></td>
      <td>Specifies which fields to output.  In this case, we want the “join field” (in this case, the first field from both files), then the second field from file #1, then the second field from file #2.</td>
    </tr>
    <tr>
      <td><code>-a 1</code></td>
      <td>Tells <code>join</code> that we want <strong>all</strong> the fields from file #1.</td>
    </tr>
    <tr>
      <td><code>-a 2</code></td>
      <td>Ditto for file #2.</td>
    </tr>
  </tbody>
</table>

<p><br />
As you can probably see, you can also get fancy and do things like left outer joins and right outer joins, depending on the parameters passed.</p>

<p>Of course, you could easily import these text files into a “real” database and generate reports that way.  But, you can accomplish a surprising amount of data manipulation and reporting with Linux’s built-in utilities and plain old text files.</p>

<h3 id="acknowledgements">Acknowledgements</h3>
<p>I couldn’t remember where I had originally seen the <code>join</code> command, but recently found it again in a <a href="http://ablagoev.github.io/linux/adventures/commands/2017/02/19/adventures-in-usr-bin.html">nice post by Alexander Blagoev</a>.  Check it out for even more obscure commands!  And, thanks Alexander!  </p>

<p>And thanks also to Igor for his own <a href="http://shiroyasha.io/coreutils-that-you-might-not-know.html">very nice post</a> that led  me back to Alexander’s.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Even Mo' Static]]></title>
    <link href="http://btorpey.github.io/blog/2016/11/12/even-mo-static/"/>
    <updated>2016-11-12T00:00:00-05:00</updated>
    <id>http://btorpey.github.io/blog/2016/11/12/even-mo-static</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/vandergraaf.jpg" width="139" height="122" /></p>

<ul id="markdown-toc">
  <li><a href="#tldr">TL;DR</a></li>
  <li><a href="#methodology">Methodology</a>    <ul>
      <li><a href="#itc-test-suite">ITC test suite</a>        <ul>
          <li><a href="#caveats">Caveats</a>            <ul>
              <li><a href="#specific-issues">Specific issues</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#real-world-test-results">Real-world test results</a></li>
  <li><a href="#false-positives">False positives</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#appendix-helper-scripts-and-sample-results">Appendix: Helper scripts and sample results</a></li>
  <li><a href="#appendix-detailed-test-results">Appendix: Detailed test results</a></li>
</ul>

<p>A while back I wrote <a href="http://btorpey.github.io/blog/2016/04/07/mo-static/">an article</a> that compared cppcheck and clang’s static analyzers (clang-check and clang-tidy).  The folks who make <a href="http://www.viva64.com/en/pvs-studio/">PVS-Studio</a> (the guys with the unicorn mascot that you’ve probably been seeing a lot of lately) saw the article, and suggested that I take a look at their Linux port, which was then in beta test, and write about it.</p>

<p>So I did.  Read on for an overview of PVS-Studio, and how it compared to <a href="http://cppcheck.sourceforge.net/">cppcheck</a>.</p>

<!-- more -->

<p>In <a href="http://btorpey.github.io/blog/2016/04/07/mo-static/">the earlier article</a>, I used a <a href="https://github.com/regehr/itc-benchmarks">benchmark suite</a> developed by Toyota ITC, and written about by <a href="http://blog.regehr.org/archives/1217">John Regehr</a>, who is a professor of Computer Science at the University of Utah.  The ITC suite consists of code that is specially written to exhibit certain errors that can be detected by static analysis, so that the code can be used to evaluate the performance of different tools.</p>

<p>In this article, I am going to use the same test suite to evaluate  PVS-Studio, and to compare it against cppcheck.  I’ll also talk about my experience using both tools to analyze two relatively large real-world codebases that I help maintain as part of my day job.</p>

<h2 id="tldr">TL;DR</h2>
<p>Using any static analysis tool is better than using none, and in general the more the merrier.  Each tool has its own design philosophy, and corresponding strengths and weaknesses.</p>

<p>Daniel Marjamäki<sup id="fnref:daniel"><a href="#fn:daniel" rel="footnote">1</a></sup> and the maintainers of <a href="http://cppcheck.sourceforge.net/">cppcheck</a> have done a terrific job creating a free tool that can go head-to-head with expensive commercial offerings.  You can’t go wrong with cppcheck, either as a gentle introduction to static analysis, or as the one-and-only tool for the budget-conscious.  But don’t take my word for it – the Debian project uses cppcheck as part of its <a href="https://qa.debian.org/daca/">Debian Automated Code Analysis</a> project to check over 100GB of C++ source code.</p>

<p><a href="http://www.viva64.com/en/pvs-studio/">PVS-Studio</a> is also a terrific tool, but it is definitely <em>not</em> free.  (When a product <a href="http://www.viva64.com/en/order/">doesn’t have published prices</a>, you know it’s going to cost serious money).</p>

<p>Whether PVS-Studio is worth the price is a judgement call, but if it can find just one bug that would have triggered a crash in production it will have paid for itself many times over. </p>

<p>And while PVS-Studio doesn’t appear to have been adopted by a high-profile project like Debian, the folks who make it are certainly not shy about running various open-source projects through their tool and <a href="http://www.viva64.com/en/inspections/">reporting the results</a>.  </p>

<p>So, if your budget can handle it, use both.  If money is a concern, then you may want to start out with cppcheck and use that to help build a case for spending the additional coin that it will take to include commercial tools like PVS-Studio in your toolbox.</p>

<p>Note also that PVS-Studio offers a trial version<sup id="fnref:free"><a href="#fn:free" rel="footnote">2</a></sup>, so you can give it a go on your own code, which is, after all, the best way to see what the tool can do.  And, if you use the provided <a href="http://btorpey.github.io/pages/REAME.md/index.html">helper scripts</a> (<a href="https://github.com/btorpey/static">repo here</a>), your results will be in a format that makes it easy to compare the tools.</p>

<h2 id="methodology">Methodology</h2>
<p>In comparing cppcheck and PVS-Studio, I used the ITC test suite that I wrote about in an <a href="http://btorpey.github.io/blog/2016/04/07/mo-static/">earlier article</a>.  I also used both tools to analyze real-world code bases which I deal with on a day-to-day basis and that I am intimately familiar with.</p>

<h3 id="itc-test-suite">ITC test suite</h3>
<p>The ITC test suite that I’ve been using to compare static analyzers is intended to provide a common set of source files that can be used as input to various static analysis tools.  It includes both real errors, as well as “false positives” intended to trick the tools into flagging legitimate code as an error.</p>

<p>So far, so good, and it’s certainly very helpful to know where the errors are (and are not) when evaluating a static analysis tool.  </p>

<h4 id="caveats">Caveats</h4>
<p>In my email discussion with Andrey Karpov of PVS, he made the point that not all bugs are equal, and that a “checklist” approach to comparing static analyzers may not be the best.  I agree, but being able to compare analyzers on the same code-base can be very helpful, not least for getting a feel for how the tools work.</p>

<p>Your mileage can, and will, vary, so it makes sense to get comfortable with different tools and learn what each does best.  And there’s no substitute for running the tools on your own code.  (The <a href="http://btorpey.github.io/pages/REAME.md/index.html">helper scripts</a> (<a href="https://github.com/btorpey/static">repo here</a>) may, well, help).</p>

<h5 id="specific-issues">Specific issues</h5>
<p>The ITC test suite includes some tests for certain categories of errors that are more likely to manifest themselves at run-time, as opposed to compile-time.    </p>

<p>For instance, the ITC suite includes a relatively large number of test cases designed to expose memory-related problems.  These include problems like leaks, double-free’s, dangling pointers, etc.</p>

<p>That’s all very nice, but in the real world memory errors are often not that clear-cut, and depend on the dynamic behavior of the program at run-time.  Both valgrind’s <a href="http://valgrind.org/info/tools.html#memcheck">memcheck</a> and clang’s <a href="http://clang.llvm.org/docs/AddressSanitizer.html">Address Sanitizer</a> do an excellent job of detecting memory errors at run-time, and I use both regularly.</p>

<p>But run-time analyzers can only analyze code that actually runs, and memory errors can hide for quite a long time in code that is rarely executed (e.g., error &amp; exception handlers). So, even though not all memory errors can be caught at compile-time, the ability to detect at least some of them can very helpful.  </p>

<p>A similar situation exists with regard to concurrency (threading) errors – though in this case neither tool detects <em>any</em> of the concurrency-related errors seeded in the ITC code.  This is, I think, a reasonable design decision  – the subset of threading errors that can be detected at compile-time is so small that it’s not really worth doing (and could give users of the tool a false sense of security).  For concurrency errors, you again will probably be better off with something like clang’s <a href="http://clang.llvm.org/docs/ThreadSanitizer.html">Thread Sanitizer</a> or valgrind’s <a href="http://valgrind.org/info/tools.html#drd">Data Race Detector</a>.</p>

<p>Also, in the interest of full disclosure, I have spot-checked some of the ITC code, but by no means all, to assure myself that its diagnostics were reasonable. </p>

<p>With those caveats out of the way, though, the ITC test suite does provide at least a good starting point towards a comprehensive set of test cases that can be used to exercise different static analyzers.</p>

<p>The results of running PVS-Studio (and other tools) against the ITC code can be found in the <a href="https://github.com/btorpey/static/tree/master/samples">samples directory of the repo</a>.</p>

<h2 id="real-world-test-results">Real-world test results</h2>
<p>I also ran both cppcheck and PVS-Studio on the code bases that I maintain as part of my day job, to get an idea of how the tools compare in more of a real-world situation.  While I can’t share the detailed comparisons, following are some of the major points.</p>

<p>For the most part, both cppcheck and PVS-Studio reported similar warnings on the same code, with a few exceptions (listed following). </p>

<p>cppcheck arguably does a better job of flagging “style” issues – and while some of these warnings are perhaps a bit nit-picky, many are not:</p>

<ul>
  <li>one-argument ctor’s not marked <code>explicit</code> </li>
  <li>functions that can/should be declared <code>static</code> or <code>const</code></li>
  <li>use of post-increment on non-primitive types </li>
  <li>use of obsolete or deprecated functions</li>
  <li>use of C-style casts</li>
</ul>

<p>PVS-Studio, on the other hand, appears to include more checks for issues that aren’t necessarily problems with the use of C++ per se, but things that would be a bug, or at least a “code smell”, in any language.</p>

<p>A good example of that is PVS-Studio’s warning on similar or identical code sequences (potentially indicating use of the copy-paste anti-pattern – I’ve written about that <a href="http://btorpey.github.io/blog/2014/09/21/repent/">before</a>).</p>

<p>Some other PVS-Studio “exclusives” include: </p>

<ul>
  <li>classes that define a copy ctor without <code>operator=</code>, and vice-versa</li>
  <li>potential floating-point problems<sup id="fnref:float"><a href="#fn:float" rel="footnote">3</a></sup>, e.g., comparing floating-point values for an exact match using <code>==</code></li>
  <li>empty <code>catch</code> clauses</li>
  <li>catching exceptions by value rather than by reference</li>
</ul>

<p>Both tools did a good job of identifying potentially suspect code, as well as areas where the code could be improved.</p>

<h2 id="false-positives">False positives</h2>
<p>False positives (warnings on code that is actually correct) are not really a problem with either cppcheck or PVS-Studio.  The few warnings that could be classified as false positives indicate code that is at the very least suspect – in most cases you’re going to want to change the code anyway, if only to make it clearer.</p>

<p>If you still get more false positives than you can comfortably deal with, or if you want to stick with a particular construct even though it may be suspect, both tools have mechanisms to suppress individual warnings, or whole classes of errors.  Both tools are also able to silence warnings either globally, or down to the individual line of code, based on inline comments.</p>

<h2 id="conclusion">Conclusion</h2>
<p>If you care about building robust, reliable code in C++ then you would be well-rewarded to include static analysis as part of your development work-flow.  </p>

<p>Both <a href="http://www.viva64.com/en/pvs-studio/">PVS-Studio</a> and <a href="http://cppcheck.sourceforge.net/">cppcheck</a> do an excellent job of identifying potential problems in your code.  It’s almost like having another set of eyeballs to do code review, but with the patience to trace through all the possible control paths, and with a voluminous knowledge of the language, particularly the edge cases and “tricky bits”.</p>

<p>Having said that, I want to be clear that static analysis is not a substitute for the dynamic analsyis provided by tools like valgrind’s <a href="http://valgrind.org/info/tools.html#memcheck">memcheck</a> and <a href="http://valgrind.org/info/tools.html#drd">Data Race Detector</a>, or clang’s <a href="http://clang.llvm.org/docs/AddressSanitizer.html">Address Sanitizer</a> and <a href="http://clang.llvm.org/docs/ThreadSanitizer.html">Thread Sanitizer</a>.  You’ll want to use them too, as there are certain classes of bugs that can only be detected at run-time.</p>

<p>I hope you’ve found this information helpful.  If you have, you may want to check out some of my earlier articles, including:</p>

<ul>
  <li><a href="http://btorpey.github.io/blog/2016/04/07/mo-static/">Mo’ Static</a></li>
  <li><a href="http://btorpey.github.io/blog/2015/04/27/static-analysis-with-clang/">Static Analysis with clang</a></li>
  <li><a href="http://btorpey.github.io/blog/2014/03/27/using-clangs-address-sanitizer/">Using clang’s Address Sanitizer</a></li>
  <li><a href="http://btorpey.github.io/blog/2015/03/17/shadow/">Who Knows what Evil Lurks…</a></li>
</ul>

<p>Last but not least, please feel free to <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#119;&#097;&#108;&#108;&#115;&#116;&#112;&#114;&#111;&#103;&#064;&#103;&#109;&#097;&#105;&#108;&#046;&#099;&#111;&#109;">contact me</a> directly, or post a comment below, if you have questions or something to add to the discussion.</p>

<h2 id="appendix-helper-scripts-and-sample-results">Appendix: Helper scripts and sample results</h2>

<p>I’ve posted the <a href="http://btorpey.github.io/pages/REAME.md/index.html">helper scripts</a> I used to run PVS-Studio, as well as the results of running those scripts on the ITC code, in the <a href="https://github.com/btorpey/static">repo</a>.</p>

<h2 id="appendix-detailed-test-results">Appendix: Detailed test results</h2>

<p>The following sections describe a subset of the tests in the ITC code and how both tools respond to them.</p>

<h3 class="no_toc" id="bit-shift-errors">Bit Shift errors</h3>
<p>For the most part, PVS-Studio and cpphceck both do a good job of detecting errors related to bit shifts. Neither tool detects all the errors seeded in the benchmark code, although they miss different errors.</p>

<h3 class="no_toc" id="buffer-overrununderrun-errors">Buffer overrun/underrun errors</h3>
<p>cppcheck appears to do a more complete job than PVS-Studio of detecting buffer overrrun and underrun errors, although it is sometimes a bit “off” – reporting errors on lines that are in the vicinity of the actual error, rather than on the actual line.  cppcheck also reports calls to functions that generate buffer errors, which is arguably redundant, but does no harm.</p>

<p>PVS-Studio catches some of the seeded errors, but misses several that cppcheck detects.</p>

<p>While not stricly speaking an overrun error, cppcheck can also detect some errors where code overwrites the last byte in a null-terminated string.</p>

<h3 class="no_toc" id="conflictingredundant-conditions">Conflicting/redundant conditions</h3>
<p>Both cppcheck and PVS-Studio do a good job of detecting conditionals that always evaluate to either true or false, with PVS-Studio being a bit better at detecting complicated conditions composed of contstants.</p>

<p>On the other hand, cppcheck flags redundant conditions (e.g., <code>if (i&lt;5 &amp;&amp; i&lt;10)</code>), which PVS-Studio doesn’t do. </p>

<h3 class="no_toc" id="loss-of-integer-precision">Loss of integer precision</h3>
<p>Surprisingly, neither tool does a particularly good job of detecting loss of integer precision (the proverbial “ten pounds of bologna in a five-pound sack” problem ;-)</p>

<h4 class="no_toc" id="assignments">Assignments</h4>
<p>I say surprisingly because these kinds of errors would seem to be relatively easy to detect.  Where both tools seem to fall short is to assume that just because a value fits in the target data type, the assignment is valid – but they fail to take into account that such an assignment can lose precision.</p>

<p>I wanted to convince myself that the ITC code was correct, so I pasted some of the code into a small test program:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span> (test1.c)</span> <a href="http://btorpey.github.io/downloads/code/static/pvs/test1.c">download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="cp">#include &lt;stdio.h&gt;</span>
</span><span class="line">
</span><span class="line"><span class="kt">int</span> <span class="n">sink</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="kt">void</span> <span class="nf">data_lost_001</span> <span class="p">()</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">	<span class="kt">char</span> <span class="n">ret</span><span class="p">;</span>
</span><span class="line">	<span class="kt">short</span> <span class="n">a</span> <span class="o">=</span> <span class="mh">0x80</span><span class="p">;</span>
</span><span class="line">	<span class="n">ret</span> <span class="o">=</span> <span class="n">a</span><span class="p">;</span><span class="cm">/*Tool should detect this line as error*/</span> <span class="cm">/*ERROR:Integer precision lost because of cast*/</span>
</span><span class="line">        <span class="n">sink</span> <span class="o">=</span> <span class="n">ret</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">   <span class="n">data_lost_001</span><span class="p">();</span>
</span><span class="line">   <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Value of sink=%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">sink</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>When you run this program, you’ll get the following output:</p>

<pre><code>$ gcc test1.c &amp;&amp; ./a.out
Value of sink=-128
</code></pre>

<p>So, <code>a</code> has the value 128, but when <code>a</code> is assigned to the (signed) char <code>ret</code>,  the bit pattern <code>0x80</code> is interpreted in the context of a (signed) char, and the sign is lost.  If <code>ret</code> had been declared as an unsigned char, then the assigment would not lose the sign of <code>a</code>.</p>

<h4 class="no_toc" id="arithmetic-expressions">Arithmetic expressions</h4>
<p>cppcheck does do a slightly better job of detecting integer overflow and underflow in arithmetic expressions compared to PVS, but still misses a number of seeded errors.</p>

<h4 class="no_toc" id="divide-by-zero">Divide by zero</h4>
<p>Both PVS-Studio and cppcheck do a good job of catching potential divide-by-zero errors, with cppcheck having a slight edge. </p>

<h3 class="no_toc" id="dead-code">Dead code</h3>
<p>PVS-Studio tends to do a somewhat better job than cppcheck at detecting various types of dead code, such as <code>for</code> loops and <code>if</code> statements where the condition will never be true.</p>

<p>PVS-Studio also very helpfully flags any unconditional <code>break</code> statements in a loop – these are almost always going to be a mistake.</p>

<h3 class="no_toc" id="concurrency">Concurrency</h3>
<p>As mentioned above, neither tool detects <em>any</em> of the concurrency-related errors seeded in the ITC code.  Again, I regard that as a reasonable design choice, given the relatively small percentage of such errors that can be detected at compile-time.</p>

<h3 class="no_toc" id="memory-errors">Memory Errors</h3>
<p>As discussed earlier, not all memory errors can be detected at compile-time, so the lack of any error output certainly doesn’t mean that the code doesn’t have memory errors – it just means that they can’t be detected by the tools. But while many memory errors cannot be detected at compile-time, for those that can be, detecting them is a big win.</p>

<h4 class="no_toc" id="double-free">Double free</h4>
<p>cppcheck does an excellent job of detecting double-free errors (11 out of 12), while PVS-Studio only flags one of the seeded errors.</p>

<h4 class="no_toc" id="free-ing-non-allocated-memory">Free-ing non-allocated memory</h4>
<p>On the other hand, PVS-Studio does a better job of detecting attempts to free memory that was not allocated dynamically (e.g., local variables).  </p>

<h4 class="no_toc" id="freeing-a-null-pointer">Freeing a NULL pointer</h4>
<p>Neither tool does a particularly good job of catching these.  Perhaps that is because freeing a NULL pointer is actually not an error, but doing so is certainly a clue that the code may have other problems.</p>

<h4 class="no_toc" id="dangling-pointers">Dangling pointers</h4>
<p>cppcheck does a somewhat better job of detecting the use of dangling pointers (where the pointed-to object has already been freed).</p>

<h4 class="no_toc" id="allocation-failures">Allocation failures</h4>
<p>If you’re writing code for an embedded system, then checking for and handling allocation failures can be important, because your application is likely written to expect them, and do something about them.  But more commonly, running out of memory simply means that you’re screwed, and attempting to deal with the problem is unlikely to make things better.</p>

<p>Neither tool detects code that doesn’t handle allocation failures, but cppcheck does flag some allocation-related problems (as leaks, which is not correct, but it is a clue that there is a problem lurking).</p>

<h4 class="no_toc" id="memory-leaks">Memory Leaks</h4>
<p>Typically, memory leaks are only evident at run-time, but there are some cases where they can be detected at compile-time, and in those cases cppcheck does a pretty good job. </p>

<h4 class="no_toc" id="null-pointer">Null pointer</h4>
<p>Both PVS-Studio and cppcheck do a good job of flagging code that dereferences a NULL pointer, although neither tool catches all the errors in the benchmark code.</p>

<h4 class="no_toc" id="returning-a-pointer-to-a-local-variable">Returning a pointer to a local variable</h4>
<p>Both PVS-Studio and cppcheck detect returning a pointer to a local variable that is allocated on the stack.</p>

<h4 class="no_toc" id="accessing-un-initialized-memory">Accessing un-initialized memory</h4>
<p>PVS-Studio does a somewhat better job than cppcheck of flagging accesses to uninitialized memory.</p>

<h3 class="no_toc" id="infinite-loops">Infinite loops</h3>
<p>Both cppcheck and PVS-Studio detect some infinite loop errors, but miss several others.  It could be that this is by design, since the code that is not flagged tends to resemble some idioms (e.g., ` while (true)`) that are often used deliberately.  </p>

<h3 class="no_toc" id="ignored-return-values">Ignored return values</h3>
<p>PVS-Studio is quite clever here – it will complain about an unused return value from a function, <em>if</em> it can determine that the function has no side effects.  It also knows about some common STL functions that do not have side effects, and will warn if their return values are ignored.</p>

<p>cppcheck doesn’t check for return values per se, but it will detect an assignment that is never referenced.  This makes some sense, since warning on ignored return values could result in a large number of false positives.</p>

<h3 class="no_toc" id="emptyshort-blocks">Empty/short blocks</h3>
<p>Both tools detect certain cases of empty blocks (e.g., <code>if (...);</code> – note the trailing semi-colon).  </p>

<p>What neither tool does is warn about “short” blocks – where a conditional block is not enclosed in braces, and so it’s not 100% clear whether the conditional is meant to cover more than one statement:</p>

<pre><code>if (...)
   statement1();
   statement2();
</code></pre>

<p>If you’ve adopted a convention that even single-statement blocks need to be enclosed in braces, then this situation may not pertain (and good for you!).  Still, I think this would be a worthwhile addition – at least in the “style” category.</p>

<h3 class="no_toc" id="dead-stores">Dead stores</h3>
<p>cppcheck does a particularly good job of detecting dead stores (where an assignment is never subsequently used).  PVS-Studio, on the other hand, flags two or more consecutive assignments to a variable, without an intervening reference.  PVS-Studio will also flag assignment of a variable to itself (which is unlikely to be what was intended).</p>

<div class="footnotes">
  <ol>
    <li id="fn:daniel">
      <p>Daniel was recently interviewed on <a href="http://cppcast.com/2016/11/daniel-marjamaki/">CppCast</a>. <a href="#fnref:daniel" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:free">
      <p>The folks at PVS-Studio asked me to mention that they’ve also recently introduced a free version of their software for educational purposes. The free version does have some strings attached, see <a href="http://www.viva64.com/en/b/0457/">this post</a> for details.<a href="#fnref:free" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:float">
      <p>See <a href="http://blog.reverberate.org/2014/09/what-every-computer-programmer-should.html">here</a> and <a href="http://floating-point-gui.de/">here</a> for an explanation of how floating-point arithmetic can produce unexpected results if you’re not careful.<a href="#fnref:float" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Custom-Tailored Configuration]]></title>
    <link href="http://btorpey.github.io/blog/2016/10/13/custom-tailor/"/>
    <updated>2016-10-13T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2016/10/13/custom-tailor</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/customtailor.jpg" /></p>

<p>As developers, we seem to take a special delight in personalizing the virtual worlds in which we work – from color palettes to keyboards, fonts, macros, you name it.  “Off-the-rack” is never good enough, we want Saville Row tailoring for our environments.</p>

<p>And a lot of the tools we use support and encourage that customization, giving us control over every little option.</p>

<p>But not every tool we use does so – read on to learn a very simple trick to how to take control even when your tool doesn’t make that easy.</p>

<!-- more -->

<p>In Linux, we have a couple of common ways to customize the way our tools work – by defining environment variables, and by using configuration files.  Sometimes these two mechanisms work well together, and we can include environment variables in configuration files to make them flexible in different situations.</p>

<p>Not every tool can expand environment varaiables in its configuration files, however.  In that case, you can use this simple Perl one-liner to subsitute values from the environment into any plain-text file.</p>

<pre><code>perl -pe '$_=qx"/bin/echo -n \"$_\"";' &lt; sample.ini
</code></pre>

<p>What’s happpening here is</p>

<p>The <code>-p</code> switch tells Perl to read every line of input and print it.</p>

<p>The <code>-e</code> switch tells Perl to execute the supplied Perl code against every line of input.</p>

<p>The code snippet replaces the value of the input line (<code>$_</code>) with the results of the shell command specified by the <code>qx</code> function.  That shell command simply echos<sup id="fnref:echo"><a href="#fn:echo" rel="footnote">1</a></sup> the value of the line (<code>$_</code>), but it does so inside double quotes (the <code>\"</code>), which causes the shell to replace any environment variable with its value.</p>

<p>And that’s it!  Since the subsitution is being done by the shell itself, you can use either form for the environment variable (either <code>$VARIABLE</code> or <code>${VARIABLE}</code>), and the replacement is always done using the rules for the current shell.</p>

<p>Here’s an example – let’s create a simple .ini type file, like so:</p>

<pre><code>username=$USER
host=$HOSTNAME
home-directory=$HOME
current-directory=$PWD
</code></pre>

<p>When we run this file through our Perl one-liner, we get:</p>

<pre><code>perl -pe '$_=qx"/bin/echo -n \"$_\"";' &lt; sample.ini
username=btorpey
host=btmac
home-directory=/Users/btorpey
current-directory=/Users/btorpey/blog/code/tailor
</code></pre>

<p>One thing to watch out for is that things can get a little hinky if your input file contains quotes, since the shell will interpret those, and probably not in the way you intend.  At least in my experience, that would be pretty rare – but if you do get peculiar output that would be something to check.</p>

<div class="footnotes">
  <ol>
    <li id="fn:echo">
      <p>Note that we use /bin/echo here, instead of just plain echo, to get around an issue with the echo command in BSD (i.e., OSX).<a href="#fnref:echo" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mo' Static]]></title>
    <link href="http://btorpey.github.io/blog/2016/04/07/mo-static/"/>
    <updated>2016-04-07T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2016/04/07/mo-static</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/nye-static.jpg" width="240" height="180" /></p>

<ul id="markdown-toc">
  <li><a href="#testing-static-analysis-tools">Testing Static Analysis Tools</a></li>
  <li><a href="#can-i-haz-teh-codez">Can I haz teh codez?</a></li>
  <li><a href="#creating-a-compilation-database">Creating a compilation database</a></li>
  <li><a href="#establishing-a-baseline">Establishing a baseline</a></li>
  <li><a href="#using-clangs-analysis-tools">Using clang’s analysis tools</a>    <ul>
      <li><a href="#analyzing-the-results">Analyzing the results</a></li>
    </ul>
  </li>
  <li><a href="#running-clang-analysis-again">Running clang analysis (again)</a></li>
  <li><a href="#using-cppcheck">Using cppcheck</a>    <ul>
      <li><a href="#installing-cppchceck">Installing cppchceck</a>        <ul>
          <li><a href="#verifying-the-installation">Verifying the installation</a></li>
        </ul>
      </li>
      <li><a href="#running-cppcheck">Running cppcheck</a></li>
      <li><a href="#analyzing-the-results-1">Analyzing the results</a></li>
    </ul>
  </li>
  <li><a href="#comparing-clang-and-cppcheck">Comparing clang and cppcheck</a></li>
  <li><a href="#conclusions">Conclusions</a></li>
  <li><a href="#acknowledgements">Acknowledgements</a></li>
  <li><a href="#references">References</a></li>
</ul>

<p>In my day job, one of my main focuses is software reliability and correctness, so it makes sense that I would be a big fan of static analysis.</p>

<p>I’ve written previously about the <a href="http://btorpey.github.io/blog/2015/04/27/static-analysis-with-clang/">static analysis provided by clang</a>.  Today, I want to take a bit of a “deep-dive” into the whole subject by putting both clang and <a href="http://cppcheck.sourceforge.net/">cppcheck</a> through their paces, using them to analyze a benchmark suite designed to exercise static analysis tools.  In the course of doing that, I’ll also provide some helper scripts that make working with the tools easier.  </p>

<!-- more -->

<h2 id="testing-static-analysis-tools">Testing Static Analysis Tools</h2>

<blockquote>
  <p>And what is good Phaedrus, and what is not good – need we ask anyone to tell us these things? <sup id="fnref:zen"><a href="#fn:zen" rel="footnote">1</a></sup>  </p>
</blockquote>

<p>Obviously, the ultimate goal is to be able to run static analysis tools against our own codebase(s) to help detect and fix problems.  But how do we know if a particular tool is actually finding problems?  And, how do we know if we’re running the tool properly?  </p>

<p>The perfect static analyzer would find all the latent bugs in our code, while not reporting any false positives<sup id="fnref:fpos"><a href="#fn:fpos" rel="footnote">2</a></sup>.  Since there are no perfect analyzers, any tool we use is going to miss some errors, and/or wrongly flag correct code.  So, the only way to evaluate an analyzer is to know where all the bugs are in our code – but if we knew that, we wouldn’t need an analyzer.</p>

<p>That’s a dilemma. To resolve it, we’re going to be using a codebase specifically designed to trigger static analysis warnings.  The code was originally developed by Toyota ITC, and is available on <a href="http://blog.regehr.org/archives/1217">John Regehr’s excellent blog</a>.  </p>

<p>The ITC benchmarks attempt to resolve our dilemma by providing both a set of code that contains errors which <em>should</em> trigger warnings, as well as a second set of code, similar to the first, but which doesn’t contain errors.  Each source file is annotated with comments documenting where the errors are (and aren’t).  And that lets us create a catalog of both real errors and potential false positives<sup id="fnref:disclaimer"><a href="#fn:disclaimer" rel="footnote">3</a></sup>. </p>

<p>To get started, download the code from <a href="https://github.com/regehr/itc-benchmarks">its GitHub repository</a>, and set the <code>ITCBENCH_ROOT</code> environment variable (which will come in handy later):</p>

<pre><code>$ git clone https://github.com/regehr/itc-benchmarks
$ export ITCBENCH_ROOT=$(pwd)/itc-benchmarks
</code></pre>

<h2 id="can-i-haz-teh-codez">Can I haz teh codez?</h2>
<p>The remainder of this article goes step-by-step through the process of creating a compilation database from the ITC benchmark code, running clang’s static analysis tools against that compilation database, building and installing cppcheck and running it against the compilation database, and analyzing the results.</p>

<p>This is all good stuff, especially if you’re going to be using these tools going forward.  But, there’s a certain amount of unavoidable yak-shaving<sup id="fnref:yak"><a href="#fn:yak" rel="footnote">4</a></sup> going on to get to that point.  So if you prefer to skip all that, I’ve included the results of running the different tools in the samples directory of the <a href="https://github.com/btorpey/static">repo</a>.  The samples include all the files we’re going to be generating the hard way, so you can follow along without all the requisite busy-work.  Hopefully, when we’re done you’ll want to go back and use these tools on your own codebase. </p>

<h2 id="creating-a-compilation-database">Creating a compilation database</h2>
<p>To run both clang and cppcheck we first need to create a “compilation database” to supply them with required build settings.  The <a href="http://clang.llvm.org/docs/JSONCompilationDatabase.html">compilation database</a> format was developed as part of the clang project, to provide a way for different tools to query the actual options used to build each file.</p>

<p>A <a href="http://eli.thegreenplace.net/2014/05/21/compilation-databases-for-clang-based-tools">good overview of how the compilation database works</a> with clang-based tools can be found at Eli Bendersky’s excellent site.  His article illustrates the importance of making sure that code analysis tools are looking at the same (pre-processed) source that the actual compiler sees, in order to generate meaningful diagnostics with a minimum of false positives.</p>

<ul>
  <li>
    <p>If you are using <a href="http://cmake.org/">cmake</a> to drive your builds, creating a compilation database couldn’t be easier – simply add the <code>-DCMAKE_EXPORT_COMPILE_COMMANDS=ON</code> parameter to the cmake build command, or add the following to your main CMakeLists.txt file:</p>

    <p><code>set(CMAKE_EXPORT_COMPILE_COMMANDS ON)</code></p>
  </li>
  <li>
    <p>If you’re not using cmake, you can still create a compilation database using plain old make by front-ending make with <a href="https://github.com/rizsotto/Bear">Bear</a><sup id="fnref:bear"><a href="#fn:bear" rel="footnote">5</a></sup>, like so:</p>

    <p><code>bear make</code></p>
  </li>
</ul>

<p>In either case, the end result should be the creation of a  <code>compile_commands.json</code> file in the current directory.</p>

<p>Sadly, the ITC benchmark suite is stuck in the past using <a href="https://twitter.com/timmartin2/status/23365017839599616">autotools</a>, and worse yet, a version that needs to be installed from source (on RH6, at least).     </p>

<p>So, in the interest of immediate gratification, I’ve included the compile_commands.json file <a href="http://btorpey.github.io/downloads/code/static/samples/compile_commands.json">here</a> – simply save it to the directory where you’ve cloned the ITC code.  (The compile_commands.json file is also contained in the samples directory of the <a href="https://github.com/btorpey/static">repo for this article</a>).</p>

<p>If you prefer to generate the compile_commands.json file yourself using Bear, you can do so like this:</p>

<pre><code>$ cd ${ITCBENCH_ROOT}  
$ ./bootstrap
$ ./configure
$ bear make
</code></pre>

<h2 id="establishing-a-baseline">Establishing a baseline</h2>
<p>To make it possible to compare results from different analyzers, we first need to establish a baseline using the ITC benchmarks, and for that we’re going to need <a href="http://btorpey.github.io/pages/REAME.md/index.html">this set of helper scripts</a>, which can be downloaded from <a href="https://github.com/btorpey/static">this GitHub repo</a>.</p>

<pre><code>$ git clone https://github.com/btorpey/static
</code></pre>

<p>Once you’ve done that, you need to add the directory to your PATH:</p>

<pre><code>$ export PATH=$(pwd)/static/scripts:$PATH 
</code></pre>

<p>Enter the following command from the ITC source directory to create a csv file with the error annotations from the ITC code:</p>

<pre><code>$ cd ${ITCBENCH_ROOT}  
$ cc_driver.pl -n grep -Hni ERROR: | 
itc2csv.pl -r ${ITCBENCH_ROOT}/ | 
sort -u &gt; itc.csv  
</code></pre>

<p>The command will create a file named <code>itc.csv</code> in the source directory that looks like this:</p>

<pre><code>$ cat itc.csv
"01.w_Defects/bit_shift.c:106","/*ERROR:Bit shift error*/"
"01.w_Defects/bit_shift.c:120","/*ERROR:Bit shift error*/"
"01.w_Defects/bit_shift.c:133","/*ERROR:Bit shift error*/"
"01.w_Defects/bit_shift.c:146","/*ERROR:Bit shift error*/"
"01.w_Defects/bit_shift.c:163","/*ERROR:Bit shift error*/"
"01.w_Defects/bit_shift.c:175","/*ERROR:Bit shift error*/"
...
</code></pre>

<p>The format of the csv file is really simple – just an entry for file and line number, and another with the error annotation munged from the source file.  This will give us a baseline against which to compare both clang and cppcheck.</p>

<h2 id="using-clangs-analysis-tools">Using clang’s analysis tools</h2>

<p>In a couple of previous posts, I wrote about <a href="http://btorpey.github.io/blog/2015/04/27/static-analysis-with-clang">static analysis with clang</a>, and <a href="http://btorpey.github.io/blog/2015/01/02/building-clang">how to build clang</a>.  This next bit assumes that you’ve got clang ready-to-go, but if that’s not the case, there can be a fair amount of work required to get to that point, so you may want to skip ahead to the section on <a href="#using-cppcheck">using cppcheck</a>.</p>

<p>We’re going to use a similar approach to the one we used above to generate the list of expected errors from the ITC code.  The command below will run clang-check against all the files in compile_commands.json, filter the results, and reformat the output in csv format:</p>

<pre><code>$ cd ${ITCBENCH_ROOT}  
$ cc_driver.pl clang-check -analyze 2&gt;&amp;1 | 
clang2csv.pl -r ${ITCBENCH_ROOT}/ |
sort -u &gt; clangcheck.csv
</code></pre>

<p>This gives us the diagnostic messages produced by clang, in the same csv format as we used for the list of errors, above: </p>

<pre><code>$ cat clangcheck.csv
"01.w_Defects/bit_shift.c:106","warning: The result of the '&lt;&lt;' expression is undefined"
"01.w_Defects/bit_shift.c:133","warning: The result of the '&lt;&lt;' expression is undefined"
"01.w_Defects/bit_shift.c:146","warning: The result of the '&lt;&lt;' expression is undefined"
"01.w_Defects/bit_shift.c:163","warning: The result of the '&lt;&lt;' expression is undefined"
"01.w_Defects/bit_shift.c:175","warning: The result of the '&lt;&lt;' expression is undefined"
...
</code></pre>

<p>We can already see that there are some differences: the ITC code expects to see a diagnostic at 01.w_Defects/bit_shift.c:120, but clang doesn’t output a warning for that line.</p>

<h3 id="analyzing-the-results">Analyzing the results</h3>

<p>What I like to do at this point is fire up my all-time favorite tool, <a href="http://btorpey.github.io/blog/2013/01/29/beyond-compare/">Beyond Compare</a>, to generate a visual diff of the two files:</p>

<p><img class="center" src="http://btorpey.github.io/images/itcvsclang.png" /> </p>

<p>This view shows the expected diagnostics extracted from the ITC source files on the left, alongside the diagnostics generated by clang on the right.  We can see that clang catches some of the bugs in the source file, but misses others.  If we continue to read down the two files, we’ll also see some potential “false positives” – i.e., diagnostics issued by clang that are not marked as expected errors in the source files. </p>

<p>The visual approach using Beyond Compare works well for me, but with a csv-formatted datafile, other approaches are possible as well.  We could import the diagnostic messages into a spreadsheet program, or even a DBMS, for archiving, tracking and comparison. </p>

<h2 id="running-clang-analysis-again">Running clang analysis (again)</h2>
<p>clang actually has two tools for doing static analysis – in the example above we ran <code>clang-check -analyze</code>, but now we’re going to use <code>clang-tidy</code> instead.</p>

<pre><code>$ cd ${ITCBENCH_ROOT}  
$ cc_driver.pl clang-tidy 2&gt;&amp;1 | 
clang2csv.pl -r ${ITCBENCH_ROOT}/ | 
sort -u &gt; clangtidy.csv
</code></pre>

<p>If you compare the results from clang-check and clang-tidy, you’ll notice that clang-tidy generally reports more warnings than clang-check.  Some of them are not necessarily defects, but are arguably bad practice (e.g., using <code>strcpy</code>).</p>

<p><img class="center" src="http://btorpey.github.io/images/clangcheckvstidy.png" /> </p>

<p>clang-tidy also outputs a slightly different format, including the name of the check in brackets.  (The name can also be used to suppress the warning).</p>

<p>The choice of which to use is up to you – my preference is to use clang-check first, and follow up with clang-tidy, simply because the warnings produced by clang-tidy either duplicate those from clang-check, or are not as serious.</p>

<p>Note that you can get a list of available checks from clang with the following command:</p>

<pre><code>$ clang -cc1 -analyzer-checker-help
...
core.DivideZero                 Check for division by zero
core.DynamicTypePropagation     Generate dynamic type information
core.NonNullParamChecker        Check for null pointers passed as arguments to a function whose arguments are references or marked with the 'nonnull' attribute
core.NullDereference            Check for dereferences of null pointers
core.StackAddressEscape         Check that addresses to stack memory do not escape the function
</code></pre>

<h2 id="using-cppcheck">Using cppcheck</h2>

<p>There’s another static analysis tool that can provide results comparable to clang.  <a href="http://cppcheck.sourceforge.net/">cppcheck</a> has been around for a while, and I had tried to get it working in the past, but had given up after bumping into a few problems.</p>

<p>I kept hearing good things about cppcheck in <a href="#references">articles and presentations by others</a>, though, so I finally decided it would be worth the trouble to get it working.</p>

<p>It turns out the problems were not that difficult to solve, given a combination of documentation and experimentation.  And the benefits were significant, so I’m quite happy to have added cppcheck to my tool box.</p>

<h3 id="installing-cppchceck">Installing cppchceck</h3>
<p>While cppcheck is available bundled with some distros, it’s often an older version, so we’re going to build and install it from source. As is more and more often the case, cppcheck has started using features of C++1x, so we’re going to need a C++1x-capable compiler to build it.</p>

<p>If you’re on an older distro (in my case, RH6) where the system compiler is not C++1x-capable, see my <a href="http://btorpey.github.io/blog/2015/01/02/building-clang/">earlier post</a> about how to build clang (and/or gcc) to get a C++1x-capable compiler.  (Basically, it uses an older version of gcc to build a newer version, and the newer version to build clang).  </p>

<p>It took some trial-and-error to get the cppcheck build parameters right, but the <a href="http://btorpey.github.io/pages/build_cppcheck.sh/index.html">supplied build script</a> should get the job done<sup id="fnref:install"><a href="#fn:install" rel="footnote">6</a></sup>.</p>

<pre><code>$ ./build_cppcheck.sh 2&gt;&amp;1 | tee build_cppcheck.out
</code></pre>

<h4 id="verifying-the-installation">Verifying the installation</h4>
<p>You’ll need to add the cppcheck directory to your PATH (assuming the install location from the build script):</p>

<pre><code>$ export PATH=/build/share/cppcheck/1.73/bin:$PATH
</code></pre>

<p>If the build and install process worked, you should be able to invoke cppcheck from the command line, like so:</p>

<pre><code>$ cppcheck --version
Cppcheck 1.73
</code></pre>

<p>If you see the message below instead, there’s a problem with the RPATH setting:</p>

<pre><code>$ cppcheck --version
cppcheck: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.15' not found (required by cppcheck)
</code></pre>

<p>The problem is typically either that the RPATH setting in the build is incorrect, or that the directory referenced by the RPATH setting does not exist.</p>

<h3 id="running-cppcheck">Running cppcheck</h3>

<p>Now we’re ready to run cppcheck, using the same approach we used with clang: </p>

<pre><code>$ cd ${ITCBENCH_ROOT}  
$ cc_driver.pl cppcheck.sh  2&gt;&amp;1 | 
cppcheck2csv.pl -r ${ITCBENCH_ROOT} | 
sort -u &gt; cppcheck.csv
</code></pre>

<p>Note that instead of invoking cppcheck directly, we’re invoking it via the <a href="http://btorpey.github.io/pages/cppcheck.sh/index.html">cppcheck.sh</a> helper script, which supplies needed parameters to cppcheck.  It also creates an include file with the compiler’s pre-defined macros, so those definitions will be visible to cppcheck.  This turns out to be particularly important with cppcheck, especially if the code you’re trying to analyze uses <code>#ifdef</code>’s to control what code actually gets compiled (or seen by cppcheck)<sup id="fnref:nosys"><a href="#fn:nosys" rel="footnote">7</a></sup>.</p>

<p>One of the settings in the helper script enables what cppcheck calls “inconclusive” results.  These are exactly what the name implies – cppcheck isn’t positive that the code is wrong, but it is at least suspicious.  Including these inconclusive results should tend to increase the number of false positives in theory, but in practice I haven’t found false positives to be a big problem with either cppcheck or clang.  </p>

<h3 id="analyzing-the-results-1">Analyzing the results</h3>
<p>One of the first things you notice with cppcheck is that it includes more checks than clang.  Some of the additional warnings are for constructs that are not exactly <em>wrong</em>, but are either non-optimal, or indicators of potential problems.  For instance, cppcheck will warn when a variable is defined in a broader scope than is actually required (“scope … can be reduced”).  </p>

<p><img class="center" src="http://btorpey.github.io/images/itcvscppcheck.png" /> </p>

<p>You can get a list of all the checks cppcheck is performing like so:</p>

<pre><code>$ cppcheck --doc 
...
## Other ##
Other checks
- division with zero
- scoped object destroyed immediately after construction
- assignment in an assert statement
- free() or delete of an invalid memory location
- bitwise operation with negative right operand
- provide wrong dimensioned array to pipe() system command (--std=posix)
</code></pre>

<p>You can also generate a list of error ID’s with this command:</p>

<pre><code>$ cppcheck --errorlist
&lt;error id="stringLiteralWrite" severity="error" msg="Modifying string literal directly or indirectly is undefined behaviour."/&gt;
&lt;error id="sprintfOverlappingData" severity="error" msg="Undefined behavior: Variable &amp;apos;varname&amp;apos; is used as parameter and destination in s[n]printf()."/&gt;
&lt;error id="strPlusChar" severity="error" msg="Unusual pointer arithmetic. A value of type &amp;apos;char&amp;apos; is added to a string literal."/&gt;
&lt;error id="incorrectStringCompare" severity="style" msg="String literal &amp;quot;Hello World&amp;quot; doesn&amp;apos;t match length argument for substr()."/&gt;
&lt;error id="literalWithCharPtrCompare" severity="style" msg="String literal compared with variable &amp;apos;foo&amp;apos;. Did you intend to use strcmp() instead?"/&gt;
&lt;error id="charLiteralWithCharPtrCompare" severity="style" msg="Char literal compared with pointer &amp;apos;foo&amp;apos;. Did you intend to dereference it?"/&gt;
&lt;error id="incorrectStringBooleanError" severity="style" msg="Conversion of string literal &amp;quot;Hello World&amp;quot; to bool always evaluates to true."/&gt;
</code></pre>

<p>You can suppress any errors you don’t care to see by passing its id in the <code>--suppress=</code> flag.</p>

<h2 id="comparing-clang-and-cppcheck">Comparing clang and cppcheck</h2>
<p>There’s a school of thought that says you should use as many compilers as possible to build your code, because each one will find different problems.  That’s still a good idea, and even more so with static analysis tools.  </p>

<p>There’s a certain amount of overlap between clang and cppcheck, but there are also significant differences.  In my experience, if clang reports something as a problem, it almost certainly is one, but clang also misses a lot of problems that it could detect.</p>

<p><img class="center" src="http://btorpey.github.io/images/clangvscppcheck.png" /> </p>

<p>cppcheck can generate more warnings, and some of them are more stylistic issues, but it does detect certain classes of problems, like dead code and arithmetic over/underflow, that clang doesn’t.</p>

<p>As I mentioned earlier, I haven’t found false positives to be a major problem with either clang or cppcheck.</p>

<p>So, each tool has its place, and I like to use both.</p>

<h1 id="conclusions">Conclusions</h1>
<p>Static analysis tools can add real value to the software development process by detecting errors, especially errors in code that is never or almost never executed.</p>

<p>Commercial tools can be expensive (although still cheap compared to the money they save), and open-source tools can sometimes be hard to use (or at least hard to learn how to use).</p>

<p>The provided <a href="http://btorpey.github.io/pages/REAME.md/index.html">helper scripts</a> (<a href="https://github.com/btorpey/static">repo here</a>) should make it much easier to use these tools, and to keep track of warnings and compare the outputs of different tools by using a common format.</p>

<p>They can also be useful for before-and-after comparisions of different versions of a single codebase – for example, as changes are being made to address issues detected by the tools.</p>

<h1 id="acknowledgements">Acknowledgements</h1>
<p>In addition to the people, projects and organizations mentioned earlier, the people at the NIST have been very helpful, and maintain an incredible collection of resources on the topic of static analysis for a number of languages, not just C++.  Some of those resources include the following, and are well worth checking out:</p>

<p><a href="https://samate.nist.gov/index.php/SAMATE_Publications.html">https://samate.nist.gov/index.php/SAMATE_Publications.html</a><br />
<a href="https://samate.nist.gov/SARD/">https://samate.nist.gov/SARD/</a>  </p>

<p>If you’ve read any of my other posts, you may have noticed that the contents sidebar at the beginning of the article is a new thing.  Especially for longer-format articles, that TOC would seem to be very helpful.  Many thanks to <a href="http://blog.riemann.cc/2013/04/10/table-of-contents-in-octopress/">Robert Riemann</a> for taking the trouble to explain how to do it.</p>

<p>I’ve been using the very nice <a href="http://macdown.uranusjr.com/">MacDown</a> editor to create these posts – thanks, Tzu-Ping!</p>

<h1 id="references">References</h1>
<p>Some helpful references that I ran across while researching this article:</p>

<p><a href="http://www.viva64.com/en/a/0087/">Static Code Analysis, John Carmack</a></p>

<p><a href="https://youtu.be/sn1Vg8A_MPU">CppCon 2015: Jason Turner “The Current State of (free) Static Analysis”</a></p>

<p><a href="https://youtu.be/rKlHvAw1z50">CppCon 2015: Neil MacIntosh “Static Analysis and C++: More Than Lint”</a></p>

<hr />

<div class="footnotes">
  <ol>
    <li id="fn:zen">
      <p>Robert Pirsig, “Zen and the Art of Motorcycle Maintenance”<a href="#fnref:zen" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:fpos">
      <p>A “false positive” is when a tool reports an error that is actually not.<a href="#fnref:fpos" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:disclaimer">
      <p>Full disclaimer: I have not taken the time to review all of the ITC source to verify that the annotations are accurate and/or complete.  For the purpose of this exercise, we’ll agree to assume that they are – but if you’d like to suggest any improvements, I’m guessing the best place to do that would on the <a href="https://github.com/regehr/itc-benchmarks">repo</a>.<a href="#fnref:disclaimer" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:yak">
      <p>See <a href="https://en.wiktionary.org/wiki/yak_shaving">https://en.wiktionary.org/wiki/yak_shaving</a> for a description of this colorful term.<a href="#fnref:yak" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:bear">
      <p>Building and installing Bear from source is relatively straightforward – just keep in mind that you need python &gt;= 2.7.<a href="#fnref:bear" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:install">
      <p>As usual, I prefer installing external packages in a non-standard location, so the build script is set up to do that.  See <a href="http://btorpey.github.io/blog/2015/01/02/building-clang/">this post</a> for an explanation and rationale of this approach).<a href="#fnref:install" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:nosys">
      <p>Note that cppcheck does not particularly like it when you include system include directories using <code>-I</code>.  Accordingly, we don’t pass the <code>-s</code> switch to  cc_driver.pl when running cppcheck.<a href="#fnref:nosys" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Remote Scripting with bash and ssh]]></title>
    <link href="http://btorpey.github.io/blog/2015/10/13/remote-scripting-with-bash-and-ssh/"/>
    <updated>2015-10-13T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2015/10/13/remote-scripting-with-bash-and-ssh</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/multimonitors.jpg" width="370" height="245" /></p>

<p>Nowadays it’s pretty common for applications to be distributed across multiple
machines, which can be good for scalability and resilience.</p>

<p>But it does mean that we have more machines to monitor – sometimes a LOT more!</p>

<p>Read on for a handy tip that will let you do a lot of those tasks from any old
session (and maybe lose some of those screens)!</p>

<!-- more -->

<p>For really simple tasks, remote shell access using ssh is fine.  But oftentimes
the tasks we need to perform on these systems are complicated enough that they
really should be scripted.</p>

<p>And especially when under pressure, (e.g.,  troubleshooting a problem in a
production system) it’s good for these tasks to be automated. For one thing,
that means they can be tested ahead of time, so you don’t end up doing the
dreaded <code>rm -rf *</code> by mistake.  (Don’t laugh – I’ve actually seen that happen).</p>

<p>Now, I’ve seen people do this by copying scripts to a known location on the
remote machines so they can be executed.  That works, but has some
disadvantages: it clutters up the remote system(s), and it creates one more
artifact that needs to be distributed and managed (e.g., updated when it
changes).</p>

<p>If you’ve got a bunch of related scripts, then you’re going to have to bite the
bullet and manage them (perhaps with something like Puppet).</p>

<p>But for simple tasks, the following trick can come in very handy:</p>

<p><code>
ssh HOST ‘bash –s ‘ &lt; local_script.sh
</code></p>

<p>What we’re doing here is running bash remotely and telling bash to get its input
from stdin.  We’re also redirecting local_script.sh to the stdin of ssh, which
is what the remote bash will end up reading.</p>

<p>As long as local_script.sh is completely self-contained, this works like a
charm.</p>

<p>For instance, to login to a remote machine and see if hyper-threading is enabled
on that machine:</p>

<p><code>
ssh HOST 'bash -s' &lt; ht.sh
</code></p>

<p>Where ht.sh looks like this:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span> (ht.sh)</span> <a href="http://btorpey.github.io/downloads/code/bash/ht.sh">download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c">#!/bin/bash</span>
</span><span class="line">
</span><span class="line"><span class="c"># cribbed from http://unix.stackexchange.com/questions/33450/checking-if-hyperthreading-is-enabled-or-not</span>
</span><span class="line"><span class="c">#</span>
</span><span class="line"><span class="c"># NOTE:  There does not seem to be a good way to determine if HT is available but not enabled on a particular machine:</span>
</span><span class="line"><span class="c"># - &#39;ht&#39; flag in /proc/cpuinfo is unreliable</span>
</span><span class="line"><span class="c"># - lscpu could be used, but is not part of RH5</span>
</span><span class="line"><span class="c"># - dmidecode could be used, but requires root permissions</span>
</span><span class="line"><span class="c">#</span>
</span><span class="line"><span class="c"># So for now we just report whether HT is enabled or not</span>
</span><span class="line">
</span><span class="line"><span class="nb">echo</span> -n <span class="k">${</span><span class="nv">HOSTNAME</span><span class="k">}</span>
</span><span class="line">
</span><span class="line"><span class="nv">nproc</span><span class="o">=</span><span class="k">$(</span>grep -i <span class="s2">&quot;processor&quot;</span> /proc/cpuinfo | sort -u | wc -l<span class="k">)</span>
</span><span class="line"><span class="nv">phycore</span><span class="o">=</span><span class="k">$(</span>cat /proc/cpuinfo | egrep <span class="s2">&quot;core id|physical id&quot;</span> | tr -d <span class="s2">&quot;\n&quot;</span> | sed s/physical/<span class="se">\\</span>nphysical/g | grep -v ^<span class="nv">$ </span>| sort -u | wc -l<span class="k">)</span>
</span><span class="line"><span class="k">if</span> <span class="o">[</span> -z <span class="s2">&quot;$(echo &quot;</span><span class="nv">$phycore</span> *2<span class="s2">&quot; | bc | grep $nproc)&quot;</span> <span class="o">]</span>; <span class="k">then</span>
</span><span class="line"><span class="k">   </span><span class="nb">echo</span> <span class="s2">&quot;: HT disabled&quot;</span>
</span><span class="line"><span class="k">else</span>
</span><span class="line"><span class="k">   </span><span class="nb">echo</span> <span class="s2">&quot;: HT enabled&quot;</span>
</span><span class="line"><span class="k">fi</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>(The script above was cribbed from http://unix.stackexchange.com/a/33509 –
thanks, Nils!)</p>

<p>Of course, all the normal redirection rules apply – you just have to keep in
mind that you’re redirecting to ssh, which is then redirecting to bash on the
input side.  On the output side, it’s reversed.</p>

<p>Give this a try the next time you need to do some quick tasks over ssh and
you’ll be able to get rid of a few of those monitors!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Static Analysis with clang]]></title>
    <link href="http://btorpey.github.io/blog/2015/04/27/static-analysis-with-clang/"/>
    <updated>2015-04-27T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2015/04/27/static-analysis-with-clang</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/coil3.jpg" width="195" height="281" /></p>

<p>I keep singing the praises of clang, and with good reason – the <a href="http://clang.llvm.org/">clang project</a> has been advancing the state of C/C++ compiler technology on Linux and OS X for quite a while now.  </p>

<p>The modular design of the compiler has also enabled the creation of a set of ancillary tools, including run-time “sanitizers” (which I <a href="http://btorpey.github.io/blog/2014/03/27/using-clangs-address-sanitizer/">wrote about earlier</a>), as well as pretty-printers, and a tool to automatically upgrade code to C++11. </p>

<p>Today I want to talk about clang’s static analysis engine, which can do a deep-dive on your code and find problems that are hard for a human to detect, but that are amenable to a brute-force approach that models the run-time behavior of a piece of code, but at compile-time.</p>

<!-- more -->

<p>This is very different from dynamic analysis tools, like valgrind and clang’s own sanitizers, which instrument the code at run-time to detect actual errors (e.g., reading uninitialized memory) that happen while the code is running.  With dynamic analysis, the only errors that can be detected are in code that is actually executed, so a latent bug that only manifests under unusual conditions<sup id="fnref:heisenbug"><a href="#fn:heisenbug" rel="footnote">1</a></sup>, can go un-detected.  By contrast, static analysis can potentially find bugs in code that is never (or almost never) actually executed.  </p>

<p>Sounds good, no?  Who wouldn’t want to find bugs “automagically”, without even needing to do any testing.  (Cause we all know how much programmers love testing ;-)</p>

<p>For example, running clang’s static analyzer on some sample code turns up warnings similar to the following:</p>

<ul>
  <li>Value stored to ‘x’ is never read</li>
  <li>The right operand of ‘!=’ is a garbage value</li>
  <li>Potential leak of memory pointed to by ‘x’</li>
  <li>Function call argument is an uninitialized value</li>
  <li>Use of memory after it is freed</li>
  <li>Called C++ object pointer is null</li>
</ul>

<p>Some of the above warnings (e.g., value stored is never read) are most likely harmless, and just sloppy coding (perhaps because of copy-paste syndrome, about which I have more to say <a href="http://btorpey.github.io/blog/2014/09/21/repent/">here</a>).  Others (e.g., called pointer is null), might be false positives, given the algorithms the analyzer uses<sup id="fnref:falsepos"><a href="#fn:falsepos" rel="footnote">2</a></sup>.  Or, they could be real bugs that you just haven’t hit yet, because the code is (almost) never executed.</p>

<p>Those are the really scary bugs, along with the ones where you can “get lucky” most of the time … except when you don’t.  The “garbage value” and “unitialized value” warnings fall into that category, and can be very hard to eyeball.  Again, dynamic analysis tools like valgrind can help find these bugs, but only if you hit that code in testing.</p>

<p>So, static analysis <em>is</em> good, but it’s not magic.  Static analyzers can only find bugs that they are programmed to find, and they certainly don’t find all bugs.  For instance, here’s a bug that clang’s static analysis doesn’t find:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="c++"><span class="line"><span class="kt">void</span> <span class="n">func</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">arg</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">  <span class="kt">char</span> <span class="n">buf1</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span>
</span><span class="line">  <span class="kt">char</span> <span class="n">buf2</span><span class="p">[</span><span class="mi">20</span><span class="p">];</span>
</span><span class="line">
</span><span class="line">  <span class="n">strncpy</span><span class="p">(</span><span class="n">buf1</span><span class="p">,</span> <span class="n">arg</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">buf2</span><span class="p">));</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>But the fact is that static analysis <em>will</em> find bugs, and it will find bugs that you most likely wouldn’t find on your own, so it’s a a good tool to have in your toolbox.  So, let’s take a look at how to do that using clang.</p>

<p>The first step is to install clang.  If you’re on OS X or Ubuntu, you should already have it, but if you’re on RedHat this can be a bit tricky, so see <a href="http://btorpey.github.io/blog/2015/01/02/building-clang/">my previous post</a> on how to get clang working.  (I’ve updated that post to add instructions for installing some of the static analysis tools that don’t normally get installed with clang).</p>

<p>It turns out that there are three (3) different ways to run clang’s static analyzer on your code, each with its own benefits and drawbacks.  We’ll consider each of these in turn:</p>

<h2 id="integrated-with-a-typical-build">Integrated with a typical build</h2>

<p>If you use reasonably normal-looking makefiles to build your code, you can get static analysis going with a minimum of fuss.  If you’re using cmake to create your makefiles, the same approach will work fine, so long as you’re not overriding the values of CMAKE_C_COMPILER etc.  (And, as usual, if you’re using autotools, <a href="https://twitter.com/timmartin2/status/23365017839599616">you’re on your own;-)</a>.  </p>

<p>In this approach, you <code>export</code> some environment variables to invoke the analyzer instead of the compiler, like the following:</p>

<table>
  <thead>
    <tr>
      <th>Variable</th>
      <th>Value</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CC</td>
      <td>ccc-analyzer</td>
      <td>C compiler is redirected to clang analyzer (which in turn invokes the compiler, using the value of CCC_CC, below).</td>
    </tr>
    <tr>
      <td>CXX</td>
      <td>c++-analyzer</td>
      <td>Similar to above, but for C++.</td>
    </tr>
    <tr>
      <td>CCC_CC</td>
      <td>clang</td>
      <td>This environment variable is used by ccc-analyzer to invoke the actual C compiler.</td>
    </tr>
    <tr>
      <td>CCC_CXX</td>
      <td>clang++</td>
      <td>ditto</td>
    </tr>
    <tr>
      <td>LD</td>
      <td>clang++</td>
      <td>Specifies that the actual compiler should be used for linking.</td>
    </tr>
    <tr>
      <td>CCC_ANALYZER_VERBOSE</td>
      <td>1</td>
      <td>(Optional) Set this flag to get verbose output from the analyzer, including the list of errors checked.</td>
    </tr>
  </tbody>
</table>

<p><br />
With those variables set, you should just be able to invoke <code>make</code> to build your project, and Bob’s your uncle.</p>

<p>One nice thing about this approach is that you get both compiler warnings and analyzer warnings together – first, the analyzer invokes the compiler on the source file, and then performs the static analysis.</p>

<h2 id="using-the-gui-tool">Using the GUI tool</h2>

<p>In a similar fashion as ccc-analyzer (above) front-ends make, you can use clang’s <a href="http://clang-analyzer.llvm.org/scan-build.html">scan-build</a> tool to front-end ccc-analyzer. In addition to invoking the compiler and analyzer, scan-build also collects the analyzer reports, including the control flow that the analyzer used to infer any errors, and presents that using a set of html pages that are written by default to the /tmp directory, and that look like this:</p>

<p><img src="http://clang-analyzer.llvm.org/images/analyzer_html.png" alt="" /></p>

<p>Personally, I find this fascinating.  Not only does the analyzer tell about what it thinks is a problem, but also <em>why</em> it thinks so.  </p>

<p>In the example above, you can see the steps that the analyzer follows to figure out that there is a problem with the code.  If you are wondering whether a particular warning is a false positive or not, this presentation can help you figure that out.  <sup id="fnref:false2"><a href="#fn:false2" rel="footnote">3</a></sup> It can also sometimes provide unexpected insights into the code that you might not come up with on your own.</p>

<p>To use this approach, you set your environment variables the same as described above, but instead of running make, you run <code>scan-build -V make</code>.  This will run your build and then launch a browser to view the results of the build.</p>

<h3 id="a-small-hitch">A small hitch…</h3>

<p>Unfortunately, scan-build (and its scan-view companion) are not installed by default with clang.  I’ve updated the build script from my <a href="http://btorpey.github.io/blog/2015/01/02/building-clang/">earlier post</a> on building clang on RedHat to install these files, but if you want to do it manually, run the following from the source tree you used to build and install clang:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="c"># we need some addl bits that are not normally installed</span>
</span><span class="line">cp -p  llvm/tools/clang/tools/scan-build/scan-build     <span class="k">$(</span>which clang<span class="k">)</span>/..
</span><span class="line">cp -p  llvm/tools/clang/tools/scan-build/ccc-analyzer   <span class="k">$(</span>which clang<span class="k">)</span>/..
</span><span class="line">cp -p  llvm/tools/clang/tools/scan-build/c++-analyzer   <span class="k">$(</span>which clang<span class="k">)</span>/..
</span><span class="line">cp -p  llvm/tools/clang/tools/scan-build/sorttable.js   <span class="k">$(</span>which clang<span class="k">)</span>/..
</span><span class="line">cp -p  llvm/tools/clang/tools/scan-build/scanview.css   <span class="k">$(</span>which clang<span class="k">)</span>/..
</span><span class="line">cp -rp llvm/tools/clang/tools/scan-view/*               <span class="k">$(</span>which clang<span class="k">)</span>/..
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h3 id="another-small-hitch">Another small hitch…</h3>
<p>In an <a href="http://btorpey.github.io/blog/2015/03/17/shadow/">earlier post</a>, I talked about how to use the <code>-isystem</code> flag to prevent selected headers from generating warnings.  Unfortunately, the <a href="https://llvm.org/bugs/show_bug.cgi?id=13237#c9">clang analyzer chokes on that flag</a> – so if you’re using it, you will need to apply the patch below to successfully run the analyzer.  </p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="nv">$ </span>svn diff ccc-analyzer
</span><span class="line">Index: ccc-analyzer
</span><span class="line"><span class="o">===================================================================</span>
</span><span class="line">--- ccc-analyzer	<span class="o">(</span>revision 234604<span class="o">)</span>
</span><span class="line">+++ ccc-analyzer	<span class="o">(</span>working copy<span class="o">)</span>
</span><span class="line">@@ -354,7 +354,8 @@
</span><span class="line">   <span class="s1">&#39;-iprefix&#39;</span> <span class="o">=</span>&gt; 1,
</span><span class="line">   <span class="s1">&#39;-iquote&#39;</span> <span class="o">=</span>&gt; 1,
</span><span class="line">   <span class="s1">&#39;-iwithprefix&#39;</span> <span class="o">=</span>&gt; 1,
</span><span class="line">-  <span class="s1">&#39;-iwithprefixbefore&#39;</span> <span class="o">=</span>&gt; 1
</span><span class="line">+  <span class="s1">&#39;-iwithprefixbefore&#39;</span> <span class="o">=</span>&gt; 1,
</span><span class="line">+  <span class="s1">&#39;-isystem&#39;</span> <span class="o">=</span>&gt; 1,
</span><span class="line"> <span class="o">)</span>;
</span><span class="line">
</span><span class="line"> my %LinkerOptionMap <span class="o">=</span> <span class="o">(</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="using-a-compilation-database">Using a compilation database</h2>

<p>Last but not least, you can also use a “compilation database” to invoke the static analyzer directly.  So, what is a compilation database, you ask?  This is a <a href="http://clang.llvm.org/docs/JSONCompilationDatabase.html">simple format</a> introduced by clang that records the actual commands used to generate intermediate build products from source files, along with their parameters.</p>

<p>The analyzer needs this information to reproduce the environment used by the compiler, including pre-processor definitions and include file search paths.  </p>

<p>If you are using <a href="http://cmake.org/">cmake</a> to drive your builds, creating a compilation database couldn’t be easier – simply add the <code>-DCMAKE_EXPORT_COMPILE_COMMANDS=ON</code> parameter to the cmake build command, or add the following to your main CMakeLists.txt file:</p>

<p><code>set(CMAKE_EXPORT_COMPILE_COMMANDS ON)</code></p>

<p>If you’re not using cmake, you can still create a compilation database using plain old make by front-ending make with <a href="https://github.com/rizsotto/Bear">Bear</a><sup id="fnref:bear"><a href="#fn:bear" rel="footnote">4</a></sup>, like so:</p>

<p><code>bear make</code></p>

<p>This will use Bear to drive the make process, leaving a <code>compile_commands.json</code> file in the current directory.</p>

<p>Once you’ve got the compilation database, invoking the analyzer can be done with a command like the following:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line"><span class="nb">export </span><span class="nv">COMPILE_DB</span><span class="o">=</span><span class="k">$(</span>/bin/pwd<span class="k">)</span>;
</span><span class="line">grep file compile_commands.json |
</span><span class="line">awk <span class="s1">&#39;{ print $2; }&#39;</span> |
</span><span class="line">sed <span class="s1">&#39;s/\&quot;//g&#39;</span> |
</span><span class="line"><span class="k">while </span><span class="nb">read </span>FILE; <span class="k">do</span>
</span><span class="line">  <span class="o">(</span><span class="nb">cd</span> <span class="k">$(</span>dirname <span class="k">${</span><span class="nv">FILE</span><span class="k">})</span>;
</span><span class="line">   clang-check -analyze -p <span class="k">${</span><span class="nv">COMPILE_DB</span><span class="k">}</span> <span class="k">$(</span>basename <span class="k">${</span><span class="nv">FILE</span><span class="k">})</span>
</span><span class="line">  <span class="o">)</span>;
</span><span class="line"><span class="k">done</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>(There are simpler ways to invoke the analyzer, but the approach shown here will visit each source file in the same order that it was originally built, which can be handy).</p>

<h2 id="conclusion">Conclusion</h2>
<p>As we said earlier, static analysis is not magic, and it certainly won’t find all your bugs.  But it will probably find some, and the ones it finds are likely to be nasty, so it’s worth a certain amount of trouble.</p>

<p>Last but not least, this is by no means a complete explanation of clang’s analyzer.  Like much of clang, the documentation lags the code, sometimes by a lot, so much of this information was obtained by trial-and-error, and/or by reading the code.  So, if you find something interesting, please <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#119;&#097;&#108;&#108;&#115;&#116;&#112;&#114;&#111;&#103;&#064;&#103;&#109;&#097;&#105;&#108;&#046;&#099;&#111;&#109;">drop me a line</a>, or leave a note in the comments section. </p>

<h2 id="references">References</h2>

<p><a href="http://clang.llvm.org/">http://clang.llvm.org/</a></p>

<p><a href="http://clang-analyzer.llvm.org/index.html">http://clang-analyzer.llvm.org/index.html</a></p>

<p><a href="http://clang.llvm.org/docs/ClangCheck.html">http://clang.llvm.org/docs/ClangCheck.html</a></p>

<div class="footnotes">
  <ol>
    <li id="fn:heisenbug">
      <p>These are often called “Heisenbugs”, in a nerd-humor pun on the <a href="http://en.wikipedia.org/wiki/Uncertainty_principle">Heisenberg Uncertainty Principle</a>.<a href="#fnref:heisenbug" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:falsepos">
      <p>For instance, clang’s analayzer attempts to figure out if a pointer can possibly be NULL by seeing if there is any code that checks for that condition.  If there is, then clang complains about any code that dereferences the pointer outside an <code>if (x != NULL)</code> block.  This algorithm isn’t perfect, but it’s about the best that can be done, especially since the analyzer only looks at a single file at a time.<a href="#fnref:falsepos" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:false2">
      <p>At least in my experience, many of the warnings that appear at first to be false positives turn out to be real bugs, especially if you follow through the control flow the analyzer uses.<a href="#fnref:false2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:bear">
      <p>Building and installing Bear from source is relatively straightforward – just keep in mind that you need python &gt;= 2.7.<a href="#fnref:bear" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Who Knows What Evil Lurks ...]]></title>
    <link href="http://btorpey.github.io/blog/2015/03/17/shadow/"/>
    <updated>2015-03-17T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2015/03/17/shadow</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/TheShadowComic01.jpg" width="222" height="333" /></p>

<p>Pity the poor Shadow!  Even with the recent glut of super-heroes in movies, games and TV, the Shadow is nowhere to be seen.  </p>

<p>But I guess that’s the whole point of being the Shadow.  </p>

<p>According to <a href="http://en.wikipedia.org/wiki/The_Shadow">this</a>, the Shadow had “the mysterious power to cloud men’s minds, so they could not see him”.  Hmmm, that sounds like more than a few bugs I’ve known.</p>

<p>Read on to learn how to get your compiler to help you find and eliminate these “shadow bugs” from your code.</p>

<!-- more -->

<p>Recently I was cleaning up the code for one of our test programs, and I
suddenly started getting a crash at shutdown that I hadn’t seen before. The
stack trace looked more or less like I expected (except for the SEGV, of
course), and I spent several minutes staring at the code before the light bulb
came on.</p>

<p>As is often the case, once the light bulb did come on, my first reaction was
“Duh!”. It was a dumb mistake, but then I started to think: if it’s such a dumb
mistake, why didn’t the compiler warn me about it? Answering that question got
me looking into the state of compiler diagnostics, and taught me a few things I
hadn’t known (or had forgotten).</p>

<p>First, let’s take a look at the bug that was a bit of a head-scratcher, and that
prompted this post. I’ve distilled it down to just a few lines of code — take a
look and see if you can spot the bug:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span> (shadow.cpp)</span> <a href="http://btorpey.github.io/downloads/code/shadow/shadow.cpp">download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
</pre></td><td class="code"><pre><code class="cpp"><span class="line"><span class="cp">#include &lt;memory&gt;</span>
</span><span class="line"><span class="k">using</span> <span class="k">namespace</span> <span class="n">std</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">D</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line"><span class="k">public</span><span class="o">:</span>
</span><span class="line">   <span class="kt">void</span> <span class="n">Init</span><span class="p">()</span>
</span><span class="line">   <span class="p">{</span>
</span><span class="line">   <span class="p">}</span>
</span><span class="line"><span class="p">};</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">C</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line"><span class="k">public</span><span class="o">:</span>
</span><span class="line">   <span class="n">C</span><span class="p">()</span>
</span><span class="line">   <span class="p">{</span>
</span><span class="line">      <span class="n">D</span><span class="o">*</span> <span class="n">_pD</span> <span class="o">=</span> <span class="k">new</span> <span class="n">D</span><span class="p">;</span>
</span><span class="line">      <span class="n">_pD</span><span class="o">-&gt;</span><span class="n">Init</span><span class="p">();</span>
</span><span class="line">   <span class="p">}</span>
</span><span class="line">
</span><span class="line">   <span class="o">~</span><span class="n">C</span><span class="p">()</span>
</span><span class="line">   <span class="p">{</span>
</span><span class="line">      <span class="k">delete</span> <span class="n">_pD</span><span class="p">;</span>
</span><span class="line">   <span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="k">private</span><span class="o">:</span>
</span><span class="line">   <span class="n">D</span><span class="o">*</span> <span class="n">_pD</span><span class="p">;</span>
</span><span class="line"><span class="p">};</span>
</span><span class="line">
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="kt">int</span> <span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">
</span><span class="line">   <span class="n">C</span> <span class="n">c</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">   <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The bug is in C’s constructor, where instead of initializing the member variable (_pD), the
code instead creates a local variable with the same name. The local variable goes
out of scope on return and gets deleted (although the allocation
persists), but the member variable of the same name remains uninitialized. The
problem comes when we delete c, since C’s dtor deletes a pointer that is just a
bunch of random bits<sup id="fnref:4"><a href="#fn:4" rel="footnote">1</a></sup>. The fix, of course, is to omit the type declaration on the
assignment, which causes the compiler to assign to the member variable, rather
than creating and then assigning to a local (stack) variable.</p>

<p>(I can already hear the howls of outrage at this code – see <sup id="fnref:1"><a href="#fn:1" rel="footnote">2</a></sup>, <sup id="fnref:2"><a href="#fn:2" rel="footnote">3</a></sup> and <sup id="fnref:3"><a href="#fn:3" rel="footnote">4</a></sup> for a discussion if you’re so inclined).</p>

<p>Granted that there are ways to avoid this problem by writing the code “correctly” (perfectly?) in the
first place. But still, if it’s such a dumb mistake, why didn’t the
compiler warn about it?</p>

<p>That was what puzzled me, especially since I thought our “diagnostic hygiene”
was pretty good. All our code is built with “-Wall -Wextra”, which is not quite
“everything but the kitchen sink”, but close.</p>

<p>But when we build with those flags, the compiler is perfectly happy:</p>
<pre>
$ clang++ -g -Wall -Wextra shadow.cpp
$
</pre>

<p>But running – that’s another story:</p>
<pre>
$ ./a.out
*** glibc detected *** ./a.out: free(): invalid pointer: 0x0000000000400600 ***
...
Aborted (core dumped)
$ 
</pre>

<p>When we load the core file into the debugger, we see that the offending instruction is the delete of _pD in C’s destructor:</p>

<pre>
$ gdb a.out core.897
(gdb) bt
#0  0x0000003a86032925 in raise () from /lib64/libc.so.6
#1  0x0000003a86034105 in abort () from /lib64/libc.so.6
#2  0x0000003a86070837 in __libc_message () from /lib64/libc.so.6
#3  0x0000003a86076166 in malloc_printerr () from /lib64/libc.so.6
#4  0x0000000000400720 in C::~C (this=0x7fffe6dbdec8) at shadow.cpp:23
#5  0x00000000004006a9 in main () at shadow.cpp:37
(gdb) 
</pre>

<p>The result above is just one of three possible results. Let’s take a look at each of these in turn:</p>

<ol>
  <li>
    <p>You may get no message at all - the code (appears to) work fine. </p>

    <p>This is the result we get if we use gcc to compile the code.  With gcc, the allocation is (presumably) being satisfied by the operating system (e.g., by calling <a href="http://pubs.opengroup.org/onlinepubs/007908775/xsh/brk.html">sbrk</a>).  Typically, the OS will zero-fill any memory that it allocates as a security precaution (see <a href="http://stackoverflow.com/questions/8029584/why-does-malloc-initialize-the-values-to-0-in-gcc">here</a>  and <a href="http://stackoverflow.com/questions/2688466/why-mallocmemset-is-slower-than-calloc">here</a> for details).  </p>

    <p>So, in this case, we’re deleting a nullptr, and that is perfectly kosher <a href="http://en.cppreference.com/w/c/memory/free">according to the standard</a>. (Why that is may be a cause for debate, but it is).<sup id="fnref:non-null"><a href="#fn:non-null" rel="footnote">5</a></sup></p>
  </li>
</ol>

<ol>
  <li>
    <p>You may get a message similar to  <code>*** glibc detected *** ./a.out: free(): invalid pointer</code> followed by a stack trace.  </p>

    <p>This happens when glibc can detect that the pointer being freed was not previously allocated.</p>

    <p>The memory management functions in glibc contain runtime checks to catch error conditions.  Some of these checks are enabled in all cases (because they are relatively inexpensive), while others must be specifically enabled.<sup id="fnref:malloc-check"><a href="#fn:malloc-check" rel="footnote">6</a></sup>  In this particular case, the code in <code>free</code> is checking to see if the address being passed in has previously been allocated using e.g. <code>malloc</code>.  If not, the code signals an error.</p>

    <p>This is the result we get when using clang – with clang, the  allocation request is being satisfied from memory that was previously allocated and freed, so the bits that make up the member variable _pD have already been scribbled on (i.e., they are non-zero), but glibc can tell the address is not one that was previously allocated.</p>
  </li>
  <li>
    <p>You may get a message similar to <code>Segmentation fault (core dumped)</code>.  </p>

    <p>In our case, C’s destructor is pretty minimal – it just deletes the _pD member variable.  In other cases, though, C’s destructor may attempt to do more complicated processing before returning, and in those cases it’s quite possible that that processing will trigger a crash on its own.  (For instance, if _pD is defined as a shared_ptr as opposed to a raw pointer, you would likely see a segmentation violation in the code that manipulates the shared_ptr).</p>
  </li>
</ol>

<h1 id="the-shadow-knows">… the Shadow Knows</h1>
<p>This all could have been avoided if the compiler recongized that the declaration of _pD in C’s constructor
hid the member variable, and with “-Wshadow” enabled in the compile, that is exactly what happens:</p>

<pre>
﻿$ clang++ -Wall -Wextra -Wshadow shadow.cpp
shadow.cpp:17:10: warning: declaration shadows a field of 'C' [-Wshadow]
      D* _pD = new D;
         ^
shadow.cpp:27:7: note: previous declaration is here
   D* _pD;   
      ^
1 warning generated.
$
</pre>

<p>gcc supports the flag also, although the message is slightly different</p>
<pre>
$ g++ -Wall -Wextra -Wshadow shadow.cpp
shadow.cpp: In constructor ‘C::C()’:
shadow.cpp:17: warning: declaration of ‘_pD’ shadows a member of 'this'
$
</pre>

<p>While both gcc and clang support the “-Wshadow” flag, the implementations are very different.<br />
gcc appears to strive for completeness, and in the process produces so many warnings as to render the use 
of “-Wshadow” pretty much useless.  That was certainly Linus’ opinion when
he wrote <a href="https://lkml.org/lkml/2006/11/28/239">this</a>, and it’s hard to
disagree.</p>

<p>The good news is that the clang developers have come up with a much
more useful implementation of “-Wshadow”, which avoids a lot of the problems
Linus talks about.  For example, on one legacy-ish code base, gcc reports over 1100 shadow
warnings vs. just three for clang.  There’s a terrific explanation  <a href="http://programmers.stackexchange.com/questions/122608/clang-warning-flags-for-objective-c-development/124574#124574">here</a>
about how the clang team decides what category a particular diagnostic should belong to.</p>

<h2 id="third-party-libraries">Third-party Libraries</h2>

<p>But, what if we use third-party libraries in our programs?  While clang does a very good job
of filtering out “false positive” shadow warnings, they can still crop up in some libraries, including Boost.  One
possible solution is to have wrapper includes that use #pragma’s to suppress (or enable) certain warnings, prior to including
the real library headers.  That is in fact the approach suggested by the Boost maintainer when someone posted 
<a href="https://svn.boost.org/trac/boost/ticket/9378#comment:15">a bug report</a> about 
the shadow warnings in Boost.  </p>

<p>But, that’s tedious, error-prone, inconvenient and expensive.  Is there a better way?</p>

<p>It turns out that there is – both gcc and clang provide the <code>-isystem</code> compiler flag to include header files, subject to 
<a href="https://gcc.gnu.org/onlinedocs/cpp/System-Headers.html">special rules</a> that effectively eliminate warnings, 
even in macros that are expanded at compile-time.  </p>

<p>Note that if you’re using cmake, the way to enable <code>-isystem</code> is to use the SYSTEM flag to include_directories, like so: 
<code>include_directories(SYSTEM ${Boost_INCLUDE_DIRS})</code></p>

<h2 id="fixing-the-code">Fixing the code</h2>
<p>There are three types of shadow warnings, each with a different cause and potential to cause trouble.  The different types are distinguished by what follows after the “warning: declaration shadows a” message:</p>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Explanation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>local variable</td>
      <td>These are typically the least likely to be real problems, since local variables have limited lifetimes by defintion.  Even if these warnings don’t indicate a genuine problem, it is best to eliminate them  by changing one or the other variable name, if only to prevent future confusion.</td>
    </tr>
    <tr>
      <td>field of “X”</td>
      <td>As in the example above, these shadow warnings often point to a potential problem, given how easy it is to inadvertently include a type prefix in the statement meant to initialize the member variable, thereby declaring (and initializing) a new local variable instead.</td>
    </tr>
    <tr>
      <td>variable in the global namespace</td>
      <td>This is perhaps the most dangerous of the three types, since accidentally introducing a new variable into scope can easily go unnoticed.  Everything appears to be working correctly, until at some point the newly introduced variable goes out of scope, exposing the un-initialized global variable.</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h1 id="conclusion">Conclusion</h1>

<p>I originally thought this would be a quick post about a somewhat obscure compiler warning – maybe a 
“tidbit”, but certainly nothing more than that.  But, as Tolkien said about “Lord of the Rings”, “the tale grew in the telling”.</p>

<p>Let’s see what we’ve covered:</p>

<ul>
  <li>What “-Wshadow” means, and why you might want to use it.</li>
  <li>How to selectively disable warnings, including “-Wshadow”, for third-party libraries when
it creates more noise than value.</li>
  <li>How even seemingly trivial code can behave very differently when compiled with different compilers (e.g., gcc vs. clang).</li>
  <li>How to use value initialization to initialize even POD types to known (zero) values.</li>
</ul>

<p>And that doesn’t even include one of my original goals, which was to talk about compiler warnings
in general, and which ones you want to make sure you use in all your builds.  That will have to wait
for next time.</p>

<h2 id="tldr">TL;DR</h2>
<ul>
  <li>Enable “-Wshadow” in addition to whatever other compiler warnings you’re using.<br />
    <ul>
      <li>Fix the warnings, even if they don’t (appear to) matter.</li>
    </ul>
  </li>
  <li>But only if you’re using <a href="http://clang.llvm.org/">clang</a>!
    <ul>
      <li>And if you’re not using clang yet, it’s time to start.</li>
      <li>If you need help getting clang up and running on your system, be sure to check out <a href="http://btorpey.github.io/blog/2015/01/02/building-clang/">my earlier post</a>.</li>
    </ul>
  </li>
</ul>

<div class="footnotes">
  <ol>
    <li id="fn:4">
      <p>At least <a href="http://en.cppreference.com/w/cpp/language/default_initialization">according to the standard</a>.  Different implementations can, and do, behave differently. Or, as the old saying goes: “In theory, there is no difference between theory and practice.  In practice, there is”.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>The first point is that we should be initializing the member variable in the ctor, rather than assigning it, which would make this mistake impossible. That’s a valid point, mostly, but there are times when you can make a case for assignment being a simpler approach — for example, when you have multiple member variables, and when the order of assignment matters. Remember that member variables are initialized in the order of their declaration, not the order in which the initializers appear. Given that the order of declaration is often not obvious, it’s easy to see why one might prefer to use assignment to enforce the order of assignment in the body of the constructor.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Another mostly valid point is that, if we are going to assign in the body of the ctor, we should at least initialize the members to some value before entering the constructor. The only defense to that is a misguided attempt to optimize out the initialization code, since we know we’re assigning to the member variable anyway. That’s arguably wrong, but not terribly so, and in any event is pretty common, at least in my experience. (OK, smarty-pants, do YOU always initialize ALL your member variables in EVERY constructor you write? Even if you’re going to assign to them in the body of the constructor?  Really? Do you want your merit badge now, or at the jamboree?)<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Last but not least, we could use <a href="http://stackoverflow.com/a/2418195/203044">value initialization</a> to ensure that even POD types in the class are <a href="http://en.cppreference.com/w/cpp/language/zero_initialization">zero-initialized</a>.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:non-null">
      <p>Another possibility is that the bits are non-NULL, but the call to free doesn’t immediately crash.  Instead, it may leave the data structures used to manage the heap in an inconsistent state, in such a way that it will cause a crash later.  This is the worst possible scenario, since this problem is almost impossible to debug.  In an upcoming column we’re going to look at this situation in more detail, and talk about ways to avoid it.<a href="#fnref:non-null" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:malloc-check">
      <p>We’ll be discusing how to use glibc’s error-checking in a future column.<a href="#fnref:malloc-check" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building clang on RedHat]]></title>
    <link href="http://btorpey.github.io/blog/2015/01/02/building-clang/"/>
    <updated>2015-01-02T00:00:00-05:00</updated>
    <id>http://btorpey.github.io/blog/2015/01/02/building-clang</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/timallen.jpg" width="230" height="240" title="" /></p>

<p>clang is a great compiler, with a boatload of extremely helpful tools, including static analysis, run-time memory and data race analysis, and many others.  And it’s apparently pretty easy to get those benefits on one of the supported platforms – basically Ubuntu and Mac (via XCode).</p>

<p>That’s fine, but if you get paid to write software, there’s a good chance it’s going to be deployed on RedHat, or one of its variants.  And, getting clang working on RedHat is a huge pain in the neck.  The good news is that I did the dirty work for you (ouch!), so you don’t have to.
<!--more-->
<br /></p>

<h2 id="bootstrapping-the-compiler">Bootstrapping the compiler</h2>

<p>Like almost all compilers, clang is written in a high-level language (in this case C++), so building clang requires a host compiler to do the actual compilation.  On Linux this is almost always gcc, since it is ubiquitous on Linux machines.  </p>

<p>There’s a hitch, though – as of version 3.3 some parts of clang are written in C++11, so the compiler used to compile clang needs to support the C++11 standard.</p>

<p>This is a real problem with RedHat, since the system compiler supplied with
RedHat 6 (the most recent version that is in wide use), is gcc 4.4.7.  That
compiler does not support C++11, and so is not able to compile clang.  So, the
first step is getting a C++11-compliant compiler so we can compile clang.  For
this example, we’re going to choose gcc 4.8.2, for a variety of reasons<sup id="fnref:5"><a href="#fn:5" rel="footnote">1</a></sup>.  The
good news is that gcc 4.8.2 is written in C++ 98, so we can build it using the
system compiler (gcc 4.4.7).  </p>

<p>The next thing we have to decide is where to install gcc 4.8.2, and we basically have these choices:</p>

<ul>
  <li>
    <p>We could install in /usr, where the new compiler would replace the system compiler.  Once we do that, though, we’ve effectively created a custom OS that will be required on all our development/QA/production machines going forward.  If “all our development/QA/production machines” == 1, this may not be a problem, but as the number increases things can get out of hand quickly. This approach also does not lend itself to being able to have more than one version of a particular package on a single machine, which is often helpful.</p>
  </li>
  <li>
    <p>We could install in /usr/local (the default for gcc, and many other packages when built from source), so the new compiler would coexist with the system compiler.  The problem with this approach is that /usr/local can (and in practice often does) rapidly turn into a dumping-ground for miscellaneous executables and libraries.  Which wouldn’t be so bad if we were diligent about keeping track of what they were and where they came from, but if we’re going to do that we might as well …</p>
  </li>
  <li>
    <p>Install somewhere else – it doesn’t really matter where, as long as there’s a convention. In this case, we’re going to use the convention that any software that is not bundled with the OS gets installed in /build/share/&lt;package&gt;/&lt;version&gt;.  This approach makes it easy to know exactly what versions of what software we’re running, since we need to specify its install directory explicitly in PATH and/or LD_LIBRARY_PATH.  It also makes it much easier to keep track of what everything is and where it came from.</p>
  </li>
</ul>

<p>Here’s a script that will download gcc 4.8.2 along with its prerequisites, build it and install it as per the convention we just discussed:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span> (build-gcc.sh)</span> <a href="http://btorpey.github.io/downloads/code/clang/build-gcc.sh">download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="c">#!/bin/bash </span>
</span><span class="line"><span class="nb">set</span> -exv
</span><span class="line">
</span><span class="line"><span class="c">## modify the following as needed for your environment</span>
</span><span class="line"><span class="c"># location where gcc should be installed</span>
</span><span class="line"><span class="nv">INSTALL_PREFIX</span><span class="o">=</span>/build/share/gcc/4.8.2
</span><span class="line"><span class="c"># number of cores</span>
</span><span class="line"><span class="nv">CPUS</span><span class="o">=</span><span class="k">$(</span>nproc<span class="k">)</span>
</span><span class="line"><span class="c"># uncomment following to get verbose output from make</span>
</span><span class="line"><span class="c">#VERBOSE=VERBOSE=1</span>
</span><span class="line"><span class="c"># uncomment following if you need to sudo in order to do the install</span>
</span><span class="line"><span class="c">#SUDO=sudo</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="c">## get everything</span>
</span><span class="line">wget http://www.netgull.com/gcc/releases/gcc-4.8.2/gcc-4.8.2.tar.bz2
</span><span class="line">wget https://gmplib.org/download/gmp/gmp-4.3.2.tar.bz2
</span><span class="line">wget http://www.multiprecision.org/mpc/download/mpc-0.8.1.tar.gz
</span><span class="line">wget http://www.mpfr.org/mpfr-2.4.2/mpfr-2.4.2.tar.bz2
</span><span class="line">wget ftp://gcc.gnu.org/pub/gcc/infrastructure/cloog-0.18.1.tar.gz
</span><span class="line">wget ftp://gcc.gnu.org/pub/gcc/infrastructure/isl-0.12.2.tar.bz2
</span><span class="line">
</span><span class="line"><span class="c">## untar gcc</span>
</span><span class="line">tar xf gcc-4.8.2.tar.bz2
</span><span class="line"><span class="c">## untar prereqs</span>
</span><span class="line"><span class="c"># gmp</span>
</span><span class="line">tar xf gmp-4.3.2.tar.bz2
</span><span class="line">mv gmp-4.3.2 gcc-4.8.2/gmp
</span><span class="line"><span class="c"># mpc</span>
</span><span class="line">tar xf mpc-0.8.1.tar.gz
</span><span class="line">mv mpc-0.8.1 gcc-4.8.2/mpc
</span><span class="line"><span class="c"># mpfr</span>
</span><span class="line">tar xf mpfr-2.4.2.tar.bz2
</span><span class="line">mv mpfr-2.4.2 gcc-4.8.2/mpfr
</span><span class="line"><span class="c"># cloog</span>
</span><span class="line">tar xf cloog-0.18.1.tar.gz
</span><span class="line">mv cloog-0.18.1 gcc-4.8.2/cloog
</span><span class="line"><span class="c"># isl</span>
</span><span class="line">tar xf isl-0.12.2.tar.bz2
</span><span class="line">mv isl-0.12.2 gcc-4.8.2/isl
</span><span class="line">
</span><span class="line"><span class="c"># build gcc</span>
</span><span class="line">rm -rf gcc
</span><span class="line">mkdir gcc
</span><span class="line"><span class="nb">cd </span>gcc
</span><span class="line">../gcc-4.8.2/configure --prefix<span class="o">=</span><span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span> --enable-languages<span class="o">=</span>c,c++ --disable-multilib
</span><span class="line">make -j <span class="k">${</span><span class="nv">CPUS</span><span class="k">}</span> <span class="k">${</span><span class="nv">VERBOSE</span><span class="k">}</span>
</span><span class="line">
</span><span class="line"><span class="c"># install it</span>
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> rm -rf <span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span>
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> make install
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To run the script, change to an empty directory and then simply invoke the
script.  If you want to keep track of all the commands and output related to the
build, you can invoke the script using the trick I wrote about in an 
<a href="http://btorpey.github.io/blog/2014/02/13/how-did-i-get-here/">earlier post</a>.</p>

<h2 id="preparing-to-build">Preparing to build</h2>

<p>Now that we’ve built gcc, we can get started building clang<sup id="fnref:1"><a href="#fn:1" rel="footnote">2</a></sup>.  By default,
clang is built to use the C++ standard library (libstdc++) that is included with
gcc. That’s the good news, since that means code generated using clang can be
intermixed freely with code generated with gcc – which is almost all the code
on a typical Linux machine.</p>

<h2 id="finding-the-c-standard-library">Finding the C++ standard library</h2>

<p>The libstdc++.so that is part of gcc is “versioned”, which means that different library versions can have different symbols defined. Since we chose to install gcc 4.8.2 in a non-standard location, there are several settings that need to be tweaked to have code find and use that version of libstdc++<sup id="fnref:4"><a href="#fn:4" rel="footnote">3</a></sup>.</p>

<p>Let’s start with a recap of how that works.</p>

<h3 id="finding-the-c-standard-library-at-build-time">Finding the C++ standard library at build-time</h3>

<p>With a default installation of gcc, everything is easy: gcc itself is in /usr/bin, include files are in /usr/include (sort of), and library files are in /usr/lib and/or /usr/lib64. In cases where files are not installed in these locations, gcc itself keeps track of where it should look for dependencies, and the following command will show these locations:</p>

<pre><code>&gt; g++ -E -x c++ - -v &lt; /dev/null
...
#include "..." search starts here:
#include &lt;...&gt; search starts here:
 /usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../include/c++/4.4.7
 /usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../include/c++/4.4.7/x86_64-redhat-linux
 /usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../include/c++/4.4.7/backward
 /usr/lib/gcc/x86_64-redhat-linux/4.4.7/include
 /usr/include
End of search list.
...
LIBRARY_PATH=/usr/lib/gcc/x86_64-redhat-linux/4.4.7/:/usr/lib/gcc/x86_64-redhat-linux/4.4.7/:/usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../lib64/:/lib/../lib64/:/usr/lib/../lib64/:/usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../:/lib/:/usr/lib/
</code></pre>

<p>With our non-standard installation of gcc 4.8.2, the same command shows the values appropriate for that version of the compiler:</p>

<pre><code>&gt; /build/share/gcc/4.8.2/bin/g++ -E -x c++ - -v &lt; /dev/null
...
#include "..." search starts here:
#include &lt;...&gt; search starts here:
 /shared/build/share/gcc/4.8.2/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.8.2/../../../../include/c++/4.8.2
 /shared/build/share/gcc/4.8.2/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.8.2/../../../../include/c++/4.8.2/x86_64-unknown-linux-gnu
 /shared/build/share/gcc/4.8.2/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.8.2/../../../../include/c++/4.8.2/backward
 /shared/build/share/gcc/4.8.2/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.8.2/include
 /shared/build/share/gcc/4.8.2/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.8.2/include-fixed
 /shared/build/share/gcc/4.8.2/bin/../lib/gcc/../../include
 /usr/include
...
LIBRARY_PATH=/shared/build/share/gcc/4.8.2/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.8.2/:/shared/build/share/gcc/4.8.2/bin/../lib/gcc/:/shared/build/share/gcc/4.8.2/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.8.2/../../../../lib64/:/lib/../lib64/:/usr/lib/../lib64/:/shared/build/share/gcc/4.8.2/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.8.2/../../../:/lib/:/usr/lib/
</code></pre>

<p>The situation with clang is a bit (!) more complicated. Not only does clang need
to be able to find its own include files and libraries, but it also needs to be
able to find the files for the compiler that clang is built with. In order
to successfully build clang with a non-standard compiler, we are going to need
to specify the following parameters to the clang build:</p>

<table>
<col width="35%" />
<col width="60%" />
<tbody>
<tr class="odd">
<td align="left"><p>CMAKE_C_COMPILER</p></td>
<td align="left"><p>The location of the C compiler to use.</p></td>
</tr>
<tr class="even">
<td align="left"><p>CMAKE_CXX_COMPILER</p></td>
<td align="left"><p>The location of the C++ compiler to use.</p></td>
</tr>
<tr class="odd">
<td align="left"><p>CMAKE_INSTALL_PREFIX</p></td>
<td align="left"><p>The location where the compiler should be installed.</p></td>
</tr>
<tr class="even">
<td align="left"><p>CMAKE_CXX_LINK_FLAGS</p></td>
<td align="left"><p>Additional flags to be passed to the linker for C++ programs.  See below for more information.</p></td>
</tr>
<tr class="odd">
<td align="left"><p>GCC_INSTALL_PREFIX</p></td>
<td align="left"><p>Setting this parameter when building clang is equivalent to specifying the <code>--gcc-toolchain</code> parameter when invoking clang. See below for more information.</p></td>
</tr>
</tbody>
</table>

<p><br /> 
While all these settings are documented in one place or another, as far as 
I know there is no single place that mentions them all.  (The clang developers apparently prefer writing code to writing
documentation ;-)  So, these settings have been cobbled together from a number
of sources (listed at the end of this article), and tested by much trial and
error.</p>

<p>The first three settings are plain-vanilla cmake settings, but the last two need some additional discussion:</p>

<h3 id="cmakecxxlinkflags">CMAKE_CXX_LINK_FLAGS</h3>

<p>In the clang build script this is set to <code>"-L${HOST_GCC}/lib64 -Wl,-rpath,${HOST_GCC}/lib64"</code>.  What this does is two-fold:</p>

<ul>
  <li>
    <p>The <code>-L</code> parameter adds the following directory to the search path for the linker.  This is needed so the linker can locate the libraries installed with gcc 4.8.2.        </p>
  </li>
  <li>
    <p>The <code>-Wl,-rpath,</code> parameter installs a <a href="http://en.wikipedia.org/wiki/Rpath">“run path”</a> into any executables (including shared libraries) created during the build.  This is needed so any executables created can find their dependent libraries at run-time. </p>

    <p>Note that you can display the run path for any executable (including shared libraries) with the following command:</p>

    <pre><code>&gt; objdump -x /build/share/clang/trunk/bin/clang++ | grep RPATH
RPATH                /build/share/gcc/4.8.2/lib64:$ORIGIN/../lib
</code></pre>
  </li>
</ul>

<h3 id="gccinstallprefix">GCC_INSTALL_PREFIX</h3>

<p>Unfortunately, by default, clang looks for include and library files in the standard system locations (e.g., /usr), regardless of what compiler was used to build clang. (I filed a <a href="http://llvm.org/bugs/show_bug.cgi?id=20510">bug report</a> for this behavior, but the clang developers apparently feel this is reasonable behavior. Reasonable people may disagree ;-)</p>

<p>The work-around for this is to specify GCC_INSTALL_PREFIX when building clang – this tells the clang build where the gcc that is being used to build clang is located. Among other things, this determines where the clang compiler will look for system include and library files at compile and link time.</p>

<h2 id="building-clang">Building clang</h2>

<p>Now that we have that out of the way, we can build clang. The following script will download clang source from svn, build and install it.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span> (build-clang.sh)</span> <a href="http://btorpey.github.io/downloads/code/clang/build-clang.sh">download</a></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
</pre></td><td class="code"><pre><code class="sh"><span class="line"><span class="c">#!/bin/bash</span>
</span><span class="line"><span class="nb">set</span> -exv
</span><span class="line">
</span><span class="line"><span class="c">## modify the following as needed for your environment</span>
</span><span class="line"><span class="c"># location where clang should be installed</span>
</span><span class="line"><span class="nv">INSTALL_PREFIX</span><span class="o">=</span>/build/share/clang/trunk
</span><span class="line"><span class="c"># location of gcc used to build clang</span>
</span><span class="line"><span class="nv">HOST_GCC</span><span class="o">=</span>/build/share/gcc/4.8.2
</span><span class="line"><span class="c"># number of cores</span>
</span><span class="line"><span class="nv">CPUS</span><span class="o">=</span><span class="k">$(</span>nproc<span class="k">)</span>
</span><span class="line"><span class="c"># uncomment following to get verbose output from make</span>
</span><span class="line"><span class="c">#VERBOSE=VERBOSE=1</span>
</span><span class="line"><span class="c"># uncomment following if you need to sudo in order to do the install</span>
</span><span class="line"><span class="c">#SUDO=sudo</span>
</span><span class="line">
</span><span class="line"><span class="c">#</span>
</span><span class="line"><span class="c"># gets clang tree from svn into ./llvm</span>
</span><span class="line"><span class="c"># params (e.g., -r) can be specified on command line</span>
</span><span class="line"><span class="c">#</span>
</span><span class="line">rm -rf llvm
</span><span class="line"><span class="c">## get everything</span>
</span><span class="line"><span class="c"># llvm</span>
</span><span class="line">svn co <span class="nv">$*</span> http://llvm.org/svn/llvm-project/llvm/trunk llvm
</span><span class="line"><span class="c"># clang</span>
</span><span class="line"><span class="nb">cd </span>llvm/tools
</span><span class="line">svn co <span class="nv">$*</span> http://llvm.org/svn/llvm-project/cfe/trunk clang
</span><span class="line"><span class="nb">cd</span> -
</span><span class="line"><span class="c"># extra</span>
</span><span class="line"><span class="nb">cd </span>llvm/tools/clang/tools
</span><span class="line">svn co <span class="nv">$*</span> http://llvm.org/svn/llvm-project/clang-tools-extra/trunk extra
</span><span class="line"><span class="nb">cd</span> -
</span><span class="line"><span class="c"># compiler-rt</span>
</span><span class="line"><span class="nb">cd </span>llvm/projects
</span><span class="line">svn co <span class="nv">$*</span> http://llvm.org/svn/llvm-project/compiler-rt/trunk compiler-rt
</span><span class="line"><span class="nb">cd</span> -
</span><span class="line">
</span><span class="line"><span class="c">## build clang w/gcc installed in non-standard location</span>
</span><span class="line">rm -rf clang
</span><span class="line">mkdir -p clang
</span><span class="line"><span class="nb">cd </span>clang
</span><span class="line">cmake -DCMAKE_C_COMPILER<span class="o">=</span><span class="k">${</span><span class="nv">HOST_GCC</span><span class="k">}</span>/bin/gcc -DCMAKE_CXX_COMPILER<span class="o">=</span><span class="k">${</span><span class="nv">HOST_GCC</span><span class="k">}</span>/bin/g++ -DGCC_INSTALL_PREFIX<span class="o">=</span><span class="k">${</span><span class="nv">HOST_GCC</span><span class="k">}</span> -DCMAKE_CXX_LINK_FLAGS<span class="o">=</span><span class="s2">&quot;-L${HOST_GCC}/lib64 -Wl,-rpath,${HOST_GCC}/lib64&quot;</span> -DCMAKE_INSTALL_PREFIX<span class="o">=</span><span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span> -DLLVM_ENABLE_ASSERTIONS<span class="o">=</span>ON -DCMAKE_BUILD_TYPE<span class="o">=</span><span class="s2">&quot;Release&quot;</span> -DLLVM_TARGETS_TO_BUILD<span class="o">=</span><span class="s2">&quot;X86&quot;</span> ../llvm
</span><span class="line">make -j <span class="k">${</span><span class="nv">CPUS</span><span class="k">}</span> <span class="k">${</span><span class="nv">VERBOSE</span><span class="k">}</span>
</span><span class="line">
</span><span class="line"><span class="c"># install it</span>
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> rm -rf <span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span>
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> make install
</span><span class="line">
</span><span class="line"><span class="c"># we need some addl bits that are not normally installed</span>
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> cp -p ../llvm/tools/clang/tools/scan-build/scan-build  <span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span>/bin
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> cp -p ../llvm/tools/clang/tools/scan-build/ccc-analyzer  <span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span>/bin
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> cp -p ../llvm/tools/clang/tools/scan-build/c++-analyzer  <span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span>/bin
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> cp -p ../llvm/tools/clang/tools/scan-build/sorttable.js  <span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span>/bin
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> cp -p ../llvm/tools/clang/tools/scan-build/scanview.css <span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span>/bin
</span><span class="line"><span class="k">${</span><span class="nv">SUDO</span><span class="k">}</span> cp -rp ../llvm/tools/clang/tools/scan-view/*  <span class="k">${</span><span class="nv">INSTALL_PREFIX</span><span class="k">}</span>/bin
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Note that you can specify a parameter to the script (e.g., <code>-r 224019</code>) to get a specific version of clang from svn.</p>

<blockquote>
  <p>Since this article was originally published, there have been some changes to the prerequisites for building clang:
you will need cmake 2.8.12.2 or later, and python 2.7 or later. </p>
</blockquote>

<h2 id="building-using-clang">Building using clang</h2>

<p>At this point, we should have a working clang compiler that we can use to build and run our own code. But once again, because the “host” gcc (and libstdc++) are installed in a non-standard location, we need to tweak a couple of build settings to get a successful build.</p>

<h3 id="specifying-the-compiler-to-use">Specifying the compiler to use</h3>

<p>There are a bunch of ways to specify the compiler, depending on what build system you’re using – I’ll mention a couple of them here.</p>

<p>If you’re using make, you can prefix the make command as follows:</p>

<pre><code>CC=clang CXX=clang++ make ... 
</code></pre>

<p>If you’re using cmake you can <a href="http://www.cmake.org/Wiki/CMake_FAQ#How_do_I_use_a_different_compiler.3F">specify the compiler to use</a> on the cmake command line, as follows:</p>

<pre><code>cmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ ... 
</code></pre>

<p>Personally, I find that ridiculously inconvenient, so in my CMakeLists.txt file I specify the compiler directly:</p>

<pre><code># cmake doc says this is naughty, but their suggestions are even worse...
if("$ENV{COMPILER}" STREQUAL "gcc")
  set(CMAKE_C_COMPILER      gcc)
  set(CMAKE_CXX_COMPILER    g++)
elseif("$ENV{COMPILER}" STREQUAL "clang")
  set(CMAKE_C_COMPILER      clang)
  set(CMAKE_CXX_COMPILER    clang++)
endif()
</code></pre>

<p>In any of the above, you can either specify the full path to the compiler, or just specify the name of the compiler executable (as above), and make sure that the executable is on your PATH.</p>

<p>Last but not least, if you’re using GNU autotools – you’re on your own, good luck! The only thing I want to say about autotools is that <a href="https://twitter.com/timmartin2/status/23365017839599616">I agree with this guy</a>.</p>

<h3 id="finding-the-c-standard-library-at-run-time">Finding the C++ standard library at run-time</h3>

<p>Any code genrated using clang is also going to need to be able to find the libraries that clang was built with at run-time. There are a couple of ways of doing that:</p>

<ul>
  <li>
    <p>Similar to what we did above when building clang, you can specify the <code>-Wl,-rpath,</code> parameter to the linker to set a run path for your executables.
 Note that if you’re using cmake, it will <a href="http://www.cmake.org/Wiki/CMake_RPATH_handling">automatically strip the rpath</a> from all files when running <code>make install</code>, so you may need to disable that by setting <code>CMAKE_SKIP_INSTALL_RPATH</code> to false in your build.</p>
  </li>
  <li>
    <p>Alternatively, you will need to make sure that the proper library directory is on your <code>LD_LIBRARY_PATH</code> at run-time<sup id="fnref:3"><a href="#fn:3" rel="footnote">4</a></sup>.</p>
  </li>
</ul>

<h2 id="so-what-could-possibly-go-wrong">So, What Could Possibly Go Wrong?</h2>

<p>If you’ve followed the directions above, you should be good to go, but be warned that, just like in “Harry Potter”, messing up any part of the spell can cause things to go spectacularly wrong. Here are a few examples:</p>

<ul>
  <li>If you try to use the system compiler (gcc 4.4.7) to build clang, you’ll get an error like the following:</li>
</ul>

<!-- -->

<pre><code>CMake Error at cmake/modules/HandleLLVMOptions.cmake:17 (message):
  Host GCC version must be at least 4.7!
</code></pre>

<ul>
  <li>The clang build executes code, that was built with clang, as part of the build step. If clang can’t find the correct (i.e., gcc 4.8.2) version of libstdc++ at build time, you will see an error similar to the following:</li>
</ul>

<!-- -->

<pre><code>Linking CXX static library ../../../../lib/libclangAnalysis.a
[ 51%] Built target clangAnalysis
[ 51%] Building CXX object tools/clang/lib/Sema/CMakeFiles/clangSema.dir/SemaConsumer.cpp.o
[ 51%] Building CXX object tools/clang/lib/ARCMigrate/CMakeFiles/clangARCMigrate.dir/TransAutoreleasePool.cpp.o
[ 51%] Building CXX object tools/clang/lib/AST/CMakeFiles/clangAST.dir/ExprConstant.cpp.o
Scanning dependencies of target ClangDriverOptions
[ 51%] Building Options.inc...
../../../../../bin/llvm-tblgen: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.15' not found (required by ../../../../../bin/llvm-tblgen)
</code></pre>

<ul>
  <li>If you build clang without specifying the <code>-Wl,-rpath</code> parameter, clang won’t be able to find the libraries it needs at compile-time:</li>
</ul>

<!-- -->

<pre><code>&gt; clang++ $* hello.cpp &amp;&amp; ./a.out
clang++: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.14' not found (required by clang++)
clang++: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by clang++)
clang++: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.15' not found (required by clang++)
</code></pre>

<ul>
  <li>If the GCC_INSTALL_PREFIX setting isn’t specified when you build with clang, it will look for system files in /usr, rather than the proper directory, and you will see something like this:</li>
</ul>

<!-- -->

<pre><code>&gt; clang++ $* hello.cpp &amp;&amp; ./a.out
In file included from hello.cpp:1:
In file included from /usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../include/c++/4.4.7/iostream:40:
In file included from /usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../include/c++/4.4.7/ostream:40:
In file included from /usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../include/c++/4.4.7/ios:40:
In file included from /usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../include/c++/4.4.7/exception:148:
/usr/lib/gcc/x86_64-redhat-linux/4.4.7/../../../../include/c++/4.4.7/exception_ptr.h:143:13: error: unknown type name 'type_info'
      const type_info*
            ^
1 error generated.
</code></pre>

<p>I’ve probably missed a couple, but you get the idea.</p>

<h2 id="conclusion">Conclusion</h2>

<p>There may be another way to build clang successfully on a RH-based system, but
if there is I’ve yet to discover it. As mentioned earlier, bits and pieces of
this information have been found in other sources, including the following:</p>

<p><a href="http://llvm.org/docs/GettingStarted.html#getting-a-modern-host-c-toolchain">http://llvm.org/docs/GettingStarted.html#getting-a-modern-host-c-toolchain</a></p>

<p><a href="http://clang-developers.42468.n3.nabble.com/getting-clang-to-find-non-default-libstdc-td3945163.html">http://clang-developers.42468.n3.nabble.com/getting-clang-to-find-non-default-libstdc-td3945163.html</a></p>

<p><a href="https://github.com/google/sanitizers/wiki/MemorySanitizerBuildingClangOnOlderSystems">https://github.com/google/sanitizers/wiki/MemorySanitizerBuildingClangOnOlderSystems</a></p>

<p><a href="http://llvm.org/docs/CMake.html">http://llvm.org/docs/CMake.html</a>
 </p>

<div class="footnotes">
  <ol>
    <li id="fn:5">
      <p>One reason is that gcc 4.9.0 can’t compile libc++, the llvm version of the C++ standard library – see <a href="http://lists.llvm.org/pipermail/cfe-dev/2014-April/036650.html">http://lists.llvm.org/pipermail/cfe-dev/2014-April/036650.html</a> for more detail.  While we’re not going to discuss using libc++ in this post, we may get into that later on.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>You will need at least version 2.8.12.2 of cmake to do the build, which is not native on RH/CentOS 6.  That version can be installed using “Add/Remove Software” or yum.  (Or, of course, you can build it from source). You will also need python 2.7 or later, which is probably better built from source, since the RH repos apparently use the non-standard name “python27” for the executable.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>I may go into more detail on this in a later post, but in the meantime if you’re interested you should consult Ulrich Drepper’s “How to Write Shared Libraries” at <a href="http://www.akkadia.org/drepper/dsohowto.pdf">http://www.akkadia.org/drepper/dsohowto.pdf</a>.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>This is the approach we use in my shop – we have a hard-and-fast rule that application code cannot contain a run path, and we deliberately strip any existing RPATH entries from code that is being deployed to QA and production as a security measure.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Into the Void]]></title>
    <link href="http://btorpey.github.io/blog/2014/09/23/into-the-void/"/>
    <updated>2014-09-23T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/09/23/into-the-void</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/m20.jpg" width="385" height="233" /> </p>

<p>I keep reading talk of the sort “I don’t know why anyone bothers
with C++ — real programmers use C.  C++ is for wussies”, or words to that
effect.</p>

<p>Well, a while ago I had to go back to C from working exclusively in C++ for a while, and
I have to say that I think the C fanboys are just nuts.</p>

<!--more-->

<p>The project I’m referring to involved packaging up NYSE’s (now SR Labs’) “Mama” middleware so
it could be released as <a href="http://www.openmama.org/">open source</a>, as well as implementing a new transport
adapter for OpenMama using the open-source <a href="http://avis.sourceforge.net/why_avis.html">Avis</a> transport<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<p>Mama is a high-level API that provides access to a number of middleware
transports, including Tibco Rendezvous, Informatica/29 West LBM and NYSE’s own
Data Fabric middleware.  Mama and Data Fabric are almost exclusively C code,
written back in the days when people avoided C++ because of issues with the
various compilers.  (Does anyone remember the fun we used to have with gcc 2.95
and templates?)</p>

<p>So, at the time using C may have been the right choice, but it’s far from ideal.</p>

<p>Like a lot of C code, what Mama does is encapsulate functionality by using
pointers to opaque structs.  These ”handles” are created by calling API
functions, and then later passed to other API functions to perform actions on
the underlying objects represented by the handles.</p>

<p>This is a very popular idiom, and with good reason — hiding the inner details of
the implementation insulates applications from changes in the implementation.
It’s called <a href="http://en.wikipedia.org/wiki/Bridge_pattern">“Bridge”</a> by the GOF, and the more 
colorful <a href="http://www.gotw.ca/gotw/024.htm">“pImpl”</a> by Herb
Sutter.</p>

<p>Of course, in C the typical way to accomplish this is with 
<a href="http://stackoverflow.com/questions/1043034/what-does-void-mean-in-c-c-and-c">void</a>
 pointers, so the
implementation spends a lot of time casting back and forth between <code>void*</code>’s and
“real” pointers.  With, of course, absolutely no error checking by the compiler.</p>

<p>For example, in the Avis protocol bridge that I implemented for the initial
release of OpenMama, there are a bunch of macros that look like this:</p>

<pre><code>#define avisPublisher(publisher) ((avisPublisherBridge*) publisher)
</code></pre>

<p>Elsewhere, the code that uses the macro:</p>

<pre><code>    mamaMsg_updateString(msg, SUBJECT_FIELD_NAME, 0, avisPublisher(publisher)-&gt;mSubject);
</code></pre>

<p>Gee, wouldn’t it be nice to be able to define these handles in such a way that
they would be opaque to the applications using the API, but the compiler could
still enforce type-checking?  Not to mention not having to cast back and forth
between <code>void*</code>’s and actual types?</p>

<p>Never mind virtual functions, forget streams (please!) and the STL, ditto templates and
operator overloading — if there’s one overriding reason to prefer C++ over C,
it’s the compiler’s support for separating interface from implementation that is
completely lacking in C.</p>

<p>You see this same “handle” pattern everywhere in C, and it’s “good” C code just
because it’s the best that can be done, but if a programmer wrote that code in
C++ he’d be laughed out of the building (and rightly so).</p>

<p>Has C++ become big and complicated?  Sure.  Is the syntax sometimes capricious
and counter-intuitive?  Absolutely.</p>

<p>But, at least for me, if I never see another <code>void*</code> as long as I live, that won’t
be too long for me.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Don’t ask.  Let’s just say it wasn’t my decision.  If you want to check out OpenMama, I would suggest using <a href="http://qpid.apache.org/proton/">Qpid/AMQP</a> instead.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Repent, Sinner!]]></title>
    <link href="http://btorpey.github.io/blog/2014/09/21/repent/"/>
    <updated>2014-09-21T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/09/21/repent</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/nun-with-ruler.jpg" width="240" height="160" /> </p>

<p>When I was a kid I went to Catholic school, and back in those days 
the nuns would indeed rap your knuckles with a ruler if you
misbehaved. That doesn’t happen so much any more, but when I see someone 
making use of the <a href="http://c2.com/cgi/wiki?CopyAndPasteProgramming">copy-paste anti-pattern</a>, 
I’m tempted to reach for a ruler myself. 
(I know, probably not a good career move ;-)</p>

<p>Short of rapping someone’s knuckles with a ruler, though, how do you show some poor sinner the error of his ways?</p>

<!--more-->

<p>Enter <a href="http://pmd.sourceforge.net/pmd-5.1.3/cpd-usage.html">CPD, or copy-paste detector</a>. 
This does pretty much what you would guess from its name – it
spins through all the code you give it, and analyzes it for repeated sequences.
<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>Here’s an example of running the GUI version against the code I used 
in an <a href="http://btorpey.github.io/blog/2014/02/12/shared-singletons/">earlier post</a> on smart pointers.</p>

<p><img class="right" src="http://btorpey.github.io/images/cpd.png" /></p>

<p>(Note that the “Ignore literals” and “Ignore identifiers” checkboxes are
disabled if you select C++ as the language - these options <a href="http://sourceforge.net/p/pmd/discussion/188193/thread/91553283">are only implemented for Java currently</a>).</p>

<p>The site has several more examples, but <a href="http://pmd.sourceforge.net/pmd-5.1.3/cpdresults.txt">this one</a> just blew my mind – 
hard to imagine how anyone could write this code in
the first place, much less be so confident that it is correct that they just
copy and paste it in two different files (with nary a comment to tie the two
together)?</p>

<pre>
=====================================================================
Found a 19 line (329 tokens) duplication in the following files: 
Starting at line 685 of /usr/local/java/src/java/util/BitSet.java
Starting at line 2270 of /usr/local/java/src/java/math/BigInteger.java
    static int bitLen(int w) {
        // Binary search - decision tree (5 tests, rarely 6)
        return
         (w &lt; 1&lt;&lt;15 ?
          (w &lt; 1&lt;&lt;7 ?
           (w &lt; 1&lt;&lt;3 ?
            (w &lt; 1&lt;&lt;1 ? (w &lt; 1&lt;&lt;0 ? (w&lt;0 ? 32 : 0) : 1) : (w &lt; 1&lt;&lt;2 ? 2 : 3)) :
            (w &lt; 1&lt;&lt;5 ? (w &lt; 1&lt;&lt;4 ? 4 : 5) : (w &lt; 1&lt;&lt;6 ? 6 : 7))) :
           (w &lt; 1&lt;&lt;11 ?
            (w &lt; 1&lt;&lt;9 ? (w &lt; 1&lt;&lt;8 ? 8 : 9) : (w &lt; 1&lt;&lt;10 ? 10 : 11)) :
            (w &lt; 1&lt;&lt;13 ? (w &lt; 1&lt;&lt;12 ? 12 : 13) : (w &lt; 1&lt;&lt;14 ? 14 : 15)))) :
          (w &lt; 1&lt;&lt;23 ?
           (w &lt; 1&lt;&lt;19 ?
            (w &lt; 1&lt;&lt;17 ? (w &lt; 1&lt;&lt;16 ? 16 : 17) : (w &lt; 1&lt;&lt;18 ? 18 : 19)) :
            (w &lt; 1&lt;&lt;21 ? (w &lt; 1&lt;&lt;20 ? 20 : 21) : (w &lt; 1&lt;&lt;22 ? 22 : 23))) :
           (w &lt; 1&lt;&lt;27 ?
            (w &lt; 1&lt;&lt;25 ? (w &lt; 1&lt;&lt;24 ? 24 : 25) : (w &lt; 1&lt;&lt;26 ? 26 : 27)) :
            (w &lt; 1&lt;&lt;29 ? (w &lt; 1&lt;&lt;28 ? 28 : 29) : (w &lt; 1&lt;&lt;30 ? 30 : 31)))));
    }

</pre>

<p>So, if you need to lead someone to the light, try PMD’s copy-paste detector.  It
may hurt a bit, but a lot less than a sharp rap on the knuckles!</p>

<p>One last caveat about CPD: it does not like symlinks at all – you must give it the real path names for any source files, or
you will just get a bunch of “Skipping … since it appears to be a symlink” messages.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>CPD is part of the <a href="http://pmd.sourceforge.net/pmd-5.1.3/">PMD tool</a>, which can do a lot of useful things with Java code. But since I’m primarily dealing with C++ code these days (and because duplicate code is such a hot-button issue for me), CPD is the part that I use.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Status Meetings]]></title>
    <link href="http://btorpey.github.io/blog/2014/08/21/status-reports/"/>
    <updated>2014-08-21T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/08/21/status-reports</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/the-office-boring-meeting1.jpg" width="262" height="211" /> </p>

<p>One of the banes of corporate life is the status meeting.  It would be nice to get rid of them, but then it would be nice to 
get rid of all the lawyers too<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, and I don’t see that happening either.  </p>

<p>So, how do we make them better?
Well, for starters we could make them shorter.  Here’s a way to do that.  </p>

<!--more-->

<p>I’ve had two really good managers in my career (although I have no idea whether
that means I’ve been blessed or cursed ;-) I learned different things from each of my managers – some of them pragmatic and
some more in the “touchy-feely” category. This one is purely pragmatic (which
is not to say that is all I learned from him, but those stories are for another day).</p>

<p>My manager (I’ll call him Mike, because that’s his name) set up this structure
for a trading system project I was working on under his direction. We would have
a status meeting on Monday mornings, which would set the agenda for the coming
week, and with the following structure in place that meeting would typically take no more
than 30 minutes. At the end, everybody knew what they needed to do, and we all
got to work with a clear purpose.</p>

<p>In preparation for this meeting, each person would create a status report, which consisted of the following categories, no more
and no less:</p>

<h3 id="planned-and-accomplished">Planned and Accomplished</h3>

<p>This is where you list the tasks that you planned to get done during the
period, and that were actually completed. And, by completed we’re not talking
about 90% – once a task gets into this category, you don’t expect to see it
again – if you do, then it wasn’t really complete in the first place.</p>

<p>This sort of implies that tasks can be accomplished in one iteration, and so naturally
forces you to think in terms of discrete quanta of accomplishment.  To paraphrase Woody Allen,
“A project is like a shark.  It has to constantly move forward or it dies”.  </p>

<h3 id="unplanned-and-accomplished">Unplanned and Accomplished</h3>

<p>Of course, in the real world things never go as smoothly as they are supposed
to. (Another way of saying this is: “In theory there is no difference between
theory and practice. In practice there is.”) So, this is where you list those
items that came up “out of the blue”, but which you had to deal with. An example
might be production problems, customer engagements, or other things that are
unpredictable.</p>

<h3 id="planned-and-not-accomplished">Planned and Not Accomplished</h3>

<p>You would like this category to be empty much of the time, but that doesn’t
always happen. Certain tasks may have dependencies on external agents over which
you have no control, for instance – so even if you’re the world’s best planner,
you’re going to get tasks in here from time to time. Typically you would also
include a brief explanation of why something didn’t happen that was supposed to,
which might feed into the “Issues for Management Attention” section later.</p>

<h3 id="planned-for-next-period">Planned for Next Period</h3>

<p>This is where you list the tasks that you intend to complete during the next
iteration. Some may be new, and if there’s anything in the “Planned and Not
Accomplished” section, those would generally show up here as well until they are
completed. Similarly, everything that appears here will end up in either
“Planned and Accomplished” or “Planned and Not Accomplished” next time.</p>

<h3 id="issues-for-management-attention">Issues for Management Attention</h3>

<p>Here’s where you discuss any “blockers” that are preventing you from
accomplishing your goals. In many cases, this will include dependencies on
external entities, and these provide an opportunity for your manager to exercise his
or her persuasive powers on other parts of the organization.</p>

<p>And that’s it – just five categories, with a real focus on what’s truly important.</p>

<p>After learning this technique, I’ve used it with great success in all sorts of
situations, and have also passed it along to others, who have all been quite happy with it. 
It strikes just the right balance between too much and too little
information, and also focuses the information in a way that is most useful.</p>

<p>On Mike’s project, we had these meetings on Monday mornings, but Friday also
works – that way you get it out of the way and come into the new week rarin’ to
go.</p>

<p>If you’re a manager, give this a try and see if it doesn’t streamline one of
your more onerous tasks. If you’re not a manager, then you probably have one,
and you can suggest using this for your status reports. Or, you can just use
this template to help you focus yourself on what you need to be doing.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Actually, some of my best friends are lawyers, so this doesn’t apply to them ;-)<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Guess What Day It Is!]]></title>
    <link href="http://btorpey.github.io/blog/2014/07/23/perl-stdin/"/>
    <updated>2014-07-23T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/07/23/perl-stdin</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/phl.pm.org-camel.png" width="320" height="240" /> </p>

<p>No, not that – it’s Perl day.  (Well, actually it’s just Wednesday, but you get the idea).</p>

<p>Sometimes it seems that everybody likes to hate on Perl, but I think their
animus is misdirected. It’s not Perl that’s the problem, it’s those \^\$(.#!)?$
regular expressions.</p>

<p>Or, as Jamie Zawinski once said 
<a href="http://en.wikiquote.org/wiki/Jamie_Zawinski">“Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems.”</a>.</p>

<p>Well, I’m here to tell you that it’s possible to write whole Perl programs that
actually accomplish useful work, <strong>without any regular expressions at all</strong>! And, if you do
that, you can actually <em>read the code!</em></p>

<p>It turns out that Perl is a dandy scripting language, and while some may take issue
with its flexibility (“There’s more than one way to do it”), others (including me) find that flexibility very useful.</p>

<!--more-->

<p>One example of that flexibility is how easy it is to create a Perl program that
can read input either from stdin, or from a file specified on the command line.</p>

<pre><code>local *INFILE;
if (defined($ARGV[0])) {
    open(INFILE, "&lt;:crlf", "$ARGV[0]") or die "Cant open $ARGV[0]\n";
}
else {
    *INFILE = *STDIN;
}

while (&lt;INFILE&gt;) {
}

close(INFILE);
</code></pre>

<p>The above snippet does just that, and also works well with command-line parsers
(e.g., <code>GetOpt</code>) that eat their parameters by removing them from the ARGV
array.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Where Am I?]]></title>
    <link href="http://btorpey.github.io/blog/2014/05/29/where-am-i/"/>
    <updated>2014-05-29T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/05/29/where-am-i</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/gilligans-island-tv-show.jpg" width="320" height="240" /> </p>

<p>From <em>Robinson Crusoe</em> to <em>Gilligan’s Island</em> to <em>Lost</em>, tales of being
stranded on a desert island seem to resonate with people in a special way. Some
of that likely has to do with the exotic locales, and the practical challenges
of getting water, food and shelter.</p>

<p>But an even more basic part is the unanswered question: “Where am I?” that makes
things so – well, <em>mysterious</em>.</p>

<p>Shell scripting can be pretty mysterious too at times, but in this installment
we’ll learn how to answer that basic question of “Where am I?” to make shell
scripting a little less mysterious.</p>

<!--more-->

<p>One of the tenets of the Unix way is brevity, and one consequence of that is
that well-behaved programs should be able to find whatever other resources they
need without having to be told where they are. Windows attempts to
solve this problem with the (gack!) registry, but Unix tends to use a simpler
approach: needed resources are placed either in well-known locations (e.g., /etc
for system programs), or where they can be found relative to the location of the
program itself.</p>

<p>Another attribute of a well-behaved Unix program is that it
should be able to run from any location, whether it’s invoked with a full path,
or found via the PATH variable.</p>

<p>So, how do we reconcile those two requirements? And specifically, how do we do
that in shell scripts? Since – regardless of what your “main” language is, if
you’re programming in Unix/Linux, you’re probably also writing a boatload of
shell scripts too.</p>

<p>It turns out that, at least in bash, there is a
simple but non-obvious way to do get the location of the script file itself,
which goes something like this:</p>

<pre><code>SCRIPT_DIR=$(cd $(dirname ${BASH_SOURCE}) &amp;&amp; /bin/pwd) 
</code></pre>

<p>Let’s follow this through and see how it works:</p>

<ul>
  <li>
    <p>The <code>$( ... )</code> construct invokes a sub-shell. This is handy since it
allows us to change the environment of the sub-shell (e.g., current directory)
without affecting the current environment.</p>
  </li>
  <li>
    <p><code>$BASH_SOURCE</code> is a builtin variable that gives us the path to the shell
script itself. For instance, if we invoke a script with <code>./scriptname.sh</code>,
then that’s what will end up in <code>${BASH_SOURCE}</code>.</p>
  </li>
  <li>
    <p>To get the full path then we extract just the path part with <code>dirname</code>, again
in a sub-shell.</p>
  </li>
  <li>We then <code>cd</code> into that directory, and if successful get the full pathname
with <code>/bin/pwd</code>.
    <ul>
      <li>Note that we use <code>/bin/pwd</code> to get the path. This version resolves any
symbolic links to return the actual physical path. There is also a <code>pwd</code>
built-in to bash, but that one does not expand symbolic links by default.
<br /></li>
    </ul>
  </li>
  <li>Finally, the result is assigned to SCRIPT_DIR.</li>
</ul>

<p>We now have the full path of the script file itself, and can use that to locate
any other resources needed by the script. For a real-world example, you can
check out the <a href="https://github.com/btorpey/latency-utils.git">these scripts</a> from
 <a href="http://btorpey.github.io/blog/2014/05/16/visualizing-latency/">my earlier post on visualizing latency</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Visualizing Latency]]></title>
    <link href="http://btorpey.github.io/blog/2014/05/16/visualizing-latency/"/>
    <updated>2014-05-16T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/05/16/visualizing-latency</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/ping.png" width="320" height="240" /> </p>

<p>I’m a visual thinker (I think I may have <a href="http://btorpey.github.io/blog/2014/04/29/a-picture-is-worth-1k-words/">mentioned that before</a> ),
so when I’m analyzing performance, latency, etc. I find it really helpful to be
able to visualize what is going on on the machine. </p>

<p>As a result, I had gotten
reasonably good at using Excel to produce charts, which sometimes helped to correlate observed
behaviors like latency spikes with other events on the machine.</p>

<p>For a bunch of reasons I wanted to move away from Excel, though, and find
another tool that would give me the same or better functionality.</p>

<!--more-->

<p>For one thing, a little over a year ago I switched to a Mac as my main machine
after years of using Windows. There was a certain amount of adjustment, but for
the most part it’s been smooth sailing. More than that, I was actually able to
recapture some of the fun and excitement I remember from my first Apple (an
Apple ][).</p>

<p>I also wanted something that would run on both the Mac and Linux, where I do
most of my testing. Last but not least, I wanted something that would be
scriptable so I could easily produce consistent charts for multiple test runs.</p>

<p>I looked briefly at R, but ditched it when it used up all the 8GB in my laptop,
plus the entire hard disk as swap, for a single dataset of 100,000 points.
Probably my bad, but I didn’t have the patience to figure out what I might be
doing wrong.</p>

<p>At that point I turned to venerable (some would say crusty) gnuplot. It’s a bit
long in the tooth, but I just wanted to plot latency over time, so how hard
could that be? Well, I guess it’s pretty easy if you already know how, but
starting from scratch is another story.</p>

<p>Which brings me to my rant of the day, directed at techies in general, and to the
(us?) Linux/Unix techies in particular.</p>

<p>Short version: I don’t want to learn gnuplot. I don’t even want to <em>have
learned</em> gnuplot – even if I could do that by just taking a pill. What I want
is to be able to produce decent-looking charts <em>without</em> knowing <em>anything</em>
about gnuplot.</p>

<p>To be fair, the gnuplot docs did have some examples – more anyway than you
would find in a typical man page, although that’s admittedly a low bar. And
while my google-fu is usually pretty good, I just couldn’t find anything on the
intertubes that would work for me, so I had to learn <em>just a little</em> gnuplot.</p>

<blockquote>
  <p>When all else fails, read the instructions.</p>
</blockquote>

<p>It turns out that gnuplot works pretty well, and will probably work even better
once I learn (sigh) how to use it better.</p>

<p>But you don’t have to learn diddly if you don’t want to. <a href="https://github.com/btorpey/latency-utils.git">Here is the first</a> in
what will hopefully be a series of recipes that you can use with little or no
modification.  Once you’ve downloaded the repo, enter the following at the command prompt:</p>

<p><code>./tsd.sh ping.csv x11</code></p>

<p>Which should result in something like this:</p>

<p><img class="center" src="http://btorpey.github.io/images/gnuplot.png" /> </p>

<p>It’s primitive, but that very primitiveness has its own appeal, especially for
those of us for whom “UI” means bash, vi or emacs.</p>

<p>A couple of points about the gnuplot command files:</p>

<ul>
  <li>
    <p>Sometimes you care about the actual time that an event took place, so you
can correlate it with some other event; sometimes you don’t. Accordingly, I’ve
created two different files: one which displays actual time (ts.gp), the other
which calculates and displays deltaT (tsd.gp).</p>
  </li>
  <li>
    <p>I’ve been programming in C (and later C++) for many years, but I don’t think
I’ve ever purposely used the comma operator before. Well, expressions in gnuplot
follow C language rules for operators, precedence, etc. and that comma operator
turns out to be handy – in this case it lets us 
update the origin in the same expression that calculates deltaT.
(The return value of the comma
operator is the right-hand expression).</p>
  </li>
</ul>

<p>– (Note that the above requires something like gnuplot 4.6)</p>

<ul>
  <li>I’ve left the default terminal in the gnuplot command files, but you can 
specify a different one on the command line.  To get a list of terminals supported 
in your version:
<code>gnuplot -e "set terminal"</code>.</li>
</ul>

<p>Comments, suggestions, pull requests, etc. welcome.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Picture is Worth 1K Words]]></title>
    <link href="http://btorpey.github.io/blog/2014/04/29/a-picture-is-worth-1k-words/"/>
    <updated>2014-04-29T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/04/29/a-picture-is-worth-1k-words</id>
    <content type="html"><![CDATA[<p>You know those mutiple-choice tests that put you in one of four quadrants based
on your answers to a bunch of seemingly irrelevant questions? We’ve all taken
them, and if you’re like me they’re kind of like reading your horoscope – it
all seems so right and true when you’re reading it, but you wonder if it would
still seem just as right and true if the horoscopes got jumbled at random?</p>

<p>Well, I took one of these tests a while back that actually told me something about myself – it was the “Learning-Style Inventory”
test, and what it said about me is that I’m waaaayyy over at the end of the
scale when it comes to visual thinking. That gave me an insight into the way my
brain works that I’ve found really helpful ever since. So, this next bit was right up my alley,
but I’m guessing you’ll like it too.</p>

<p><a href="http://www.overbyte.com.au/misc/Lesson3/CacheFun.html"><img class="right" src="http://overbyte.com.au/wp-content/uploads/2012/01/InteractiveMemAccess-620x424.png" width="320" height="240" /></a></p>

<p>We read a lot lately about NUMA architecture and how it presents a fundamental
change in the way we approach writing efficient code: it’s no longer about the
CPU, it’s all about RAM. We all nod and say “Sure, I get that!”  Well, I thought
I got it too, but until I saw <a href="http://www.overbyte.com.au/misc/Lesson3/CacheFun.html">this web page</a>, 
I really didn’t. </p>

<p>See the full discussion at <a href="http://overbyte.com.au/index.php/overbyte-blog/entry/optimisation-lesson-3-the-memory-bottleneck">http://overbyte.com.au/index.php/overbyte-blog/entry/optimisation-lesson-3-the-memory-bottleneck</a>.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using clang's Address Sanitizer (without clang)]]></title>
    <link href="http://btorpey.github.io/blog/2014/03/27/using-clangs-address-sanitizer/"/>
    <updated>2014-03-27T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2014/03/27/using-clangs-address-sanitizer</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/DragonSmall.png" /> </p>

<p>Valgrind has been an indispensable tool for C/C++ programmers for a long
time, and I’ve used it quite happily – it’s a tremendous tool for doing dynamic
analysis of program behavior at run time. valgrind<sup id="fnref:3"><a href="#fn:3" rel="footnote">1</a></sup> can detect reads of
uninitialized memory, heap buffer overruns, memory leaks, and other errors that
can be difficult or impossible to find by eyeballing the code, or by static
analysis tools.  But that comes with a price, which in some cases can be quite steep, and some new
tools promise to provide some or all of the functionality valgrind provides without the drawbacks.</p>

<!--more-->

<p>For one thing, valgrind can
be <em>extremely</em> slow.  That is an unavoidable side-effect of one of valgrind’s
strengths, which is that it doesn’t require that the program under test be
instrumented beforehand – it can analyze any executable (including shared
objects) “right out of the box”.  That works because valgrind effectively
emulates the hardware the program runs on, but that leads to a potential
problem: valgrind instruments <em>all</em> the code, including shared objects –and
that includes third-party code (e.g., libraries, etc.) that you may not have any
control over.</p>

<p>In my case, that ended up being a real problem.  The main reason
being that a significant portion of the application I work with is hosted in a
JVM (because it runs in-proc to a Java-based FIX engine, using a thin JNI
layer).  The valgrind folks say that the slowdown using their tool can be up to
20x, but it seemed like more, because the entire JVM was being emulated.</p>

<p>And, because valgrind emulates <em>everything</em>, it also detects and reports
problems in the JVM itself.  Well, it turns out that the JVM plays a lot of
tricks that valgrind doesn’t like, and the result is a flood of complaints that
overwhelm any potential issues in the application itself.</p>

<p>So, I was very interested in learning about a similar technology that promised
to address some of these problems.  Address Sanitizer (Asan from here on) was
originally developed as part of the clang project, and largely by folks at Google.
They took a different approach: while valgrind emulates the machine at run-time, Asan works by instrumenting
the code at compile-time.</p>

<p>That helps to solve the two big problems that I was having with valgrind: its
slowness, and the difficulty of excluding third-party libraries from the
analysis.</p>

<h2 id="asan-with-clang">Asan with clang</h2>

<p>Since I was already building the application using clang for its excellent
diagnostics and static analysis features, I thought it would be relatively
straightforward to introduce the Asan feature into the build.  Turns out there
is a bump in that road: clang’s version of Asan is supplied only as a
static library that is linked into the main executable.  And while it should be
possible to re-jigger things to make it work as a shared library, that would
turn into a bit of science project.  That, and the fact that the wiki page discussing it
(https://github.com/google/sanitizers/wiki/AddressSanitizerAsDso) didn’t sound
particularly encouraging (“however the devil is in the detail” – uhh, thanks, no).</p>

<p>Rats!  However, the wiki page
did mention that there was a version of Asan that worked with gcc, and that
version apparently did support deployment as a shared object.  So, I decided to give that a try…</p>

<h2 id="asan-with-gcc">Asan with gcc</h2>

<p>It turns out that the gcc developers haven’t been sitting still – in
fact, it looks like there is a bit of a healthy rivalry between the clang and gcc
folks, and that’s a good thing for you and me.  Starting with version 4.8 of the
gcc collection, Asan is available with gcc as well.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></p>

<p>Getting the latest gcc version (4.8.2 as of this writing), building and
installing it was relatively straight-forward.  By default, the source build
installs into /usr/local, so it can co-exist nicely with the native gcc for the
platform (in the case of Red Hat/CentOS 6.5, that is the relatively ancient gcc
4.4 branch).</p>

<h2 id="building-with-asan">Building with Asan</h2>
<p>Including support for Asan in your build is pretty simple – just include the <code>-fsanitize=address</code>
flag in both the compile and link step.  (Note that this means you need to invoke the linker via the compiler
driver, rather than directly.  In practice, this means that the executable you specify for the link step should be 
g++ (or gcc), not ld).  </p>

<p>While not strictly required, it’s also a very good idea to include the <code>-fno-omit-frame-pointer</code> flag
in the compile step.  This will prevent the compiler from optimizing away the frame pointer (ebp) register.  While
disabling any optimization might seem like a bad idea, in this case the performance benefit is likely minimal at best<sup id="fnref:5"><a href="#fn:5" rel="footnote">3</a></sup>, but the 
inability to get accurate stack frames is a show-stopper.</p>

<h2 id="running-with-asan">Running with Asan</h2>
<p>If you’re checking an executable that you build yourself, the prior steps are all you need – libasan.so will get linked
into your executable by virtue of the <code>-fsanitize=address</code> flag.</p>

<p>In my case, though, the goal was to be able to instrument code running in the JVM.  In this case, I had to force libasan.so
into the executable at runtime using <code>LD_PRELOAD</code>, like so:</p>

<p><code>LD_PRELOAD=/usr/local/lib64/libasan.so.0 java ...</code></p>

<p>And that’s it!</p>

<h2 id="tailoring-asan">Tailoring Asan</h2>

<p>There are a bunch of options available to tailor the way Asan works: at compile-time you can supply a “blacklist” of functions that
Asan should NOT instrument, and at run-time you can further customize Asan using the <code>ASAN_OPTIONS</code> environment variable, which
is discussed <a href="https://github.com/google/sanitizers/wiki/AddressSanitizerFlags">here</a>.</p>

<p>By default, Asan is silent, so you may not be certain that it’s actually working unless it aborts with an error, which would look like
<a href="http://en.wikipedia.org/wiki/AddressSanitizer#Examples&quot;">one of these</a>.</p>

<p>You can check that Asan is linked in to your executable using ldd:</p>

<pre>
$ ldd a.out
	linux-vdso.so.1 =&gt;  (0x00007fff749ff000)
	libasan.so.0 =&gt; /usr/local/lib64/libasan.so.0 (0x00007f57065f7000)
	libstdc++.so.6 =&gt; /usr/local/lib64/libstdc++.so.6 (0x00007f57062ed000)
	libm.so.6 =&gt; /lib64/libm.so.6 (0x0000003dacc00000)
	libgcc_s.so.1 =&gt; /usr/local/lib64/libgcc_s.so.1 (0x00007f57060bd000)
	libc.so.6 =&gt; /lib64/libc.so.6 (0x0000003dad000000)
	libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x0000003dad800000)
	libdl.so.2 =&gt; /lib64/libdl.so.2 (0x0000003dad400000)
	/lib64/ld-linux-x86-64.so.2 (0x0000003dac800000)
</pre>

<p>You can also up the default verbosity level of Asan to get an idea of what is going on at run-time:</p>

<p><code>export ASAN_OPTIONS="verbosity=1:..."</code></p>

<p>If you’re using <code>LD_PRELOAD</code> to inject Asan into an executable that was not built
using Asan, you may see output that looks like the following:</p>

<pre>
==25140== AddressSanitizer: failed to intercept 'memset'
==25140== AddressSanitizer: failed to intercept 'strcat'
==25140== AddressSanitizer: failed to intercept 'strchr'
==25140== AddressSanitizer: failed to intercept 'strcmp'
==25140== AddressSanitizer: failed to intercept 'strcpy'
==25140== AddressSanitizer: failed to intercept 'strlen'
==25140== AddressSanitizer: failed to intercept 'strncmp'
==25140== AddressSanitizer: failed to intercept 'strncpy'
==25140== AddressSanitizer: failed to intercept 'pthread_create'
==25140== AddressSanitizer: libc interceptors initialized
</pre>

<p>Don’t worry – it turns out that is a bogus warning related to running Asan as a shared object.  Unfortunately, the Asan
developers don’t seem to want to fix this (http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58680).    </p>

<h2 id="conclusion">Conclusion</h2>

<p>So, how did this all turn out?  Well, it’s pretty early in the process, but Asan
has already caught a memory corruption problem that would have been extremely
difficult to track down otherwise.  (Short version is that due to some
unintended name collissions between shared libraries, we were trying to put 10
pounds of bologna in a 5 pound sack.  Or, as one of my colleagues more accurately pointed out, 8 pounds
of bologna in a 4 pound sack ;-)</p>

<p>valgrind is still an extremely valuable tool, especially because of its
convenience and versatility; but in certain edge cases Asan can bring things to
the table, like speed and selectivity, that make it the better choice.</p>

<h2 id="postscript">Postscript</h2>

<p>Before closing there are a few more things I want to mention about Asan in
comparison to valgrind:</p>

<ul>
  <li>
    <p>If you look at the processes using Asan with top, etc. you may be a bit
shocked at first to see they are using 4TB (or more) of memory.  Relax –
it’s not real memory, it’s virtual memory (i.e., address space).  The
algorithm used by Asan to track memory “shadows” actual memory (one bit for
every byte), so it needs that whole address space.  Actual memory use is
greater with Asan as well, but not nearly as bad as it appears at first
glance.  Even so, Asan disables core files by default, at least in 64-bit
mode.</p>
  </li>
  <li>
    <p>As hoped, Asan is way faster than valgrind, especially in my “worst-case”
scenario with the JVM, since the only code that’s paying the price of
tracking memory accesses is the code that is deliberately instrumented.
That also eliminates false positives from the JVM, which is a very good
thing.</p>
  </li>
  <li>
    <p>As for false positives, the Asan folks apparently don’t believe in them,
because there is no “suppression” mechanism like there is in valgrind.
Instead, the Asan folks ask that if you find what you think is a false
positive, you file a bug report with them.  In fact, when Asan finds a
memory error it immediately aborts – the rationale being that allowing Asan
to continue after a memory error would be much more work, and would make
Asan much slower.  Let’s hope they’re right about the absence of false
positives, but even so this “feature” is bound to make the debug cycle
longer, so there are probably cases where valgrind is a better choice – at
least for initial debugging.</p>
  </li>
  <li>
    <p>Asan and valgrind have slightly different capabilities, too:</p>

    <ul>
      <li>
        <p>Asan can find stack corruption errors, while valgrind only tracks heap
allocations.</p>
      </li>
      <li>
        <p>Both valgrind and Asan can detect memory leaks (although Asan’s leak
checking support is “still experimental” - see
<a href="https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer">https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer</a>).</p>
      </li>
      <li>
        <p>valgrind also detects reads of un-initialized memory, which Asan does
not.</p>

        <ul>
          <li>The related <a href="https://github.com/google/sanitizers/wiki/MemorySanitizer">Memory Sanitizer</a>
tool apparently can do that.  It has an additional restriction that
the main program must be built with -fpie to enable
position-independent code, which may make it difficult to use in
certain cases, e.g. for debugging code hosted in a JVM.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>A detailed comparison of Asan, valgrind and other tools can be found <a href="https://github.com/google/sanitizers/wiki/AddressSanitizerComparisonOfMemoryTools">here</a>.</p>

<h2 id="resources">Resources</h2>

<p><a href="http://en.wikipedia.org/wiki/AddressSanitizer">http://en.wikipedia.org/wiki/AddressSanitizer</a></p>

<p>https://github.com/google/sanitizers/wiki/AddressSanitizer</p>

<p>http://clang.llvm.org/docs/AddressSanitizer.html</p>

<div class="footnotes">
  <ol>
    <li id="fn:3">
      <p>In this paper, I use the term valgrind, but I really mean valgrind with the memcheck tool.  valgrind includes a bunch of other tools as well – see <a href="http://valgrind.org">http://valgrind.org</a> for details.<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>As is another tool, the Thread Sanitizer, which detects data races between threads at run-time.  More on that in an upcoming post.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>Omitting the frame pointer makes another register (ebp) available to the compiler, but since there are already at least a dozen other registers for the compiler to use, this extra register is unlikely to be critical.  The compiler can also omit the code that saves and restores the register, but that’s a couple of instructions moving data between registers and L1 cache. <a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Measuring Latency in Linux]]></title>
    <link href="http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux/"/>
    <updated>2014-02-18T00:00:00-05:00</updated>
    <id>http://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux</id>
    <content type="html"><![CDATA[<p><img class="left" src="http://btorpey.github.io/images/dali.png" /></p>

<p>For measuring latency in modern systems, we need to be able to measure intervals
in microseconds at least, and preferably in nanoseconds or better. The good news
is that with relatively modern hardware and software, it is possible to
accurately measure time intervals as small as (some smallish number of)
nanoseconds. </p>

<p>But, it’s important to understand what you’re measuring and what
the different edge cases might be to ensure that your results are accurate.          </p>

<!--more-->

<h2 id="tldr">TL;DR</h2>

<p>The short version is that for best results you should be using:</p>

<ul>
  <li>
    <p>Linux kernel 2.6.18 or above – this is the first version that includes the
hrtimers package. Even better is 2.6.32 or above, since this includes
support for most of the different clock sources.</p>
  </li>
  <li>
    <p>A CPU with a constant, invariant TSC (time-stamp counter). This means that
the TSC runs at a constant rate across all sockets/cores, regardless of
frequency changes made to the CPU by power management code. If the CPU
supports the RDTSCP instruction, so much the better.</p>
  </li>
  <li>
    <p>The TSC should be configured as the clock source for the Linux kernel at
boot time.</p>
  </li>
  <li>
    <p>You should be measuring the interval between two events that happen on the
same machine (intra-machine timing).</p>
  </li>
  <li>
    <p>For intra-machine timing, your best bet is generally going to be to read the
TSC directly using assembler. On my test machine it takes about 100ns 
to read the TSC from software, so that is the limit of this method’s accuracy. YMMV, of course, which is why I’ve included <a href="https://github.com/btorpey/clocks.git">source code</a> that you can use to do your own measurements.</p>
    <ul>
      <li>Note that the 100ns mentioned above is largely due to the fact that my Linux box
doesn’t support the RDTSCP instruction, so to get reasonably accurate timings it’s also necessary
to issue a CPUID instruction prior to RDTSC to serialize its execution. On another machine that supports
the RDTSCP instruction (a recent MacBook Air), overhead is down around 14ns.  More on that 
<a href="#rdtscp">later…</a></li>
    </ul>
  </li>
</ul>

<p>The following sections will talk about how clocks work on Linux, how to access
the various clocks from software, and how to measure the overhead of accessing
them.</p>

<h3 id="intra-machine-vs-inter-machine-timings">Intra-machine vs. Inter-machine Timings</h3>

<p>However, before jumping into the details of the above recommendations, I want to
talk a little about the different problems in intra-machine vs. inter-machine
time measurements. Intra-machine timing is the simplest scenario, since it is
generally pretty easy to ensure that you use the same clock source for all your
timing measurements.</p>

<p>The problem with inter-machine timing is that, by definition, you’re dealing
with (at least) two different clock sources. (Unless of course you are timing
round-trip intervals – if that’s the case, you’re lucky). And the problem with
having two clock sources is described somewhat amusingly by this old chestnut:</p>

<blockquote>A man with a watch knows what time it is. A man with two watches is never sure.<footer><cite>Segal&#8217;s Law</cite></footer></blockquote>

<p>For inter-machine timings, you’re pretty much stuck with the CLOCK_REALTIME
clock source (the source for gettimeofday), since you presumably need a clock
that is synchronized across the two (or more) machines you are testing. In this
case, the accuracy of your timing measurements will obviously depend on how well
the clock synchronization works, and in all but the best cases you’ll be
lucky to get accuracy better than some small number of microseconds.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>We’re not going to talk much more about inter-machine timing in this article,
but may get into it another time.</p>

<h2 id="how-linux-keeps-time">How Linux Keeps Time</h2>

<p>With that out of the way, let’s take a look at how Linux keeps time. It starts
when the system boots up, when Linux gets the current time from the RTC (Real
Time Clock). This is a hardware clock that is powered by a battery so it
continues to run even when the machine is powered off. In most cases it is not
particularly accurate, since it is driven from a cheap crystal oscillator whose
frequency can vary depending on temperature and other factors.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> The boot
time retrieved from the RTC is stored in memory in the kernel, and is used as an
offset later by code that derives wall-clock time from the combination of boot
time and the tick count kept by the TSC.</p>

<p>The other thing that happens when the system boots is that the TSC (Time Stamp
Counter) starts running. The TSC is a register counter that is also driven from
a crystal oscillator – the same oscillator that is used to generate the clock
pulses that drive the CPU(s). As such it runs at the frequency of the CPU, so
for instance a 2GHz clock will tick twice per nanosecond.</p>

<p>There are a number of other clock sources which we’ll discuss later, but in most
cases the TSC is the preferred clock source for two reasons: it is very
accurate, and it is very cheap to query its value (since it is simply a
register). But, there are a number of caveats to keep in mind when using the TSC
as a timing source.</p>

<ul>
  <li>
    <p>In older CPU’s, each core had its own TSC, so in order to be sure that two
measurements were accurate relative to each other, it was necessary to pin
the measuring code to a single core.</p>
  </li>
  <li>
    <p>Also in older CPU’s, the TSC would run at the frequency of the CPU itself,
and if that changed (for instance, if the frequency was dynamically reduced,
or the CPU stopped completely for power management), the TSC on that CPU
would also slow down or stop. (It is sometimes possible to work around this
problem by disabling power management in the BIOS, so all CPU’s always run
at 100%  no more, no less).</p>
  </li>
</ul>

<p>Both of these problems are solved in more recent CPUs: a <em>constant</em> TSC keeps
all TSC’s synchronized across all cores in a system, and an <em>invariant</em> (or
<em>nonstop</em>) TSC keeps the TSC running at a fixed rate regardless of changes in
CPU frequency. To check whether your CPU supports one or both, execute the
following and examine the values output in flags:</p>

<pre>
$ cat /proc/cpuinfo | grep -i tsc
flags : ... tsc  rdtscp constant_tsc nonstop_tsc ...
</pre>

<p>The flags have the following meanings:</p>

<table>
  <thead>
    <tr>
      <th>Flag</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>tsc</td>
      <td>The system has a TSC clock.</td>
    </tr>
    <tr>
      <td>rdtscp</td>
      <td>The RDTSCP instruction is available.</td>
    </tr>
    <tr>
      <td>constant_tsc</td>
      <td>The TSC is synchronized across all sockets/cores.</td>
    </tr>
    <tr>
      <td>nonstop_tsc</td>
      <td>The TSC is not affected by power management code.</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h3 id="other-clock-sources">Other Clock Sources</h3>

<p>While the TSC is generally the preferred clock source, given its accuracy and
relatively low overhead, there are other clock sources that can be used:</p>

<ul>
  <li>
    <p>The HPET (High Precision Event Timer) was introduced by Microsoft and Intel
around 2005. Its precision is approximately 100 ns, so it is less accurate
than the TSC, which can provide sub-nanosecond accuracy. It is also much
more expensive to query the HPET than the TSC.</p>
  </li>
  <li>
    <p>The acpi_pm clock source has the advantage that its frequency doesn’t change
based on power-management code, but since it runs at 3.58MHz (one tick every
279 ns), it is not nearly as accurate as the preceding timers.</p>
  </li>
  <li>
    <p>jiffies signifies that the clock source is actually the same timer used for
scheduling, and as such its resolution is typically quite poor. (The default
scheduling interval in most Linux variants is either 1 ms or 10 ms).</p>
  </li>
</ul>

<p>To see the clock sources that are available on the system:</p>

<pre>
$ cat /sys/devices/system/clocksource/clocksource0/available_clocksource
tsc hpet acpi_pm
</pre>

<p>And to see which one is being used:</p>

<pre>
$ cat /sys/devices/system/clocksource/clocksource0/current_clocksource
tsc
</pre>

<p>Typically the clock source is set by the kernel automatically at boot time, but
you can force a particular clock source by including the appropriate
parameter(s) on the command line that boots Linux (e.g., in
/boot/grub/grub.conf):</p>

<p><code>ro root=/dev/... clocksource=tsc</code></p>

<p>You can also change the clock source while the system is running  e.g., to
force use of HPET:</p>

<pre>
$ echo hpet &gt; /sys/devices/system/clocksource/clocksource0/current_clocksource
</pre>

<p>The above discusssion refers to what I will call hardware clocks, although
strictly speaking these clocks are a mixture of hardware and software. At the
bottom of it all there’s some kind of hardware device that generates periodic
timing pulses, which are then counted to create the clock. In some cases (e.g.,
the TSC) the counting is done in hardware, while in others (e.g., jiffies) the
counting is done in software.</p>

<h2 id="wall-clock-time">Wall-Clock Time</h2>

<p>The hardware (or hardware/software hybrid) clocks just discussed all have one
thing in common: they are simply counters, and as such have no direct
relationship to what most of us think of as time, commonly referred to as
wall-clock time.</p>

<p>To derive wall-clock time from these counters requires some fairly intricate
software, at least if the wall-clock time is to be reasonably accurate. What
reasonably accurate means of course depends on how important it is (i.e., how
much money is available) to make sure that wall-clock time is accurate. </p>

<p>The whole process of synchronizing multiple distributed clocks is hellishly complicated, and we’re not going to go into it here. There are many different mechanisms for synchronizing distributed clocks, from the relatively simple (e.g., NTP<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>) to the not-quite-so-simple (e.g., PTP<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>), up to specialized proprietary solutions<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>.</p>

<p>The main point is that synchronizing a system’s wall-clock time with other
systems requires a way to adjust the clock to keep it in sync with its peers.
There are two ways this can be done:</p>

<ul>
  <li>
    <p>Stepping is the process of making (one or more) discontinuous changes to
the wall-clock component of the system time. This can cause big jumps in the
wall-clock time, including backwards jumps, although the time adjustment
software can often be configured to limit the size of a single change. A
common example is a system that is configured to initialize its clock at
boot time from an NTP server.</p>
  </li>
  <li>
    <p>Slewing (sometimes called disciplining) involves actually changing the frequency (or frequency
multiplier) of the oscillator used to drive a hardware counter like the TSC.
This can cause the clock to run relatively faster or slower, but it cannot
jump, and so cannot go backwards.</p>
  </li>
</ul>

<h2 id="available-clock-sources">Available Clock Sources</h2>

<p>The most common way to get time information in Linux is by calling the
gettimeofday() system call, which returns the current wall-clock time with
microsecond precision (although not necessarily microsecond accuracy). Since
gettimeofday() calls clock_gettime(CLOCK_REALTIME, ), the following discussion
applies to it as well.</p>

<p>Linux also implements the POSIX clock_gettime() family of functions, which let
you query different clock sources, including:</p>

<table id="mytab">
<tbody>
<tr>
  <td>CLOCK_REALTIME </td>
  <td>Represents wall-clock time. Can be both stepped and slewed by time adjustment code (e.g., NTP, PTP).</td>
</tr>
<tr>
  <td>CLOCK_REALTIME_COARSE </td>
  <td>A lower-resolution version of CLOCK_REALTIME.</td>
</tr>
<tr>
  <td>CLOCK_REALTIME_HR  </td>
  <td>A higher-resolution version of CLOCK_REALTIME. 
                        Only available with the real-time kernel.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC </td>
  <td>Represents the interval from an abitrary time. 
                        Can be slewed but not stepped by time adjustment code. 
                        As such, it can only move forward, not backward.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC_COARSE </td>
  <td>A lower-resolution version of CLOCK_MONOTONIC.</td>
</tr>
<tr>
  <td>CLOCK_MONOTONIC_RAW </td>
  <td>A version of CLOCK_MONOTONIC that can neither be slewed nor stepped by time adjustment code.</td>
</tr>
<tr>
  <td>CLOCK_BOOTTIME</td>
  <td>A version of CLOCK_MONOTONIC that additionally reflects time spent in suspend mode.  Only available in newer (2.6.39+) kernels.</td>
</tr>
</tbody>
</table>
<p><br /></p>

<p>The availability of the various clocks, as well as their resolution and
accuracy, depends on the hardware as well as the specific Linux implementation.
As part of the <a href="https://github.com/btorpey/clocks.git">accompanying source code</a> for this article I’ve
included a small test program (clocks.c) that when compiled<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup> and run will
print the relevant information about the clocks on a system. On my test
machine<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup> it shows the following:</p>

<pre>
clocks.c
                    clock	       res (ns)	           secs	          nsecs
             gettimeofday	          1,000	  1,391,886,268	    904,379,000
           CLOCK_REALTIME	              1	  1,391,886,268	    904,393,224
    CLOCK_REALTIME_COARSE	        999,848	  1,391,886,268	    903,142,905
          CLOCK_MONOTONIC	              1	        136,612	    254,536,227
      CLOCK_MONOTONIC_RAW	    870,001,632	        136,612	    381,306,122
   CLOCK_MONOTONIC_COARSE	        999,848	        136,612	    253,271,977
</pre>

<p>Note that it’s important to pay attention to what clock_getres() returns – a particular clock source can (and does, as can be seen above with the COARSE clocks) sometimes return what may look like higher-precision values, but any digits beyond its actual precision are likely to be garbage.  (The exception is gettimeofday – since it returns a timeval, which is denominated in micros, the lower-order digits are all zeros).</p>

<p>Also, the value returned from clock_getres() for CLOCK_MONOTONIC_RAW is clearly garbage, although I’ve seen similar results on several machines.</p>

<p>Finally, note that the resolution listed for CLOCK_REALTIME is close to, but not
quite, 1 million – this is an artifact of the fact that the oscillator cannot
generate a frequency of exactly 1000 Hz – it’s actually 1000.15 Hz.</p>

<h2 id="getting-clock-values-in-software">Getting Clock Values in Software</h2>

<p>Next up is a brief discussion of how to read these different clock values from
software.</p>

<p><a name="rdtscp"></a></p>

<h3 id="assembler">Assembler</h3>

<p>In assembler language, the RDTSC instruction returns the value of the TSC
directly in registers edx:eax. However, since modern CPU’s support out-of-order
execution, it has been common practice to insert a serializing instruction (such
as CPUID) prior to the RDTSC instruction in order to ensure that the execution
of RDTSC is not reordered by the processor.</p>

<p>More recent CPU’s include the RDTSCP instruction, which does any necessary
serialization itself. This avoids the overhead of the CPUID instruction, which
can be considerable (and variable). If your CPU supports RDTSCP, use that instead of the
CPUID/RDTSC combination.</p>

<h3 id="cc">C/C++</h3>

<p>Obviously, the RDTSC instruction can be called directly from C or C++, using
whatever mechanism your compiler provides for accessing assembler language, or
by calling an assembler stub that is linked with the C/C++ program. (An example
can be found at <a href="http://agner.org/optimize/#asmlib">Agner Fog’s excellent website</a>).</p>

<p>Calling gettimeofday() or clock_gettime() is pretty straightforward – see the
accompanying <a href="https://github.com/btorpey/clocks/blob/master/clocks.c">clocks.c source file</a> for examples.</p>

<h3 id="java">Java</h3>

<p>Java has only two methods that are relevant to this discussion:</p>

<ul>
  <li>
    <p>System.currentTimeMillis() returns the current wall-clock time as the number
of milliseconds since the epoch. It calls gettimeofday(), which in turn
calls clock_gettime(CLOCK_REALTIME, …).</p>
  </li>
  <li>
    <p>System.nanoTime returns the number of nanoseconds since some unspecified
starting point. Depending on the capabilities of the system, it either calls
gettimeofday(), or clock_gettime(CLOCK_MONOTONIC, ).</p>
  </li>
</ul>

<p>The bad news is that if you need clock values other than the above in Java,
you’re going to need to roll your own, e.g. by calling into C via JNI. The good
news is that doing so is not much more expensive than calling nanoTime (at least in my tests).</p>

<h3 id="overhead-of-clock-queries">Overhead of Clock Queries</h3>

<p>The Heisenberg Uncertainty Principle says, in a nutshell, that the act of
observing a phenomenom changes it. A similar issue exists with getting
timestamps for latency measurement, since it takes a finite (and sometimes
variable) amount of time to read any clock source.  In other words, just because the TSC on a 2GHz machine ticks twice per nanosecond doesn’t mean we can measure intervals of a nanosecond – we also need to account for the time it takes to read the TSC from software.</p>

<p>So, how expensive is it to perform these different clock queries? Included is some <a href="https://github.com/btorpey/clocks.git">sample code</a> that you can
use to measure the time it takes to query various clock sources, from both C++
and Java (using JNI to call C code).</p>

<p>Both the C++ and Java versions take the same approach: call the particular clock
function in a tight loop, and store the result. We do this a large number of
times, and hang on to the results from the final iteration. This has the effect
of allowing Java to do any jitting it needs to, and for both the C++ and Java
versions to help ensure that code and data is in the processor’s cache memory.</p>

<p>The results of running the test on my test machine are (all timings are in nanoseconds):</p>

<pre>
ClockBench.cpp
                   Method       samples     min     max     avg  median   stdev
           CLOCK_REALTIME       255       54.00   58.00   55.65   56.00    1.55
    CLOCK_REALTIME_COARSE       255        0.00    0.00    0.00    0.00    0.00
          CLOCK_MONOTONIC       255       54.00   58.00   56.20   56.00    1.46
      CLOCK_MONOTONIC_RAW       255      650.00 1029.00  690.35  839.50   47.34
   CLOCK_MONOTONIC_COARSE       255        0.00    0.00    0.00    0.00    0.00
              cpuid+rdtsc       255       93.00   94.00   93.23   93.50    0.42
                    rdtsc       255       24.00   28.00   25.19   26.00    1.50
Using CPU frequency = 2.660000

ClockBench.java
                   Method       samples     min     max     avg  median   stdev
          System.nanoTime       255       54.00   60.00   55.31   57.00    1.55
           CLOCK_REALTIME       255       60.00   84.00   62.50   72.00    1.92
              cpuid+rdtsc       255      108.00  112.00  109.03  110.00    1.39
                    rdtsc       255       39.00   43.00   39.85   41.00    1.37
Using CPU frequency = 2.660000
</pre>

<p>A few things to note about these results:</p>

<ul>
  <li>
    <p>Both of the COARSE clocks show a latency of zero for getting the clock
value. This tells us that the time it takes to
get the clock value is less than the resolution of the clock. (Our previous
test showed a resolution of 1ms for the COARSE clocks).</p>
  </li>
  <li>
    <p>For some reason, the CLOCK_MONOTONIC_RAW clock is very expensive to query. I
can’t explain this –  you would think that its lack of adjustment would make
it faster, not slower. This is unfortunate, as otherwise it would be an
excellent choice for intra-machine timing.</p>
  </li>
  <li>
    <p>As you might expect, the combination of CPUID and RDTSC is slower than
RDTSCP, which is slower than RDTSC alone. In general, this would
suggest that RDTSCP should be preferred if available, with a fallback to
CPUID+RDTSC if not. (While RDTSC alone is the fastest, the fact that it can
be inaccurate as a result of out-of-order execution means it is only useful
for timing relatively long operations where that inaccuracy is not
significant – but those are precisely the scenarios where its speed is less
important).</p>
  </li>
  <li>
    <p>Also as expected, the Java versions are slightly slower than the C++
versions, presumably due to the overhead of going through JNI.</p>
  </li>
</ul>

<h3 id="update">Update</h3>
<p>Well, I admit that Java is not my strong suit, but I nevertheless understand the implications
of “warm-up” and JIT-ing when benchmarking Java code.  My understanding (and the docs seem to agree)
is that Java methods get JIT-ed after approx. 10,000 invocations.  I also thought (and still do) that 
100 iterations of 200 invocations would be more than 10,000.  Whatever – I adjusted the number of iterations
for the Java benchmark, and that made a big difference – especially in the timings for System.nanotime, which now
agree much more closely with other published benchmarks, specifically the results published in 
<a href="http://shipilev.net/blog/2014/nanotrusting-nanotime/">“Nanotrusting the Nanotime”</a>.  Thanks, Aleksey!</p>

<h3 id="update-2">Update #2</h3>
<p>Slyain Archenault pointed out that the omission of unistd.h in SysTime.c caused it to (silently) fall back
to using gettimeofday.  I’ve updated the code to include the proper header, and also to issue a warning 
if _POSIX_TIMERS remains undefined.   Thanks, Sylvain!</p>

<h2 id="conclusion">Conclusion</h2>

<p>I thought this would be a very brief and somewhat trivial research project. In
fact, it turned out to be far more complicated (and less well-documented) than I
expected. I guess I should have known: everything related to time and computers
turns out to be a major pain in the neck!</p>

<p>Anyway, I hope this proves helpful. (I know I would have been very happy to have
had this when I started looking into clock sources).  </p>

<p>As always, please feel free to <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#119;&#097;&#108;&#108;&#115;&#116;&#112;&#114;&#111;&#103;&#064;&#103;&#109;&#097;&#105;&#108;&#046;&#099;&#111;&#109;">contact me</a>
directly with comments, suggestions, corrections, etc.</p>

<h2 id="additional-resources">Additional Resources</h2>

<p>Following are the main anchor points that I kept coming back to you as I
researched this article.</p>

<p><a href="http://elinux.org/Kernel_Timer_Systems">http://elinux.org/Kernel_Timer_Systems</a></p>

<p><a href="http://elinux.org/High_Resolution_Timers">http://elinux.org/High_Resolution_Timers</a></p>

<p><a href="http://juliusdavies.ca/posix_clocks/clock_realtime_linux_faq.html">http://juliusdavies.ca/posix_clocks/clock_realtime_linux_faq.html</a></p>

<p><a href="http://en.wikipedia.org/wiki/Time_Stamp_Counter">http://en.wikipedia.org/wiki/Time_Stamp_Counter</a></p>

<p><a href="http://stackoverflow.com/questions/10921210/cpu-tsc-fetch-operation-especially-in-multicore-multi-processor-environment">http://stackoverflow.com/questions/10921210/cpu-tsc-fetch-operation-especially-in-multicore-multi-processor-environment</a></p>

<p><a href="http://www.citihub.com/requesting-timestamp-in-applications/">http://www.citihub.com/requesting-timestamp-in-applications/</a></p>

<p><a href="http://www.intel.com/content/www/us/en/intelligent-systems/embedded-systems-training/ia-32-ia-64-benchmark-code-execution-paper.html">http://www.intel.com/content/www/us/en/intelligent-systems/embedded-systems-training/ia-32-ia-64-benchmark-code-execution-paper.html</a></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>The best case being hardware on each machine with a CSAC (chip-scale atomic clock) or OCXO (oven-controlled crystal oscillator). These can be a bit pricey, however.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>The accuracy of a typical RTC in a PC-type computer is rated at +/- 20ppm, so it can gain or lose 20 us each second. This turns out to be approximately one minute per month, which may be OK for a cheap digital watch, but for a computer is not too good. For more information, see <a href="http://www.maximintegrated.com/app-notes/index.mvp/id/58">http://www.maximintegrated.com/app-notes/index.mvp/id/58</a>.<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Network Time Protocol, RFC 1305 (<a href="https://tools.ietf.org/html/rfc1305">https://tools.ietf.org/html/rfc1305</a>)<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Precision Time Protocol, IEEE 1588 (<a href="http://www.nist.gov/el/isd/ieee/ieee1588.cfm">http://www.nist.gov/el/isd/ieee/ieee1588.cfm</a>)<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>From companies like Symmetricon, Corvil, TS Associates and others.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Note that the program must be compiled, as well as run, on the target system – it uses the presence or absence of pre-processor symbols to determine whether a particular clock source is available.<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>CentOS 6.5 running on a Dell 490 with dual Xeon 5150’s at 2.6 GHz.<a href="#fnref:7" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[You may ask yourself - "How did I get here?"]]></title>
    <link href="http://btorpey.github.io/blog/2014/02/13/how-did-i-get-here/"/>
    <updated>2014-02-13T00:00:00-05:00</updated>
    <id>http://btorpey.github.io/blog/2014/02/13/how-did-i-get-here</id>
    <content type="html"><![CDATA[<p><img class="right" src="http://btorpey.github.io/images/talkingHeads1.jpg" width="244" height="176" /> </p>

<p>In addition to being a great line from David Byrne and Talking Heads (from “Life
During Wartime”), this is also a question I often ask myself when
looking at log files. Today’s tip is worth the price of the whole blog (i.e.,
free), but I predict that you’ll be glad you know it.</p>

<!--more-->

<p>It’s pretty common to pipe the output of a command, or string of commands, to a
file to have a record of what happened when executing the command, something
like this:</p>

<p><code>big_gnarly_command_line_with_options 2&gt;&amp;1 | tee logfile.out</code></p>

<p>That works great for capturing the <em>output</em> of the command, but what about the
big_gnarly_command_line_with_options itself?</p>

<p>Try this instead:</p>

<p><code> bash -x -c "big_gnarly_command_line_with_options" 2&gt;&amp;1 | tee logfile.out</code></p>

<p>Now, your output file will look like this:</p>

<p><code>+ big_gnarly_command_line_with_options</code><br />
<code>... output of big_gnarly_command_line_with_options ...</code></p>

<p>If your gnarly command is actually several gnarly commands, enclose the whole
gnarly list in parentheses and separate with semicolons (or &amp;&amp;), like so:</p>

<p><code> bash -x -c "(big_gnarly_command_line_with_options_1;
big_gnarly_command_line_with_options_2)" 2&gt;&amp;1 | tee logfile.out</code></p>

<p>Normal quoting rules apply:</p>

<ul>
  <li>
    <p>If you enclose the command(s) in double-quotes (“), variable substitution
will be done on the command line</p>
  </li>
  <li>
    <p>If you need to include a double-quote within double-quotes, you need to
escape it (with the backslash (\) character)</p>
  </li>
  <li>
    <p>If you enclose the command line(s) in single quotes (‘), no variable
substitution is done</p>
  </li>
  <li>
    <p>There is no way to include a single-quote within single-quotes, but there is
a trick that gives a similar effect, that you can read about here (<a href="http://stackoverflow.com/a/1250279/203044">http://stackoverflow.com/a/1250279/203044</a>).</p>
  </li>
</ul>

<p>Now you’ll never need to ask yourself “How did I get here”?</p>

]]></content>
  </entry>
  
</feed>
