<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Confessions of a Wall Street Programmer]]></title>
  <link href="http://btorpey.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://btorpey.github.io/"/>
  <updated>2023-02-09T16:27:51-05:00</updated>
  <id>http://btorpey.github.io/</id>
  <author>
    <name><![CDATA[Bill Torpey]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Danger, Will Robinson!]]></title>
    <link href="http://btorpey.github.io/blog/2023/02/09/danger/"/>
    <updated>2023-02-09T00:00:00-05:00</updated>
    <id>http://btorpey.github.io/blog/2023/02/09/danger</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/robot1.jpg" width="183" height="200">

I've recently been dipping my toes in the very deep water that is "undefined behavior" in C and C++, and the more I learn about it, the scarier it is.  

This was inspired by a rather tricky crash that I needed to track down as part of moving the code-base at my day job to more modern compilers and language standards.

<!-- more -->
<br clear="all">

Compiler writers have been getting more agressive about taking advantage of optimization opportunities presented by undefined behavior.  And, while there has been some push-back to that, it doesn't appear to have changed things much.

"Why should I care?" you may ask yourself.  "My code compiles without any warnings, so I must not have any UB".

Unfortunately, it's not that simple.  *Some* compilers warn about *some* UB, but it's hit-or-miss.  Strictly speaking, the compiler is under no obligation to warn you about your use of UB -- in fact, the compiler is under no obligation to do anything at all once your code is found to contain UB.

And even if the compiler warns about some construct, it's easy to ignore the warning, especially since any negative consequences won't be apparent until running a release build.  Why is that?  It's because the worst aspects of UB tend to get triggered in conjunction with code optimization. 

In my case, one of my former colleagues had an extreme case of "lazy programmer syndrome" combined with a dose of cleverness, which is a dangerous combination.  He needed to write code to generate C++ classes from a text definition, and one of the things the code needed to do was initialize the class members in the ctor.

Rather than generate initializers for the primitive types (non-primitive types have their ctors called by the compiler), he decided to just nuke everything with a call to `memset` -- after all, zeroes are zeroes, right?

> If it doesn't fit, force it.  If it still doesn't fit, get a bigger hammer. 

Well, not quite -- since the classes being generated were all `virtual`, the `memset` call was also nuking the vtable, causing the app to crash pretty much immediately on launch.  That might have deterred others, but not our intrepid coder, who figured out a really "clever" way to get around the problem, using a dummy ctor and placement `new`.  The code he ended up with looked more or less like this:

<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='c++'><span class='line'><span class="k">class</span> <span class="nc">BadIdea</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>   <span class="n">BadIdea</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">{}</span>
</span><span class='line'>   <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name</span><span class="p">;</span>
</span><span class='line'>   <span class="kt">int</span> <span class="n">key</span><span class="p">;</span>
</span><span class='line'>   <span class="kt">double</span> <span class="n">value</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="k">public</span><span class="o">:</span>
</span><span class='line'>   <span class="n">BadIdea</span><span class="p">();</span>
</span><span class='line'>   <span class="k">virtual</span> <span class="o">~</span><span class="n">BadIdea</span><span class="p">()</span> <span class="p">{}</span>
</span><span class='line'><span class="p">};</span>
</span><span class='line'>
</span><span class='line'><span class="n">BadIdea</span><span class="o">::</span><span class="n">BadIdea</span><span class="p">()</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>   <span class="n">memset</span><span class="p">((</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="k">this</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
</span><span class='line'>   <span class="k">new</span> <span class="p">(</span><span class="k">this</span><span class="p">)</span>  <span class="n">BadIdea</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

And guess what -- it worked!  

OK, every ctor turned into **two** calls, which caused **two** calls to all the base class ctor's (but only **ONE** call to any base class dtors on the way out -- sure hope those ctors didn't allocate memory).  But hey, the code is clever, so it must be efficient, right?  Anyway, it *worked*.

Until it didn't.  

As part of bringing this codebase up to more modern standards, I started building it with different compilers, starting with the system compiler (gcc 4.8.5) on our production OS (CentOS 7), then on to gcc 5.3.0, and clang 10.  Everything fine -- no worries.  

Then a couple of things happened -- another colleague started working with CentOS 8, whose system compiler is gcc 8.x, and I started using Ubuntu 20.04, where the system compiler is gcc 9.3.0.  All of a sudden, nothing worked, but only when built in release mode -- in debug mode, everything was fine.  No warning messages from the compiler, either[^1].

This was the first clue that UB might be the culprit, which was confirmed by running the code in the debugger and setting a breakpoint at the call to `memset`. 

> Gregory (Scotland Yard detective): “Is there any other point to which you would wish to draw my attention?”
> <br>
> Holmes: “To the curious incident of the dog in the night-time.”
<br>
> Gregory: “The dog did nothing in the night-time.”
<br>
> Holmes: “That was the curious incident.”
> 
>  \- "Silver Blaze", Arthur Conan Doyle

Of course, nothing happened.  The call to `memset` was gone -- the compiler having determined that the code was UB, and simply refused to generate any machine code for it at all.  So, there was no place for the debugger to put the breakpoint.
 
Disassembling the generated code provided additional proof that the compiler simply ignored what it rightly determined was an obviously foolish construct:
 
<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='c++'><span class='line'><span class="n">BadIdea</span><span class="o">::</span> <span class="n">BadIdea</span><span class="p">()</span>
</span><span class='line'><span class="o">=&gt;</span> <span class="mh">0x00007f61b180d010</span> <span class="o">&lt;+</span><span class="mi">0</span><span class="o">&gt;:</span>	<span class="n">push</span>   <span class="o">%</span><span class="n">rbp</span>
</span><span class='line'>   <span class="mh">0x00007f61b180d011</span> <span class="o">&lt;+</span><span class="mi">1</span><span class="o">&gt;:</span>	<span class="n">mov</span>    <span class="o">%</span><span class="n">rsp</span><span class="p">,</span><span class="o">%</span><span class="n">rbp</span>
</span><span class='line'>   <span class="mh">0x00007f61b180d014</span> <span class="o">&lt;+</span><span class="mi">4</span><span class="o">&gt;:</span>	<span class="n">push</span>   <span class="o">%</span><span class="n">r15</span>
</span><span class='line'>   <span class="mh">0x00007f61b180d016</span> <span class="o">&lt;+</span><span class="mi">6</span><span class="o">&gt;:</span>	<span class="n">push</span>   <span class="o">%</span><span class="n">r14</span>
</span><span class='line'>   <span class="mh">0x00007f61b180d018</span> <span class="o">&lt;+</span><span class="mi">8</span><span class="o">&gt;:</span>	<span class="n">push</span>   <span class="o">%</span><span class="n">r13</span>
</span><span class='line'>   <span class="mh">0x00007f61b180d01a</span> <span class="o">&lt;+</span><span class="mi">10</span><span class="o">&gt;:</span>	<span class="n">push</span>   <span class="o">%</span><span class="n">r12</span>
</span><span class='line'>   <span class="mh">0x00007f61b180d01c</span> <span class="o">&lt;+</span><span class="mi">12</span><span class="o">&gt;:</span>	<span class="n">push</span>   <span class="o">%</span><span class="n">rbx</span>
</span><span class='line'>   <span class="mh">0x00007f61b180d01d</span> <span class="o">&lt;+</span><span class="mi">13</span><span class="o">&gt;:</span>	<span class="n">mov</span>    <span class="o">%</span><span class="n">rdi</span><span class="p">,</span><span class="o">%</span><span class="n">rbx</span>
</span><span class='line'>   <span class="mh">0x00007f61b180d020</span> <span class="o">&lt;+</span><span class="mi">16</span><span class="o">&gt;:</span>	<span class="n">lea</span>    <span class="mh">0x28</span><span class="p">(</span><span class="o">%</span><span class="n">rbx</span><span class="p">),</span><span class="o">%</span><span class="n">r13</span>
</span><span class='line'>   <span class="p">...</span>
</span><span class='line'>	<span class="p">{</span>
</span><span class='line'>		<span class="n">memset</span><span class="p">(</span><span class="k">this</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">FixMessage</span><span class="p">));</span>
</span><span class='line'>		<span class="k">new</span> <span class="p">(</span><span class="k">this</span><span class="p">)</span> <span class="n">FixMessage</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</span><span class='line'>	<span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

## What is Undefined Behavior?

The definition of undefined bahvior (from the C++98 standard, ISO/IEC 14882) is:

> behavior, such as might arise upon use of an erroneous program construct orerroneous data, for which this International Standard imposes no requirements.Undefined behavior may also be expected when this International Standard omitsthe description of any explicit definition of behavior. [Note: permissible undefined behavior ranges from ignoring the situation completely with unpredictableresults, to behaving during translation or program execution in a documentedmanner characteristic of the environment (with or without the issuance of adiagnostic message), to terminating a translation or execution (with theissuance of a diagnostic message). Many erroneous program constructs do notengender undefined behavior; they are required to be diagnosed.

That's quite a mouthful, but unfortunately doesn't say much other than "if the Standard doesn't specify the behavior of a piece of code, then the behavior of that code is undefined".

Searching the standard for the word "undefined" yields 191 hits in 110 sections of the document. Some of these are not a whole lot more helpful:

>  If an argument to a function has an invalid value (such as a value outside the domain of the function, or a pointer invalid for its intended use), the behavior is undefined.

I'm not aware of any definitive, comprehensive list of UB.  

So, what are some useful definitions of UB?  Well, here's a list from the [UBSAN home page](https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html):
- Array subscript out of bounds, where the bounds can be statically determined- Bitwise shifts that are out of bounds for their data type- Dereferencing misaligned or null pointers- Signed integer overflow- Conversion to, from, or between floating-point types which would overflow the destination

Some instances of undefined behavior can be rather surprising -- for instance, access to unaligned data (through a pointer), is strictly speaking undefined.  Never mind that the x86 processor is perfectly capable of accessing misaligned data, and in fact [does so with little or no penalty](https://lemire.me/blog/2012/05/31/data-alignment-for-speed-myth-or-reality/) -- according to the Standard it is still UB, and the compiler is free to do pretty much anything it wants with the code (including silently ignoring it).

So, for instance, typical deserialization code like this won't necessarily do what you think:

<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="kt">char</span><span class="o">*</span> <span class="n">buf</span><span class="p">;</span>
</span><span class='line'><span class="p">...</span>
</span><span class='line'><span class="kt">int</span> <span class="n">value</span> <span class="o">=</span> <span class="o">*</span><span class="p">((</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">buf</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

## Why is UB bad?

Most people would consider it A Bad Thing if the compiler took the source code they wrote and just decided to generate machine code that had nothing to do with the source code.  But that is exactly what the Standard allows -- at least according to compiler writers.  And compiler writers take advantage of that to implement optimizations that can completely change the meaning of the original source code -- a good example can be found [here](https://blog.llvm.org/2011/05/what-every-c-programmer-should-know_14.html).

There is starting to be some push-back on the "UB allows anything at all" approach, for instance [this paper](https://www.yodaiken.com/2021/10/06/plos-2021-paper-how-iso-c-became-unusable-for-operating-system-development/) from one of the members of the ISO C Standard Committee.  But, for now at least, compiler writers apparently feel free to get creative in the presence of UB.

Probably the worst thing about UB, though, is the part that I discuss at the beginning of this article -- UB can harmlessly exist in code for years until "something" changes that triggers it.  That "something" could be something "big", like a new compiler, but it could also be something "small", like a minor change to the source code that just happens to trigger an optimization that the compiler wasn't using previously.

## What to do about UB

Interestingly, searching online for the `class-memaccess` compiler warning (sometimes) associated with the `memset` call in the original example returns a bunch of results where project maintainers simple disabled the warning (`-Wno-class-memaccess`).

This is probably not what you want.

Ideally, you would eliminate all instances of potential UB from your code, since whether it "works" today is no guarantee that it will behave similarly in the future.  But first, you have to find them, and that's the tricky bit.

## Detecting Undefined Behavior

Detecting UB would seem to be a real [Catch-22](https://en.wikipedia.org/wiki/Catch-22_(logic)), since the compiler isn't required to tell you about UB, but on the other hand is allowed to do whatever it wants with it.

The other problem with detecting UB is that it is often, but not always, detectable only at runtime.

The good news is that the clang folks have created a ["sanitizer" expressly for UB](https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html).  

If you've used [AddressSanitizer](https://clang.llvm.org/docs/AddressSanitizer.html) or any of clang's other sanitizers, using UBSAN is basically straightforward.  A couple of points that may be helpful:

- Typical instances of UB are only detectable at runtime, which means the code that triggers UB must be executed.  
- Because compilers can (and do) optimize away UB-triggering code, it is best to compile with optimization disabled (`-O0`).  UBSAN can't detect UB in code that doesn't exist.
- Some helper scripts for UBSAN, ASAN, etc. can be found [here](https://github.com/nyfix/memleaktest/wiki).  These are the scripts that I use to do my testing, and my employer has graciously agreed to release these as open source.   

## More About Undefined Behavior

A lot of really smart people have been writing about UB for a while now -- this article just scratches the surface of the topic:

["What Every C Programmer Should Know About Undefined Behavior"](https://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html) , Chris Lattner

["A Guide to Undefined Behavior in C and C++"](https://blog.regehr.org/archives/213) , John Regehr

["Schrödinger's Code, Undefined behavior in theory and practice"](https://queue.acm.org/detail.cfm?id=3468263) , Terence Kelly

["Undefined behavior can result in time travel"](https://devblogs.microsoft.com/oldnewthing/20140627-00/?p=633) , Raymond Chen


["Garbage In, Garbage Out: Arguing about Undefined Behavior"](https://www.youtube.com/watch?v=yG1OZ69H_-o) , Chandler Carruth

["How ISO C became unusable for operating systems development"](https://www.yodaiken.com/2021/10/06/plos-2021-paper-how-iso-c-became-unusable-for-operating-system-development/) , Victor Yodaiken

## A short digression on disassembling code

There are several ways to get an assembler listing of C++ source code, with varying degrees of usefulness.

- By far the best, in my opinion, is the listing produced by `gdb`, which is the example used above.  `gdb` does a great job of marrying up generated code with source file lines, which makes it much easier for people who are not expert in x86 machine code (me!) to see what is happening.  Just set a breakpoint at or near the code you want to see and enter
 `disas /m`  

- Next best is `objdump`, which does an OK job, but is not nearly as nice as `gdb`. Use it like so:
    `objdump -drwCl -Mintel <file>`

- The least useful format is the intermediate assembler file produced as part of the compilation process.  With gcc, you can generate an assembler listing with the `-S -fverbose-asm` compiler flags.  (If you're using `cmake`, specify `-save-temps` instead of `-S`).  This will create `.s` files with assembler listings alongside the `.o` files in your build directory.

<hr>

[^1]: It turns out that the cast to `void*` disables the warning message that the compiler would otherwise give on this code.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Moving from CentOS to Ubuntu]]></title>
    <link href="http://btorpey.github.io/blog/2021/02/13/moving-from-centos-to-ubuntu/"/>
    <updated>2021-02-13T00:00:00-05:00</updated>
    <id>http://btorpey.github.io/blog/2021/02/13/moving-from-centos-to-ubuntu</id>
    <content type="html"><![CDATA[<p><img class="left" src="/images/twins.jpg" width="234" height="288">

I recently needed to build a Linux development system from scratch, and while I was at it I decided to provide dual-boot capability between CentOS and Ubuntu.  

Having used RH/CentOS pretty much exclusively since moving from Unix (Solaris) to Linux many years back, I learned that even though CentOS and Ubuntu are both Linux, they are very different in ways both large and small.     I shaved a few yaks along the way, and made lots of notes -- hopefully they'll help if you're thinking about making a similar transition.

With [recent events](https://blog.centos.org/2020/12/future-is-centos-stream/) in CentOS-land this has become even more relevant — read on to see how you can easily move back and forth between CentOS and Ubuntu.

<!-- more -->
<br clear="all">

Not too long ago my main Linux development machine, a tiny NUC-style box, stopped booting.  On investigation it turned out that it may not have been a great idea to build it with a 1TB mSATA SSD — to get 1TB on an mSATA form-factor it ends up being really dense and prone to overheating.  I bought a replacement 1TB SSD in a more capacious 2.5” form-factor, and decided to take the time to revisit the original configuration.

One thing that has changed for me over the past couple of years is that I have spent quite a bit of time at my day job developing a middleware transport based on ZeroMQ.  My employer generously agreed to open-source the resulting code (which you can find [here](https://github.com/nyfix/OZ)), but doing so opened up a bunch of issues.  The biggest one was the fact that my employer’s choice of OS has been RedHat, and later CentOS, and while RH/CentOS has been a great choice in terms of stability for our production environment, it has been much less great as a development system.  Which resulted in me spending a lot of time over the past several years figuring out things like how to [build newer compilers](http://btorpey.github.io/blog/2015/01/02/building-clang/) in order to take advantage of improvements in C++ and related tools.

By contrast, most of the “cool kids” working on open-source projects use something other than RH/CentOS, with [Ubuntu](https://ubuntu.com/) looking to be the most popular.  It’s not reasonable to expect others to spin up a whole new development system just to check out a new open-source project, so being stuck on RH/CentOS would seriously impact any interest we might be hoping to generate in the project.

So, my original plan was to build out the new machine to support at least three OS’s: CentOS 7 (our current production environment), CentOS 8 (which we expected to be our next production environment), and Ubuntu (in order to better support our open-source project).  About halfway through building the system RedHat/CentOS dropped the now well-known bombshell that CentOS 8 was no more — at least, not in any form that would be acceptable to us.  

The result is that I ended up building just the CentOS 7 and Ubuntu systems, leaving space for a possible third OS at some point (perhaps [Rocky](https://rockylinux.org/)?).  I’ve come to really appreciate the more modern tools in Ubuntu, which are a boon for development, and the quirks that drove me nuts on CentOS (like not being able to paste text from my Mac) are pretty much gone.
But I needed to learn (and un-learn) a lot in the process.  

Moving to a new OS is a fiddly business, so if you’re thinking about moving from RH/CentOS to Ubuntu (which I suspect many people are at this point), this guide can definitely help you make that transition.  

With that bit of background out of the way, let's get started.

# Installing Ubuntu
We're using Ubuntu 20.04 LTS (long-term support) in this article, since it most closely matches the level of support that we (used to) expect from CentOS.  You can grab an installation ISO [here](https://ubuntu.com/download/desktop).

The Ubuntu install is pretty self-explanatory, (and there's a nice tutorial [here](https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview)).  I chose "Normal Installation" to get as much as possible at one go.

## sudo vs. root

Specifying a user is where things start to get different -- when installing CentOS, for instance, you enter a password for the superuser (`root`) during installation.  

Ubuntu installations, however, typically don't have a `root` user.  Instead, the user you create during installation is automatically given `sudo` rights to all the things that `root` would normally be allowed to do.

So with Ubuntu, instead of using `root` to administer the system directly, like so:

<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>su - 
</span><span class='line'>mount ...
</span><span class='line'>exit</span></code></pre></td></tr></table></div></figure></notextile></div>

You would just use `sudo` instead:

<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo mount ...</span></code></pre></td></tr></table></div></figure></notextile></div>

## Fixing `sudo` timeout

One downside of using `sudo` for administration is that by default Ubuntu will ask for your password every single time.  To avoid that, edit the sudoers file:

    sudo vi /etc/sudoers

And add the following line (this will cause the system to remember your `sudo` password for five minutes):

    Defaults        timestamp_timeout=300


Even so, it can be a hassle typing `sudo` over and over again, especially if you have a lot of taks to perform.  To get around that, you can create a root shell like so:

    sudo /bin/bash
    
or, equivalently:

    sudo -i    

## Enabling root access
So far, although we can run commands with superuser permissions using `sudo`, we can't actually login to the system as `root`.  There are lots of good reasons why this is A Good Idea, and they are well explained [here](https://help.ubuntu.com/community/RootSudo).

So, just to be clear, you should never do what I'm about to tell you how to do...

> As several readers have enthusiastically pointed out, you (a) should never need to enable root, and (b) if you do this on a machine that is exposed to the internet you are asking for Big Trouble.  ***You have been warned*** ...

But if you **really** need to login as root, then you'll need to activate the root user by supplying a password:

    sudo passwd root
    sudo usermod -U root


### Logging in as `root` from the console

To enable root login from the console, you need to edit `/etc/pam.d/gdm-password` and comment out the line containing:

    auth required pam_succeed_if.so user != root quiet_success

so that it looks like this:

    #auth required pam_succeed_if.so user != root quiet_success


### Logging in as `root` via ssh

> One more time -- this is A Very Bad Idea, but if you insist ...

To enable root login via ssh, edit `/etc/ssh/sshd_config` and change 

    #PermitRootLogin prohibit-password

to    

    PermitRootLogin yes
    
On the other hand, if you want to sleep well at night, secure in the knowledge that you are (somewhat) safe from marauding script kiddies, instead change the setting in `/etc/ssh/sshd_config` to:

    PermitRootLogin no


## Set bash as the system default shell
Unlike CentOS, Ubuntu does not use bash as its [default shell](https://wiki.ubuntu.com/DashAsBinSh).  

While there are lots of "better" shells out there, I've become familiar with bash, and I've got lots of scripts that ~~might~~ will break if moved to another shell, and which I just don't want to futz with.  Plus, if things get too hairy for bash, I generally just switch to a real programming language, [like Perl](http://btorpey.github.io/blog/2014/07/23/perl-stdin/). 

To reconfigure the default shell on Ubuntu, you can use the following command:

    sudo dpkg-reconfigure dash

To change a particular user's default shell from `sh` to `bash`:

    sudo chsh -s /bin/bash {user}


## Disable SELinux
Many users, myself included, find SELinux to be a major hassle, and not appropriate for a development (desktop) OS.  In addition, there is still some software, typically older programs, that don't run properly with SELinux.

In CentOS, I disable SELinux, but it's [already disabled](https://wiki.ubuntu.com/SELinux) in Ubuntu, so nothing needs to be done. The Ubuntu equivalent, [AppArmor](https://wiki.ubuntu.com/AppArmor) has so far not interfered with anything in the way that SELinux does on CentOS, and so I haven't had the need to disable it, or in fact tweak it at all.

## Disable iptables
In a similar vein, I generally disable iptables in CentOS.  With Ubuntu, iptables is enabled, but by default it allows all traffic.  So, out-of-the-box everything just works, but you can configure the firewall to be more restrictive if you want to.

> Just to be clear, I disable iptables because (a) I'm on a private subnet with statically-assigned non-routable IP addresses that are not accessible other than from the subnet itself, and (b) I develop network middleware software that both connects to and listens at ephemeral ports, so iptables is pretty much out of the question.  If you don't have similar needs, you're probably better off using iptables the way it was intended -- unfortunately I can't help you with that.
    
## Activate swap partition
On CentOS, I've generally had to explicitly activate any swap partitions, but Ubuntu automatically detects and mounts any swap partitions that it finds on the boot disk.

## Update everything
It's generally a good idea to keep the OS up-to-date, and with Ubuntu that can be accomplished with one or more of the following commands:

    sudo apt update        # Fetches the list of available updates
    sudo apt upgrade       # Installs some updates; does not remove packages
    sudo apt full-upgrade  # Installs updates; may also remove some packages, if needed
    sudo apt autoremove    # Removes any old packages that are no longer needed

See [this](https://askubuntu.com/a/196777) for more on keeping Ubuntu up-to-date.

## Install addl packages
Even with a "normal" installation, there are some useful packages that don't get installed initially:

    sudo apt install tree
    sudo apt install ddd
    sudo apt install dwarves
    sudo apt install oprofile
    sudo apt install linux-tools
    sudo apt install linux-tools-generic
    sudo apt install linux-tools-`uname -r`

## Adding a (shared) user
In my case, since I'm dual-booting between CentOS and Ubuntu, I wanted to create a user that can share files with the same user on CentOS.  

To do that, create a user with the same username and userid as the CentOS user.  In the example below, 8177 is the numeric ID of the CentOS user, referred to as `myuser`.  This user belongs to the group named `shared`, that also shares the same group ID as the CentOS group.

    sudo groupadd -g 8177 shared
    sudo useradd -m -g shared -u 8177 myuser
    sudo passwd myuser
    sudo usermod -a -G users myuser

Another setting that will make it easier to share files between different users and/or OS's is to make files group-writable by default.  To do this, add the following to your `.bashrc`:

    # Set umask to allow group write access.
    umask 002

> Note that the umask setting applies only to newly-created files -- it doesn't affect existing files.

## Setup samba
This step is optional -- you could theoretically use sftp or even NFS (ugh!) to share files with other machines on your network.  

The commands below will setup a minimal Samba system -- again using `myuser` as the name for the shared user -- change that to whatever you choose.

    sudo /bin/bash
    apt install samba
    cd /etc/samba
    cp -p smb.conf smb.conf.orig
    cat > smb.conf <<EOF
    [global]
    	workgroup = WORKGROUP
    	server string = Samba server
    
    	security = user
    	passdb backend = tdbsam
    
    [myuser]
    	path = /home/myuser
    	browseable = yes
    	writable = yes
    	valid users = @shared
    
    [root]
    	path = /
    	browseable = yes
    	writable = no
    EOF
    smbpasswd -a myuser
    systemctl enable smb.service
    systemctl start smb.service
    exit


## Set up ssh access
You're going to want to be able to login to the system remotely, so the sooner you setup ssh the better.

The ssh daemon may not have been installed -- if not, you should install it now:

    sudo apt install openssh-client
    sudo apt install openssh-server
    sudo systemctl start sshd.service
    sudo systemctl status sshd.service

Then, from another machine where you have already generated a public/private key-pair:

    ssh-copy-id -i ~/.ssh/<identity> myuser@<host>

This will copy the public key associated with <identity> to myuser's $HOME/.ssh/authorized_keys on the specified host.  This will let you login via ssh without specifying a password.

You will likely also want to copy and/or create private keys in your ~/.ssh directory, so you can access other resources like GitHub, Stash, etc.  

> The short version is that you'll want to have a private key in `~/.ssh` of the system you are connecting *from*, and the corresponding public key in the `~/.ssh/authorized_keys` file of the system you are connecting *to*.  (Certain services, like GitHub, have their own mechansim for storing public keys).  

You can read more about ssh here:

- <https://www.digitalocean.com/community/tutorials/how-to-configure-ssh-key-based-authentication-on-a-linux-server>
- <https://superuser.com/questions/215504/permissions-on-private-key-in-ssh-folder>


## GUI login
While the good old command line is fine for lots/most things, some applications are only available in GUI form, or can do things in GUI mode that they can't do from the command line.

### Screen Sharing
There are a few options for screen sharing in Ubuntu -- the simplest is to activate Screen Sharing via the Settings application.  This allows you to require a password, as well as to restrict connections to a particular network adapter.

You can connect to the shared screen using a VNC viewer application by specifying `{hostname}:0`.  

> On Mac, you can also choose "Go","Connect to Server" from the Finder menu, and specify `vnc://{hostname}`. 

This will give you a GUI into the (one-and-only) console screen.  A disadvantage of this approach is that there is only one console screen, and it is a fixed size (matching the size of the physical screen). 

### VNC
Ubuntu defaults to [TightVNC](https://www.tightvnc.com/), but also provides [TigerVNC](https://tigervnc.org/), which for whatever reason seems to work better for me.  To install it:

    sudo apt install tigervnc-standalone-server

Once it's installed, create a password for accessing your desktop:

    vncpasswd
    
There are a bunch of different desktops that you can run with VNC, but I prefer to use Gnome -- for that, configure your VNC startup script like so:

    cd ~/.vnc
    cp xstartup xstartup.orig
    cat > xstartup <<EOF
    #!/bin/sh
    
    [ -x /etc/vnc/xstartup ] && exec /etc/vnc/xstartup
    [ -r $HOME/.Xresources ] && xrdb $HOME/.Xresources
    vncconfig -iconic &
    dbus-launch --exit-with-session gnome-session &
    EOF
    chmod +x xstartup

To start the VNC server:

    vncserver -localhost no -geometry 1920x1050

(Or whatever geometry you prefer).

- The VNC session can sometimes get "stuck" if the screen saver kicks in.  For that reason it's a good idea to disable the screen saver: You can do this from "Settings", "Power" -- set "Blank Screen" to "Never".
- If a session does get stuck, you can un-stick it with `sudo loginctl unlock-sessions`.  (See <https://askubuntu.com/questions/1224957/i-cannot-log-in-a-vnc-session-after-the-screen-locks-authentification-error> for more).

There are a number of VNC viewers available:

- [RealVNC](https://www.realvnc.com/en/connect/download/viewer/)
- [TightVNC](https://www.tightvnc.com/)
- [TigerVNC](https://tigervnc.org/)

Personally, I find the TigerVNC server and RealVNC viewer to be the best combination, but as always your mileage may vary.

## Enable core files

If you're running *other* people's code, you may need to be able to debug core files ;-)  By default, Ubuntu won't create any, so follow these steps to enable core file creation.

First make sure `ulimit` is set properly (e.g., in your `.bash_profile`):

    ulimit -c unlimited

Ubuntu has its equivalent to CentOS' ABRT service called [`apport`](https://wiki.ubuntu.com/Apport), which *definitely* interferes with creation of core files, so you will need to disable it:

    sudo systemctl disable apport.service

Next set the core file pattern used to create core files -- I use a pattern of the form "{program name}.core.{pid}" (with core file in the processes' current directory), but that is mostly an accident of history.  The full documentation for the tokens you can include in the file name can be found [here](<https://man7.org/linux/man-pages/man5/core.5.html>).

To change the current value (in memory):

    sudo sysctl -w kernel.core_pattern=%e.core.%p
 
To make the change permanent, edit `/etc/sysctl.conf` (as root) and add the following line: 

    kernel.core_pattern=%e.core.%p

I work with in-memory databases that store data in shared memory a lot, so a useful tweak for me is to exclude shared memory segments from core files:

     echo 0x31 > /proc/self/coredump_filter

## Configure gdb

There are a number of non-default settings that can make gdb more useful, or just more pleasant to use. I set these in my [`~/.gdbinit`](https://man7.org/linux/man-pages/man5/gdbinit.5.html):

    # let gdb load settings from anywhere
    set auto-load safe-path /
    
    # allow breakpoints in dynmically loaded modules
    set breakpoint pending on
    
    # esp. useful w/set logging
    set height 0
    
    # more readable strings w/repeating characters
    set print repeats 0
    
    # show libraries as they are loaded
    set verbose on
    
    # load pretty-printers for std::
    python
    # find the printers.py file associated with current compiler
    # (typically in usr/share/<compiler-version>/python/libstdcxx/v6/printers.py), installed from
    cmd = "echo -n $(dirname $(find $(cd $(dirname $(which gcc))/.. && /bin/pwd) -name printers.py 2>/dev/null))"
    import os
    tmp = os.popen(cmd).read()
    # import the pretty printers
    import sys
    sys.path.insert(0, tmp)
    from printers import register_libstdcxx_printers
    register_libstdcxx_printers (None)
    end
    
    # if you want to use Ctrl-C w/debugee
    #handle SIGINT stop pass
     
## Enabling gdb attach
By default, Ubuntu [doesn't let non-child processes attach to another process](https://wiki.ubuntu.com/SecurityTeam/Roadmap/KernelHardening#ptrace%20Protection).  

Obviously, this breaks `gdb -p ...` and related.  To disable this feature, edit `/etc/sysctl.d/10-ptrace.conf` (as root) and change: 

    kernel.yama.ptrace_scope = 1

to 

    kernel.yama.ptrace_scope = 0
 
To change the current value in memory:

    sudo echo 0 > /proc/sys/kernel/yama/ptrace_scope
    

## Configuring perf    

The `perf` program and its friends are very useful for seeing where a particular program spends its time.  But by default, it has [certain restrictions](https://unix.stackexchange.com/a/14256/198530).

To remove those restrictions permanently, edit `/etc/sysctl.conf` and add:

    kernel.perf_event_paranoid = 0

To make a temporary change (until reboot):

    echo 0 > /proc/sys/kernel/perf_event_paranoid
    
## Compiler
You can determine which compiler was used to build the kernel on Linux -- on Ubuntu it shows that the system compiler is gcc 9.3.0 (2019) (vs gcc 4.8.5 (2015) on CentOS 7):  

    $ cat /proc/version    Linux version 5.8.0-43-generic (buildd@lcy01-amd64-018) (gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, GNU ld (GNU Binutils for Ubuntu) 2.34) #49~20.04.1-Ubuntu SMP Fri Feb 5 09:57:56 UTC 2021

The newer compiler includes a bunch of new features, bug fixes, etc. and also has different [default settings for some diagnostics](https://wiki.ubuntu.com/ToolChain/CompilerFlags#Default_Flags), including:

    -fasynchronous-unwind-tables
    -fstack-protector-strong
    -Wformat
    -Wformat-security
    -fstack-clash-protection
    -fcf-protection

In addition to the above flags, gcc 9.3.0 on Ubuntu includes a default setting for `-D_FORTIFY_SOURCE=2`, which causes additional checks to be inserted -- one of them is a check for buffer overflow, which will cause an executable to abort if an overflow is detected:

    *** buffer overflow detected ***: terminated
    Aborted (core dumped)

A typical stack trace at the time of the core will look something like this:

    #0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50
    #1  0x00007f55d1bec859 in __GI_abort () at abort.c:79
    #2  0x00007f55d1c573ee in __libc_message (action=action@entry=do_abort, fmt=fmt@entry=0x7f55d1d8107c "*** %s ***: terminated\n") at ../sysdeps/posix/libc_fatal.c:155
    #3  0x00007f55d1cf9b4a in __GI___fortify_fail (msg=msg@entry=0x7f55d1d81012 "buffer overflow detected") at fortify_fail.c:26
    #4  0x00007f55d1cf83e6 in __GI___chk_fail () at chk_fail.c:28
    #5  0x00007f55d1cf7cc6 in __strcpy_chk (dest=dest@entry=0x7f55cd871808 "\001", src=src@entry=0x7f55c0039e0b ".0000000000000001", destlen=destlen@entry=17) at strcpy_chk.c:30
    #6  0x00007f55cfa303d3 in strcpy (__src=0x7f55c0039e0b ".0000000000000001", __dest=0x7f55cd871808 "\001") at /usr/include/x86_64-linux-gnu/bits/string_fortified.h:90
    ...

For more information, see [Stackguard interals](https://en.wikibooks.org/wiki/GNU_C_Compiler_Internals/Stackguard_4_1).

The default Ubuntu settings proved their worth quickly by identifying an "off-by-one" buffer overflow in [OZ](https://github.com/nyfix/OZ) that had eluded Address Sanitizer, valgrind, glibc, cppcheck, clang-tidy and PVS-Studio.

## Linker

### Unresolved symbols
If you suddenly start getting "unresolved symbol" errors from your builds, one possible reason is that the Ubuntu linker (`ld`) works differently than on CentOS.

Unlike RedHat/CentOS, the Ubuntu linker only searches a library *once*, at the point that it is encountered on the command line (<https://manpages.ubuntu.com/manpages/focal/man1/ld.1.html>):

> The linker will search an archive only once, at the location where it is specified on
           the command line.  If the archive defines a symbol which was undefined in some object
           which appeared before the archive on the command line, the linker will include the
           appropriate file(s) from the archive.  However, an undefined symbol in an object
           appearing later on the command line **will not cause the linker to search the archive
           again.**

This is the documented behavior in the man pages, but the CentOS linker actually behaves as if all the libraries specified on the command line were specified in `--start-group`/`--end-group` flags. -- in other words, the order of libraries on CentOS is immaterial.

If you are getting "unresolved" errors at link time, it is most likely because the order of libraries used to build the executable is incorrect.  You can either correct the order, add `--start-group`/`--end-group` commands, or possibly use a different linker, as discussed [here](https://stackoverflow.com/questions/34164594/gcc-ld-method-to-determine-link-order-of-static-libraries/34168951).

### Implicit shared library dependencies
Another difference between CentOS and Ubuntu linkers is the way they handle dependencies between shared libraries.  You can see these [DT_NEEDED](https://man7.org/linux/man-pages/man5/elf.5.html) dependencies with the `readelf --dynamic` command.

These differences are caused by different default flags being passed to the linker -- you can see these with:

    gcc -dumpspecs | less
    
The output isn't the easiest thing to understand, but if you look at the output you'll see the template for default parameters following the `*link:` line -- e.g., on CentOS it will look something like this:

    *link:    %{!r:--build-id} --no-add-needed ...
#### CentOS
On CentOS, the linker defines `--no-add-needed` (which is a deprecated alias for `--no-copy-dt-needed-entries`), and does *not* define `--as-needed`.  

What this means is that the linker: 

- will output a DT_NEEDED entry for *every* library specified on the command line (even if it is not used to resolve any symbols), and
- will *not* copy DT_NEEDED entries from libraries specified on the command line.

The second part changed [as of CentOS 7](https://bugzilla.redhat.com/show_bug.cgi?id=1292230), as a result of an upstream [change in Fedora](https://fedoraproject.org/wiki/Features/ChangeInImplicitDSOLinking). 

The short version is you get a DT_NEEDED entry for every library specified on the command line, but not for the libraries that those libraries need.

#### Ubuntu
Ubuntu does things differently -- its linker defaults to `--as-needed`, which means that the linker:

- will output a DT_NEEDED entry for libraries specified on the command line, but *only* if that library is used to resolve one or more symbols, and
- will also copy DT_NEEDED entries incluced in any of those libraries, but again *only* if it is needed to resolve a symbol.

The short version is that you get a DT_NEEDED entry *only* for libraries that are used to resolve a symbol.

#### Summary

In short, CentOS adds DT_NEEDED entries for all the libraries specified on the command line, but not for any of their dependencies; while Ubuntu adds entries for libraries specified on the command line, as well as their dependencies, but *only* if those libraries are actually needed.

As always, if you want or need to know more about shared libraries on Linux, you should check out [Drepper's paper](https://akkadia.org/drepper/dsohowto.pdf), which is still the authoritative source.

## clang
clang goes to a lot of trouble to co-exist with gcc -- for instance,  preferring to use gcc's libstdc++ for the C++ standard library, enabling code compiled by clang to call and be called by code compiled using gcc.

On Ubuntu this can be a problem though, because sometimes clang *thinks* it found a real installation of gcc, but in fact the installation is incomplete, and unusable.  If your clang builds complain about missing include or library files, it's likely that clang is trying to use a borked install of gcc.  

But, how does clang know where to find those files in the first place?  Partly this has to do with [how clang is built](http://btorpey.github.io/blog/2015/01/02/building-clang/), since clang is itself typically built using gcc.  You can see which gcc installations clang finds at run-time, with the following command:

    $ clang++ -v -E
    clang version 10.0.0-4ubuntu1 
    Target: x86_64-pc-linux-gnu
    Thread model: posix
    InstalledDir: /usr/bin
    Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/10
    Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/9
    Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/10
    Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/9
    Selected GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/10
    Candidate multilib: .;@m64
    Selected multilib: .;@m64

In my case, the gcc 10 installation was incomplete, but clang tried to use it anyway.  And, since ubuntu installs all its gcc versions in `/usr`, passing `--gcc-toolchain` to clang doesn't really help.  In my case, I had to remove the offending, unusable gcc installations:

    sudo apt remove gcc-10
    sudo apt remove gcc-10-base
    sudo apt remove libgcc-10-dev

Once that was done, clang found the correct version (9) of gcc:

    $ clang++ -v -E
    clang version 10.0.0-4ubuntu1 
    Target: x86_64-pc-linux-gnu
    Thread model: posix
    InstalledDir: /usr/bin
    Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/10
    Found candidate GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/9
    Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/10
    Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/9
    Selected GCC installation: /usr/bin/../lib/gcc/x86_64-linux-gnu/9
    Candidate multilib: .;@m64
    Selected multilib: .;@m64

# Conclusion
That's all I've found so far, but I'll keep updating this post as I run into more differences between CentOS and Ubuntu.  As I said above, I'm really enjoying Ubuntu, and I intend to use it almost exclusively for development going forward, booting back to CentOS only to regression-test changes, at least in the short term.  In the meantime, I'll be watching what goes on with [Rocky](https://rockylinux.org/) and/or other projects that spring up to fill the void left by IBM/RH/CentOS.

If you have any questions, suggestions, etc. about this article, please leave a comment below, or [email me directly](<mailto:wallstprog@gmail.com>).
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Memory Checking]]></title>
    <link href="http://btorpey.github.io/blog/2019/07/14/memory-checking/"/>
    <updated>2019-07-14T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2019/07/14/memory-checking</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/memory-testing.jpg">

At my day job, I spend a fair amount of time working on software reliability.  One way to make software more reliable is to use memory-checking tools like valgrind's [memcheck](http://www.valgrind.org/info/tools.html#memcheck) and clang's [AddressSanitizer](https://github.com/google/sanitizers/wiki/AddressSanitizer) to detect memory errors at runtime.  

But these tools are typically not appropriate to use all the time -- valgrind causes programs to run much more slowly than normal, and AddressSanitizer needs a special instrumented build of the code to work properly.  So neither tool is typically well-suited for production code.

But there's another memory-checking tool that is "always on".  That tool is plain old `malloc`, and it is the subject of this article.

<!-- more -->

The [GNU C library](https://www.gnu.org/software/libc/) (glibc for short) provides implementations for the C standard library functions (e.g., `strlen` etc.) including functions that interface to the underlying OS (e.g., `open` et al.).  glibc also provides functions to manage memory, including `malloc`, `free` and their cousins, and in most code these memory management functions are among the most heavily used.

It's not possible to be a C programmer and not be reasonably familiar with the memory management functions in glibc.  But what is not so well-known is the memory-checking functionality built into the library.

It turns out that glibc contains two separate sets of memory management functions -- the core functions do minimal checking, and are significantly faster than the "debug" functions, which provide additional runtime checks.

The memory checking in `malloc` is controlled by an environment variable, named appropriately enough `MALLOC_CHECK_` (note the trailing underscore).  You can configure `malloc` to perform additional checking, and whether to print an error message and/or abort with a core file if it detects an error.  You can find full details at <http://man7.org/linux/man-pages/man3/mallopt.3.html>, but here's the short version:

Value | Impl | Checking | Message | Backtrace + mappings (since glibc 2.4+) | Abort w/core
------| ---- | -------- | ------ | ------ | ------
**default (unset)** | **Fast** | **Minimal** | **Detailed** | **Yes** | **Yes**
0 | Fast | Minimal | None |  No | No
1 | Slow | Full | Detailed | No | No
2 | Slow | Full | None | No | Yes
3 | Slow | Full | Detailed | Yes | Yes
5 | Slow | Full | Brief | No | No
7 | Slow | Full | Brief | Yes | Yes

<br>

What may be surprising is that the default behavior is for `malloc` to do at least minimal checking at runtime, and to **abort the executable with a core file** if those checks fail.  

This may or may not be what you want.  Given that the minimal checks in the fast implementation only detect certain specific errors, and that those errors (e.g., double free) tend not to cause additional problems, you may decide that a "no harm, no foul" approach is more appropriate (for example with production code where aborting with a core file is frowned upon ;-).

The other relevant point here is that setting `MALLOC_CHECK_` to any non-zero value causes `malloc` to use the slower heap functions that perform additional checks.  I've included a [sample benchmark program](https://github.com/WallStProg/malloc-check/blob/master/malloc-bench.cpp) that shows the additional checking adds about 30% to the overhead of the `malloc`/`free` calls.  (And while the benchmark program is dumb as dirt, its results are similar to results on "real-world" tests).

### Multi-threaded Performance
If the benchmark code is to be believed, the impact on performance of the extra checking when `MALLOC_CHECK_` is set to a non-zero value is **much** (as in an order of magnitude) greater when multiple threads are accessing the heap concurrently.  This would suggest that there is contention on the data structures used for full checking, over and above normal heap contention.

It would be nice if one could get a fast implementation with the option to output an error message and continue execution, but with the current[^rh7] implementation of glibc that doesn't appear to be possible.  If you want the fast implementation but you don't want to abort on errors, the only option is to turn off checking entirely (by explicitly setting `MALLOC_CHECK_` to 0).  

[^rh7]: Current for RedHat/CentOS 7 in any case, which is glibc 2.17.

Note also that the [documentation](http://man7.org/linux/man-pages/man3/mallopt.3.html) is a bit misleading:

> Since glibc 2.3.4, the default value for the M_CHECK_ACTION              parameter is 3.

While it's true that with no value specified for `MALLOC_CHECK_` an error will cause a detailed error message with backtrace and mappings to be output, along with an abort with core file, that is **NOT** the same as explicitly setting `MALLOC_CHECK_=3` -- that setting also causes `malloc` to use the slower functions that perform additional checks.

### "Minimal" vs. "Full" Checking

- In the table above, the "checking" setting for `MALLOC_CHECK_=0` is "minimal" -- the checks are still performed, but errors are simply not reported.
  - Note that it is not possible to completely disable checking -- minimal checking is *always* performed, even if the results are ignored. 
- The errors that can be detected with "minimal" checking are limited to a small subset of those detected with "full" checking -- sometimes even for the same error.  For instance, with minimal checking a double-free can be detected *only* if the second free occurs immediately after the first.  With full checking the double-free is detected even if there are intervening calls to `malloc` and `free`.

And, of course, the built-in checking in glibc can't detect a *lot* of errors that can be found with more robust tools, like [valgrind](http://www.valgrind.org/) and [AddressSanitizer](https://github.com/google/sanitizers/wiki/AddressSanitizer).  Nevertheless, `MALLOC_CHECK_` can be a useful adjunct to those tools for everyday use in development.

## Conclusions
- For typical development, it's probably best to explicitly set `MALLOC_CHECK_=3`.  This provides additional checking over and above the default setting, at the cost of somewhat poorer performance.
- For production use, you may want to decide whether the benefit of minimal checking is worth the possibility of having programs abort with errors that may be benign.  If the default is not appropriate, you basically have two choices:
  - Setting `MALLOC_CHECK_=1` will allow execution to continue after an error, but will at least provide a message that can be logged[^log] to provide a warning that things are not quite right, and trigger additional troubleshooting, but at the cost of somewhat poorer performance.
  - If you can't afford to give up any performance at all, you can set `MALLOC_CHECK=0`, but any errors detected will be silently ignored.
- Last but not least, if you're running multi-threaded code the performance penalty with full checking is potentially much more significant.  You'll probably want to benchmark your code  both with and without full checking if you're thinking of enabling it in multi-threaded code.

[^log]: The error message from glibc is written directly to the console (tty device), not to `stderr`, which means that it will not be redirected.  If you need the message to appear on stderr, you will need to [set another environment variable](https://bugzilla.redhat.com/show_bug.cgi?id=1519182):
    `export LIBC_FATAL_STDERR_=1`
    
## Code
The code for this article is available [here](https://github.com/WallStProg/malloc-check.git).  There's a benchmark program, which requires [Google Benchmark](https://github.com/google/benchmark).  There are also sample programs which demonstrate a double-free error that can be caught even with minimal checking (`double-free.c`), and which cannot (`double-free2.c`), and a simple script that ties everything together.  

## Footnotes





</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lots o' static]]></title>
    <link href="http://btorpey.github.io/blog/2017/09/17/lotso-static/"/>
    <updated>2017-09-17T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2017/09/17/lotso-static</id>
    <content type="html"><![CDATA[<p><img class="left" src="/images/static-cat.jpg" width="250" height="188">

- Will be replaced with the ToC, excluding the "Contents" header
{:toc}

I've written before about [static analysis](/blog/categories/static-analysis/), but in those earlier posts I wasn't able to give specific examples of real-world code where static analysis is able to discover latent errors.

In the earlier articles I used a synthetic code-base [from ITC Research](https://github.com/regehr/itc-benchmarks) to test clang, cppcheck and PVS-Studio.  I also ran all three tools on the code-bases that I'm responsible for maintaining at my "day job", but I wasn't able to share detailed results from that analysis, given that the code is not public.

In this article, I want to expand the discussion of static analysis by diving into a real-world, open-source code base that I've been working with lately, with specific examples of the kinds of problems static analysis can expose.

<!-- more -->


## OpenMAMA
For this example, I'll be using the [OpenMAMA](http://openmama.org) source code.  OpenMAMA is an open-source messaging middleware framework that provides a high-level API for a bunch of messaging transports, including open-source (Qpid/AMQP, ZeroMQ) and commercial (DataFabric, Rendezvous, Solace, etc).

OpenMAMA is an interesting project -- it started back in 2004 with Wombat Financial Software, which was attempting to sell its market-data software, but found it to be tough sledding.  While Wombat's software performed better and was less expensive than Tibco's Rendezvous (the de-facto standard at the time), no one wanted to rewrite their applications to target an API from a small company that might not be around in a couple of years.

So Wombat developed an open API which could sit on top of any messaging middleware, and they called it MAMA, for Middleware Agnostic Messaging API.  They also developed bindings for Rendezvous, in addition to their own software, so that prospective customers would have a warm and fuzzy feeling that they could write their applications once, and seamlessly switch out the underlying middleware software with little or no changes to their code.

That strategy worked well enough that in 2008 Wombat was acquired by the New York Stock Exchange, which renamed the software "Data Fabric" and used it as the backbone of their market-data network (SuperFeed).

When the company I was working for was also acquired by NYSE in 2009 I was tasked with replacing our existing middleware layer with the Mama/Wombat middleware, and in the process I came to appreciate the "pluggable" architecture of MAMA -- it doesn't make the issues related to different messaging systems go away, but it does provide a framework for dealing with them.

In 2011 NYSE Technologies donated OpenMAMA to the Linux Foundation.  Then, in 2014, the Wombat business was sold by NYSE to [Vela Trading Technologies](https://www.velatradingtech.com/) (née SR Labs), which provides the proprietary Data Fabric middleware, and is also the primary maintainer for OpenMAMA.  There are a number of different [open-source and commercial implementations of OpenMAMA](http://www.openmama.org/what-is-openmama/supported-software).

Which brings us to the present day -- I've recently started working with OpenMAMA again, so it seemed like a good idea to use that code as an example of how to use static analysis tools to identify latent bugs.

And, just to be clear, this is not a criticism of OpenMAMA -- it's an impressive piece of work, and has proven itself in demanding real-world situations.  

## Following along

The analysis presented here is based on OpenMAMA release 6.2.1, which can be found [here](https://github.com/OpenMAMA/OpenMAMA/releases/tag/OpenMAMA-6.2.1-release).

I used [cppcheck version 1.80](http://cppcheck.sourceforge.net/) and [clang version 5.0.0](http://releases.llvm.org/download.html#5.0.0).

Check out the [earlier articles in this series](/blog/categories/static-analysis/) for more on building and running the various tools, including a bunch of helper scripts in the [GitHub repo](https://github.com/btorpey/static).

For the OpenMAMA analysis, I first built OpenMAMA using [Bear](https://github.com/rizsotto/Bear) to create a compilation database from the scons build:  

~~~bash
bear scons blddir=build product=mama with_unittest=n \
middleware=qpid with_testtools=n with_docs=n
~~~

With the compilation database in place, I ran the following scripts[^tools], redirecting their output to create the result files:

~~~bash
cc_cppcheck.sh -i common/c_cpp/src/c/ -i mama/c_cpp/src/c/ -c cc_clangcheck.sh -i common/c_cpp/src/c/ -i mama/c_cpp/src/c/ -c cc_clangtidy.sh -i common/c_cpp/src/c/ -i mama/c_cpp/src/c/ -c 
~~~

The results from running the tools on OpenMAMA can also be found in [the repo](https://github.com/btorpey/static/tree/master/openmama), along with a `compile_commands.json` file that can be used without the need to build OpenMAMA from source[^mamabuild].  To do that, use the following commands:

    cd [directory]
    git clone https://github.com/OpenMAMA/OpenMAMA.git
    git clone https://github.com/btorpey/static.git
    export PATH=$(/bin/pwd)/static/scripts:$PATH
    cp static/openmama/* OpenMAMA
    cd OpenMAMA
    cc_cppcheck.sh -i common/c_cpp/src/c/ -i mama/c_cpp/src/c/ -c 

I use the wonderful [Beyond Compare](/blog/2013/01/29/beyond-compare/) to, well, compare the results from different tools.

[^tools]: Simply clone the [GitHub repo](https://github.com/btorpey/static) to any directory, and then add the `scripts` directory to your `PATH`.

[^mamabuild]: OpenMAMA has its share of prerequisites -- you can get a full list [here](https://openmama.github.io/openmama_build_instructions.html).

## False Positives
Before we do anything else, let's deal with the elephant in the room -- false positives.  As in, warning messages for code that is actually perfectly fine.  Apparently, a lot of people have been burned by "lint"-type programs with terrible signal-to-noise ratios.  I know -- I've been there too.

Well, let me be clear -- these are not your father's lints.  I've been running these tools on a lot of real-world code for a while now, and there are essentially  NO false positives.  If one of these tools complains about some code, there's something wrong with it, and you really want to fix it.

## Style vs. Substance
cppcheck includes a lot of "style" checks, although the term can be misleading  -- there are a number of "style" issues that can have a significant impact on quality.

One of them crops up all over the place in OpenMAMA code, and that is the "The scope of the variable '\<name>' can be reduced" messages.  The reason for these is because of OpenMAMA's insistence on K&R-style variable declarations (i.e., all block-local variables must be declared before any executable statements).  Which, in turn, is caused by OpenMAMA's decision to support several old and broken Microsoft compilers[^vs].

[^vs]: The list of supported platforms for OpenMAMA is [here](https://openmama.github.io/openmama_supported_platforms.html).  You can also find a lot of griping on the intertubes about Microsoft's refusal to support C99, including [this rather weak response](https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/suggestions/2089423-c99-support) from Herb Sutter.  Happily, VS 2013 ended up supporting (most of) C99\. 

The consensus has come to favor declaring variables as close to first use as possible, and that is part of the [C++ Core Guidelines](https://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#es21-dont-introduce-a-variable-or-constant-before-you-need-to-use-it).  The only possible down-side to this approach is that it makes it easier to inadvertently declare "shadow" variables (i.e., variables with the same name in both inner and outer scopes), but modern compilers can flag shadow variables, which mitigates this potential problem (see my earlier article ["Who Knows What Evil Lurks..."](/blog/2015/03/17/shadow/) for more).

Some other "style" warnings produced by cppcheck include:

> \[mama/c_cpp/src/c/bridge/qpid/transport.c:1413]: (style) Consecutive return, break, continue, goto or throw statements are unnecessary.

These are mostly benign, but reflect a lack of understanding of what statements like `continue` and `return` do, and can be confusing.
<br>
<br>

> \[common/c_cpp/src/c/list.c:295 -> common/c_cpp/src/c/list.c:298]: (style) Variable 'rval' is reassigned a value before the old one has been used.

There are a lot of these in OpenMAMA, and most of them are probably caused by the unfortunate decision to standardize on K&R-style local variable declarations, but in other cases this can point to a potential logic problem.  (Another good reason to avoid K&R-style declarations).

<br>
Similar, but potentially more serious is this one:

> \[mama/c_cpp/src/c/bridge/qpid/transport.c:275]: (style) Variable 'next' is assigned a value that is never used.

Maybe the variable was used in an earlier version of the code, but is no longer needed.  Or maybe we ended up using the wrong variable when we mean to use `next`.

## Dead Code

There are also cases where the analyzer can determine that the code as written is meaningless

> \[mama/c_cpp/src/c/bridge/qpid/subscription.c:179]: (style) A pointer can not be negative so it is either pointless or an error to check if it is.

If something cannot happen, there is little point to testing for it -- so testing for impossible conditions is almost always a sign that something is wrong with the code.

Here are a few more of the same ilk:

> \[mama/c_cpp/src/c/dictionary.c:323]: (style) Checking if unsigned variable '*size' is less than zero.

<!-- -->
> \[mama/c_cpp/src/c/statslogger.c:731]: (style) Condition 'status!=MAMA_STATUS_OK' is always false

<!-- -->
> \[mama/c_cpp/src/c/dqstrategy.c:543]: (style) Redundant condition: If 'EXPR == 3', the comparison 'EXPR != 2' is always true.

Whether these warnings represent real bugs is a question that needs to be answered on a case-by-case basis, but I hope we can agree that they at the very least represent a ["code smell"](https://en.wikipedia.org/wiki/Code_smell), and the fewer of these in our code, the better.

## Buffer Overflow
There are bugs, and there are bugs, but bugs that have a "delayed reaction", are arguably the worst, partly because they can be so hard to track down.  Buffer overflows are a major cause of these kinds of bugs -- a buffer overflow can trash return addresses on the stack causing a crash, or worse they can alter the program's flow in ways that seem completely random.

Here's an example of a buffer overflow in OpenMAMA that was detected by cppcheck:

> \[common/c_cpp/src/c/strutils.c:632]: (error) Array 'version.mExtra[16]' accessed at index 16, which is out of bounds.


Here's the offending line of code:

~~~
    version->mExtra[VERSION_INFO_EXTRA_MAX] = '\0';
~~~


And here's the declaration:

~~~c
    char mExtra[VERSION_INFO_EXTRA_MAX];
~~~


It turns out that this particular bug was fixed subsequent to the release -- the bug report is [here](https://github.com/OpenMAMA/OpenMAMA/pull/310).  Interestingly, the bug report mentions that the bug was found using clang's Address Sanitizer,  which means that code must have been executed to expose the bug.     Static analyzers like cppcheck can detect this bug without the need to run the code, which is a big advantage of static analysis.  In this example, cppcheck can tell at compile-time that the access is out-of-bounds, since it knows the size of mExtra.

Of course, a static analyzer like cppcheck can't detect *all* buffer overflows -- just the ones that can be evaluated at compile-time.  So, we still need Address Sanitizer, or valgrind, or some other run-time analyzer, to detect overflows that depend on the run-time behavior of the program.  But I'll take all the help I can get, and detecting at least some of these nasty bugs at compile-time is a win.

## NULL pointer dereference
In contrast to the buffer overflow type of problem, dereferencing a NULL pointer is not mysterious at all -- you're going down hard, right now.

So, reasonable programmers insert checks for NULL pointers, but reasonable is not the same as perfect, and sometimes we get it wrong.

> \[mama/c_cpp/src/c/msg.c:3619] -> [mama/c_cpp/src/c/msg.c:3617]: (warning, inconclusive) Either the condition '!impl' is redundant or there is possible null pointer dereference: impl.

Here's a snip of the code in question -- see if you can spot the problem:

~~~c
3613    mamaMsgField
3614    mamaMsgIterator_next (mamaMsgIterator iterator)
3615    {
3616        mamaMsgIteratorImpl* impl         = (mamaMsgIteratorImpl*)iterator;
3617        mamaMsgFieldImpl*    currentField = (mamaMsgFieldImpl*) impl->mCurrentField;
3618
3619        if (!impl)
3620            return (NULL);
~~~

cppcheck works similarly to other static analyzers when checking for possible NULL pointer dereference -- it looks to see if a pointer is checked for NULL, and if it is, looks for code that dereferences the pointer outside the scope of that check.

In this case, the code checks for `impl` being NULL, but not until it has already dereferenced the pointer.  cppcheck even helpfully ties together the check for NULL and the (earlier) dereference. (Ahem -- yet another reason to avoid K&R-style declarations).

## Leaks
Similarly to checking for NULL pointers, detecting leaks is more of a job for valgrind, Address Sanitizer or some other run-time analysis tool.  However, that doesn't mean that static analysis can't give us a head-start on getting rid of our leaks.

For instance, cppcheck has gotten quite clever about being able to infer run-time behavior at compile-time, as in this example:

> \[mama/c_cpp/src/c/transport.c:269]: (error) Memory leak: transport
<br>
> \[mama/c_cpp/src/c/transport.c:278]: (error) Memory leak: transport
Here's the code:

~~~c
253 mama_status
254 mamaTransport_allocate (mamaTransport* result)
255 {
256     transportImpl*  transport    =   NULL;
257     mama_status     status       =   MAMA_STATUS_OK;
258
259
260     transport = (transportImpl*)calloc (1, sizeof (transportImpl ) );
261     if (transport == NULL)  return MAMA_STATUS_NOMEM;
262
263     /*We need to create the throttle here as properties may be set
264      before the transport is actually created.*/
265     if (MAMA_STATUS_OK!=(status=wombatThrottle_allocate (&self->mThrottle)))
266     {
267         mama_log (MAMA_LOG_LEVEL_ERROR, "mamaTransport_allocate (): Could not"
268                   " create throttle.");
269         return status;
270     }
271
272     wombatThrottle_setRate (self->mThrottle,
273                            MAMA_DEFAULT_THROTTLE_RATE);
274
275     if (MAMA_STATUS_OK !=
276        (status = wombatThrottle_allocate (&self->mRecapThrottle)))
277     {
278         return status;
279     }
280
281     wombatThrottle_setRate (self->mRecapThrottle,
282                             MAMA_DEFAULT_RECAP_THROTTLE_RATE);
283
284     self->mDescription          = NULL;
285     self->mLoadBalanceCb        = NULL;
286     self->mLoadBalanceInitialCb = NULL;
287     self->mLoadBalanceHandle    = NULL;
288     self->mCurTransportIndex    = 0;
289     self->mDeactivateSubscriptionOnError = 1;
290     self->mGroupSizeHint        = DEFAULT_GROUP_SIZE_HINT;
291     *result = (mamaTransport)transport;
292
293     self->mName[0] = '\0';
294
295     return MAMA_STATUS_OK;
296 }
~~~

cppcheck is able to determine that the local variable `transport` is never assigned in the two early returns, and thus can never be freed.
<br>

Not to be outdone, clang-tidy is doing some kind of flow analysis that allows it to catch this one:

> \[mama/c_cpp/src/c/queue.c:778]: warning: Use of memory after it is freedHere's a snip of the code that clang-tidy is complaining about:

~~~ c
651 mama_status652 mamaQueue_destroy (mamaQueue queue)653 {654     mamaQueueImpl* impl = (mamaQueueImpl*)queue;655     mama_status    status = MAMA_STATUS_OK;...776         free (impl);777778         mama_log (MAMA_LOG_LEVEL_FINEST, "Leaving mamaQueue_destroy for queue 0x%X.", queue);779         status = MAMA_STATUS_OK;780     }781782    return status;783 }
~~~
clang-tidy understands that `queue` and `impl` are aliases for the same variable, and thus knows that it is illegal to access `queue` after `impl` has been freed.  In this case, the access causes no problems, because we're only printing the address, but clang-tidy can't know that[^interproc].

[^interproc]: Unless it knows what `mama_log` does.  It turns out that clang-tidy can do inter-procedural analysis, but only within a single translation unit.  There is some work ongoing to add support for analysis across translation units by Gábor Horvath et al. -- for more see ["Cross Translational Unit Analysis in Clang Static Analyzer: Prototype and Measurements"](http://llvm.org/devmtg/2017-03//2017/02/20/accepted-sessions.html#7).


## Pointer Errors
I've <del>ranted</del> written [before](/blog/2014/09/23/into-the-void/) on how much I hate `void*`'s.  For better or worse, the core OpenMAMA code is written in C, so there are a whole bunch of casts between `void*`s and "real" pointers that have the purpose of encapsulating the internal workings of the internal objects managed by the code.

In C this is about the best that can be done, but it can be hard to keep things straight, which can be a source of errors (like this one):

> \[mama/c_cpp/src/c/fielddesc.c:76]: (warning) Assignment of function parameter has no effect outside the function. Did you forget dereferencing it?And here's the code:

~~~ c
65  mama_status66  mamaFieldDescriptor_destroy (mamaFieldDescriptor descriptor)67  {68      mamaFieldDescriptorImpl* impl = (mamaFieldDescriptorImpl*) descriptor;6970      if (impl == NULL)71          return MAMA_STATUS_OK;7273      free (impl->mName);74      free (impl);7576      descriptor = NULL;77      return MAMA_STATUS_OK;78  }
~~~

Of course `mamaFieldDescriptor` is defined as a `void*`, so it's perfectly OK to set it to NULL, but since it's passed by value, the assignment has no effect other than to zero out the copy of the parameter on the stack.

## But Wait, There's More!
The preceding sections go into detail about specific examples of serious errors detected by cppcheck and clang.  But, these are very much the tip of the iceberg.

Some of the other problems detected include:

- use of non-reentrant system functions (e.g., `strtok`) in multi-threaded code;
- use of obsolete functions (e.g., `gethostbyname`);
- incorrect usage of `printf`-style functions;
- incorrect usage of `strcpy`-style functions (e.g., leaving strings without terminating NULL characters);
- incorrect usage of varargs functions;
- different parameter names in function declarations vs. definitions;

Some of these are nastier than others, but they are all legitimate problems and should be fixed.

The full results for both tools are available in the [GitHub repo](https://github.com/btorpey/static/tree/master/openmama), so it's easy to compare the warnings against the code.

## Conclusion
The state of the art in static analysis keeps advancing, thanks to people like Daniel Marjamäki and the rest of the [cppcheck team](https://github.com/danmar/cppcheck/graphs/contributors), and Gábor Horváth and the [team supporting clang](https://github.com/llvm-mirror/clang/graphs/contributors).

In particular, the latest releases of cppcheck and clang-tidy are detecting errors that previously could only be found by run-time analyzers like valgrind and Address Sanitizer.  This is great stuff, especially given how easy it is to run static analysis on your code.

The benefits of using one (or more) static analysis tools just keep getting more and more compelling -- if you aren't using one of these tools, I hope this will encourage you to do so.

If you found this article interesting or helpful, you might want to also check out the other posts in [this series](/blog/categories/static-analysis/).  And please leave a comment below or [drop me a line](<mailto:wallstprog@gmail.com>) with any questions, suggestions, etc.

<hr>
## Footnotes








</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[We Don't Need No Stinkin' Databases]]></title>
    <link href="http://btorpey.github.io/blog/2017/05/10/join/"/>
    <updated>2017-05-10T00:00:00-04:00</updated>
    <id>http://btorpey.github.io/blog/2017/05/10/join</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/Gold_Hat_portrayed_by_Alfonso_Bedoya.jpg" width="220" height="162">

I've been working on performance analysis recently, and a large part of that is scraping log files to capture interesting events and chart them.

I'm continually surprised by the things that you can do using plain old bash and his friends, but this latest one took the cake for me.

<!-- more -->

Did you know that Linux includes a utility named `join`?  Me neither.  Can you guess what it does?  Yup, that's right -- it does the equivalent of a database join across plain text files.

Let me clarify that with a real-world example -- one of the datasets I've been analyzing counts the number of messages sent and received in a format roughly like this:

|Timestamp| Recv |
|:--------|-----:|
| HH:MM:SS| x |

<br>
Complicating matters is that sent and received messages are parsed out separately, so we also have a separate file that looks like this:

|Timestamp| Send |
|:--------|-----:|
| HH:MM:SS| y |

<br>
But what we really want is something like this:

|Timestamp| Recv | Send |
|:--------|-----:|------:|
| HH:MM:SS| x | y |

<br>
Here are snips from the two files:

    $ cat recv.txt
    Timestamp	Recv
    2016/10/25-16:04:58	7
    2016/10/25-16:04:59	1
    2016/10/25-16:05:00	7
    2016/10/25-16:05:01	9
    2016/10/25-16:05:28	3
    2016/10/25-16:05:31	9
    2016/10/25-16:05:58	3
    2016/10/25-16:06:01	9
    2016/10/25-16:06:28	3
    $ cat send.txt
    Timestamp	Send
    2016/10/25-16:04:58	6
    2016/10/25-16:05:01	18
    2016/10/25-16:05:28	3
    2016/10/25-16:05:31	9
    2016/10/25-16:05:58	3
    2016/10/25-16:06:01	9
    2016/10/25-16:06:28	3
    2016/10/25-16:06:31	9
    2016/10/25-16:06:58	3


I had stumbled across the `join` command and thought it would be a good way to combine the two files.

Doing a simple join with no parameters gives this:

    $ join recv.txt send.txt
    Timestamp Recv Send
    2016/10/25-16:04:58 7 6
    2016/10/25-16:05:01 9 18
    2016/10/25-16:05:28 3 3
    2016/10/25-16:05:31 9 9
    2016/10/25-16:05:58 3 3
    2016/10/25-16:06:01 9 9
    2016/10/25-16:06:28 3 3

As you can see, we're missing some of the measurements.  This is because by default `join` does an [inner join](https://en.wikipedia.org/wiki/Join_(SQL)#Inner_join) of the two files (the intersection, in set theory).

That's OK, but not really what we want.  We really need to be able to reflect each value from both datasets, and for that we need an [outer join](https://en.wikipedia.org/wiki/Join_(SQL)#Outer_join), or union.

It turns out that `join` can do that too, although the syntax is a bit more complicated:

    $ join -t $'\t' -o 0,1.2,2.2 -a 1 -a 2 recv.txt send.txt
    Timestamp	Recv	Send
    2016/10/25-16:04:58	7	6
    2016/10/25-16:04:59	1
    2016/10/25-16:05:00	7
    2016/10/25-16:05:01	9	18
    2016/10/25-16:05:28	3	3
    2016/10/25-16:05:31	9	9
    2016/10/25-16:05:58	3	3
    2016/10/25-16:06:01	9	9
    2016/10/25-16:06:28	3	3
    2016/10/25-16:06:31		9
    2016/10/25-16:06:58		3


A brief run-down of the parameters is probably in order:

|Parameter | Description
|----------|------------
| `-t $'\t'` | The `-t` parameter tells `join` what to use as the separator between fields.  The tab character is the best choice, as most Unix utilities assume that by default, and both Excel and Numbers can work with tab-delimited files.<br>The leading dollar-sign is a [trick](https://unix.stackexchange.com/a/46931/198530) used to to pass a literal tab character on the command line  .
| `-o 0,1.2,2.2` | Specifies which fields to output.  In this case, we want the "join field" (in this case, the first field from both files), then the second field from file #1, then the second field from file #2.
| `-a 1` | Tells `join` that we want **all** the lines from file #1 (regardless of whether they have a matching line in file #2).
| `-a 2` | Ditto for file #2.

<br>
As you can probably see, you can also get fancy and do things like left outer joins and right outer joins, depending on the parameters passed.

Of course, you could easily import these text files into a "real" database and generate reports that way.  But, you can accomplish a surprising amount of data manipulation and reporting with Linux's built-in utilities and plain old text files.

### Acknowledgements
I couldn't remember where I had originally seen the `join` command, but recently found it again in a [nice post by Alexander Blagoev](http://ablagoev.github.io/linux/adventures/commands/2017/02/19/adventures-in-usr-bin.html).  Check it out for even more obscure commands!  And, thanks Alexander!  

And thanks also to Igor for his own [very nice post](http://shiroyasha.io/coreutils-that-you-might-not-know.html) that led  me back to Alexander's.</p>
]]></content>
  </entry>
  
</feed>
